library(MASS)
##################################################################################################
myPanel <- function(x, y, subscripts, ...) {
panel.xyplot(x, y, pch='.',cex=2, ...)
#ltext(x - 1, y + 1, letters[subscripts], cex=0.5)
}
myPrePanel <- function(x, y, ...) {
  list(xlim=c(min(x) - 1, max(x) + 1),
  ylim=c(min(y) - 1, max(y) + 1))
  }
 myStrip <- function(which.panel, ...) {
   font <- rep(1, 2)
   font[which.panel] <- 2
   col=rep("grey", 2)
   col[which.panel] <- "black"
   llines(c(0, 1, 1, 0, 0), c(0, 0, 1, 1, 0))
   ltext(c(0.33, 0.66), rep(0.5, 2), 1:2,
   font=font, col=col)
   }
##################################################################################################
### xtab: an R function used to generate a latex table; particularly useful for including the associated standard deviation with the estimate.
### Btw, there is a more popular R package named xtable.
xtab<-function(x, x2=NULL, caption=NULL, align=NULL, digits=NULL, rownames=NULL, colnames=NULL){
m=nrow(x)
n=ncol(x)
if(!is.null(rownames(x))) rownames=rownames(x)
if(!is.null(colnames(x))) colnames=colnames(x)
cat('% latex table generated by xtab function\n')
cat('%',date(),'\n')
cat('\\begin{table}[ht]\n')
if(!is.null(caption))
cat('\\caption{',caption,'}\n',sep='')
cat('\\begin{center}\n')
if(is.null(align)) {
align = 'l'
for(i in 1:n) align = paste(align,'r',sep='')
}
if(nchar(align)==1){
atmp=align
for(i in 1:n) align = paste(align,atmp,sep='')
}

if(is.null(digits)) digits = rep(2,n)
if(length(digits)==1) digits = rep(digits,n+1)
cat('\\begin{tabular}{', align,'}\n', sep='')
cat('\\hline\n')
if(!is.null(colnames)){
for(j in 1:n)
cat('&',colnames[j],sep='')
cat('\\\\\n\\hline\n')
}
for(i in 1:m){
cat(rownames[i])

for(j in 1:n){
if(!is.character(x[i,j]))
cat('&',format(round(x[i,j],digits[j]),nsmall=digits[j]),sep='')
else  cat('&',x[i,j],sep='')
if(!is.null(x2))  {
if(!is.character(x2[i,j]))
cat('(',format(round(x2[i,j],digits[j]),nsmall=digits[j]),')',sep='')
else
cat('(',x2[i,j],')',sep='')
}
}
cat('\\\\\n')

}
cat('\\hline\n')
cat('\\end{tabular}\n')
cat('\\end{center}\n')
cat('\\end{table}\n')
}
##################################################################################################
#  code to fit scatterplot with model
#  y=Xb+e, using constrained least-squares,
#  where the constraints are in the form Ab>=0,
#  where A is irreducible.
#  Also does the test of null: Ab=0 vs Ab>0
#
#  uses mix-of-betas null distribution
#  returns p-value, constrained and unconstrained fits.
constrp=function(xmat,y,amat){
	n=length(y)
	sm=1e-10
	m=length(xmat)/n
	pmat=xmat%*%solve(t(xmat)%*%xmat)%*%t(xmat)
	umat=chol(t(xmat)%*%xmat)
	uinv=solve(umat)
	atil=amat%*%uinv
	z=t(uinv)%*%t(xmat)%*%y
	ans=coneproj(z,atil)
	bhat=uinv%*%ans$phihat
	fhatc=xmat%*%bhat
	fhatuc=pmat%*%y
	ans=new.env()
	ans$fhatc=fhatc
	ans$bhat=bhat
  ans
}
##################################################################################################
coneproj=function(y,amat){
	n=length(y);m=length(amat)/n
	sm=1e-8;h=1:m<0;obs=1:m;check=0
	delta=-amat;b2=delta%*%y
	if(max(b2)>sm){
		i=min(obs[b2==max(b2)])
		h[i]=TRUE
	}else{check=1;theta=1:n*0}
	while(check==0){
		xmat=matrix(delta[h,],ncol=n)
		a=solve(xmat%*%t(xmat))%*%xmat%*%y
		if(min(a)<(-sm)){
			avec=1:m*0;avec[h]=a
			i=min(obs[avec==min(avec)])
			h[i]=FALSE;check=0
		}else{
			check=1
			theta=t(xmat)%*%a
			b2=delta%*%(y-theta)
			if(max(b2)>sm){
				i=min(obs[b2==max(b2)])
				h[i]=TRUE;check=0
			}
		}
	}
	ans=new.env()
	ans$dim=n-sum(h)
	ans$phihat=y-theta
	(ans)
}
###############################################################################################################
# computes Q of QR decomposition; returns col rank of amat
###############################################################################################################
qrdecomp=function(xm){
sm=1e-8;
	n=length(xm[,1]);m=length(xm[1,])
	nq=1
	lv=0
	qm=xm
	i=0
	while(lv==0&i<=m){
		i=i+1
		lv=sum(xm[,i]^2)
	}
	if(lv==0){stop}
	qm[,nq]=xm[,i]/sqrt(lv)
	for(j in (i+1):m){
		res=xm[,j]
		for(k in 1:nq){
			res=res-sum(qm[,k]*xm[,j])*qm[,k]
		}
		lv=sum(res^2)
		if(lv>sm){
			nq=nq+1
			qm[,nq]=res/sqrt(lv)
		}
	}
	qm=qm[,1:nq]
	ans=new.env()
	ans$qm=qm
	ans$rank=nq
	ans
}
################################################################################
## For regression with a single covariate.
## fit a piecewise linear function with k knots.
## The function is constrained to be non-increasing and convex.
const.fit=function(ys,xs,k=3)
{
  knot.v=quantile(xs,(1:(k-1))/k)
  xmat=cbind(1,xs)
  for(kn in 1:(k-1))
  {
    xtemp=xs-knot.v[kn]
    xtemp[xtemp<0]=0
    xmat=cbind(xmat,xtemp)
    #cat(knot.v[kn],'\n')
  }
  smat=c()
  for(kn in 1:k)
  {
   smat=rbind(smat,t(c(0,rep(1,kn),rep(0,k-kn))))
   #if(kn<k) smat=rbind(smat,)
  }
  smat=-smat
  for(kn in 1:(k-1))
  {
   #smat=rbind(smat,t(c(0,rep(1,kn),rep(0,k-kn))))
   #if(k>(kn+2))
   smat=rbind(smat,t(c(rep(0,kn+1),1,rep(0,k-kn-1))))
   #if(k==(kn+2)) smat=rbind(smat,t(c(rep(0,kn+1),1,-1)))
  }
  #par(mfrow=c(1,1))
  #plot(xs,ys,type='p')
  #points(xs,constrp(xmat,ys,smat)$fhatc,type='p',pch='.',cex=2,col='red')
  #plot(ys,constrp(xmat,ys,smat)$fhatc,type='p')
  #abline(0,1,col='red')
  bet=constrp(xmat,ys,smat)$bhat
  interp.v=c()
  slope.v=c()
  res=list(fit=constrp(xmat,ys,smat)$fhatc)
  for(kn in 0:(k-1))
  {
  interp=bet[1]
  if(kn>0) interp=interp-sum(bet[3:(3+kn-1)]*knot.v[1:(kn)])
  slope=sum(bet[2:(2+kn)])
  interp.v=c(interp.v,interp)
  slope.v=c(slope.v,slope)
  }
  #cat(interp.v,"\n",slope.v,"\n")
  res$reg=cbind(c(min(xs),knot.v),c(knot.v,max(xs)),interp.v,slope.v)
  return(res)
}
   #k=4
   #n=50
   #xs=sort(runif(n))
   #ys=-xs
   #ys[ys<(-0.31)]=-0.31
   #ys=ys+rnorm(n,0,.004)
   #const.fit(ys,xs,k=4)
   #plot(xs,ys,type='p')
   #lines(xs, const.fit(ys,xs,k=4)$fit,type='p',pch=19,col='red')
   #ys=c(1,2,10,11,13,17,18,19,10,5,1,1,2.48,2.17,5,3.79,3.79,3.48,2.48,1.55,2,1.24,1.24,1,0.93,0.93,0.93,0.93,3,8,3,6,10,11,5,7,7,3,2,4,1.44,1.26,1,1.62,1.62,1.44,1.44,1.44,0.9,1,0.72,0.72,1,0.54,0.54,0.54,0.54,2,4,4,8,17,19,13,14,4,5,3,1,4.72,2.38,3,3.06,2.72,2.72,2.72,1.7,2,1.36,1.36,1,1.02,1.02,1.02,1.02)
   #xs=c(1495,1455.5,1543.505,1501.183,1476.08,1449.88,1428.444,1426.934,1378.31,1383.402,1549.9,1337,1406.379,1406.378,1405.432,1444.248,1439.024,1450.466,1406.379,1406.381,1406.38,1265.742,1265.742,1406.38,1406.376,1406.376,1406.376,1265.742,1514,1458.255,1468,1513.172,1518.005,1450.191,1459.008,1408.579,1441.14,1402.34,1440.01,1392.5,1406.382,1406.381,1265.74,1406.383,1406.383,1406.382,1406.382,1406.382,1406.378,1406.38,1265.736,1265.736,1406.38,1406.389,1406.389,1406.389,1265.741,1575,1477.5,1523.25,1509.38,1483,1452.89,1456.003,1450.36,1476.5,1441.6,1272.01,1450,1467.235,1406.378,1265.74,1406.379,1406.379,1406.379,1406.379,1406.382,1406.38,1265.743,1265.743,1406.38,1406.382,1406.382,1406.382,1265.735)
   #const.fit(ys,xs,k=4)
## Given a factor variable, generate the design matrix (matrix of 1 and 0's).
## input is a factor variable.
gen.design=function(fact,cont=matrix(1,nrow=length(fact),ncol=1,byrow=T))
{
  n=length(fact)
  unik=unique(fact)
  p=length(unik)
  mat=matrix(0,nrow=n,ncol=p,byrow=T)
  for(j in 1:p)
  {
   mat[fact==unik[j],j]=1
  }
   fact.mat=mat
   mat=c()
   for(k in 1:ncol(cont))
     {
       mat=cbind(mat,fact.mat*cont[,k])
     }
  return(mat)
}
## calculate the log-inverse gamma density with certain parameters
ig.logdensity=function(arg,alpha,bet)
{
  return(alpha*log(bet)-log(gamma(alpha))-(alpha+1)*log(arg)-bet/arg)
}
################################################################################
## Matrix orthogonalization.
orthogonalize=function(X1,X2)
{
  n=nrow(X2)
  Imat=diag(rep(1,n))
  return((Imat-X1%*%solve(t(X1)%*%X1)%*%t(X1))%*%X2)
}
### Given a data matrix, stratum indicator and sampling weight (inverse of inclusion probability),
### calculate the matrix of stratum Hajek means, same dim as data matrix
strat.mean=function(dat,strat,wt,indic="Hajek",N.size=0)
{
 dat=as.matrix(dat)
 H=length(unique(strat))
 res=c()
 for(h in 1:H)
 {
  stra.size=sum(strat==h)
  if(stra.size>0)
  {
  temp=apply(as.matrix((dat[strat==h,]*wt[strat==h])/sum(wt[strat==h])),2,sum)
  if(indic=="HT") temp=apply(as.matrix((dat[strat==h,]*wt[strat==h])/N.size[h]),2,sum)
  res=rbind(res,matrix(rep(temp,stra.size),nrow=stra.size,ncol=length(temp),byrow=T))
  }
 }
 return(res)
}
### Gieven two data matrices, stratum indicator, wt, deltaij matrix and population size.
#### calculate covariance estimator.
strat.covest=function(dat1,dat2,strat,wt,deltaij,N)
{
  resid=as.matrix(dat1)
  dat2=as.matrix(dat2)
  return((t(resid*wt)%*%(deltaij/(deltaij+(1/wt)%*%t(1/wt)))%*%(dat2*wt))/N^2)
}
## Calculate the inverse of symmetric matrix
inverse=function(mat)
{
  tmp=solve(chol(mat))
  return(tmp%*%t(tmp))
}
inverse2=function(mat)
{
  val=eigen(mat)$values
  vec=eigen(mat)$vectors
  return(vec%*%diag(1/val)%*%t(vec))
}
## Risk function
## and return the criterion and derivative of each risk function
## loss takes on "OPT" or "GREG"
risk=function(beta,y,x,pii,pij,loss)
{
   y.cur=y
   x.cur=x
   mat=1/(N*diag(pii))

   if(loss=="OPT")
   {
    mat=((1/pii)%*%t(1/pii)-1/pij)/(N^2)*sum(1/pii)
    y.cur=y-sum(y/pii)/sum(1/pii)
    x.cur=t(t(x)-apply(x/pii,2,sum)/sum(1/pii))
   }
   res=list(crit=t(y.cur-x.cur%*%beta)%*%mat%*%(y.cur-x.cur%*%beta),derv=2*t(x.cur)%*%mat%*%(-y.cur+x.cur%*%beta))
   return(res)
}
## draw stratified PPS sample with sample size nh and
## return the index of selected elements + inclusion probabilities
PPSWR=function(stratid,pii,size)
{
  H=length(size)
  index=1:length(stratid)
  indexh=c()
  ind.samp=c()
  pii.samp=c()
  for(h in 1:H)
  {
   indexh=index[stratid==h]
   piih=pii[stratid==h]/sum(pii[stratid==h])
   xixi=apply(runif(size[h])>matrix(rep(cumsum(piih),size[h]),nrow=size[h],byrow=T),1,sum)+1
   ind.samp=c(ind.samp,indexh[xixi])
   pii.samp=c(pii.samp,piih[xixi])
  }
  return(list(ind=ind.samp,pii=pii.samp))
}
## test the sampling function
 #par(mfrow=c(2,2))
 # cnt=c()
#  for(i in 1:sum(Nh))
#  {
#   cnt=c(cnt,sum(i==indx))
# }
# for(h in 1:H)
# plot(pii.pop[strat.id==h],cnt[strat.id==h],type='p',pch='.',cex=2)
# x11()
# par(mfrow=c(1,1))
 #plot(pii.pop,cnt,type='p',pch='.',cex=2)
##########################################
#  R function for solving g_1(lambda)=0  #
##########################################
## Chen and Sitter empirical likelihood
Lag2<-function(u,ds,mu,init.val)
{
   n<-length(ds)
   u<-(u-rep(1,n)%*%t(mu))
   M<-init.val
   dif<-1
   tol<-1e-06
   while(dif>tol){
      uM.tmp=u%*%M
      #cat((uM.tmp),'\n')
      #print(mean(uM.tmp< -(1-(1e-4))))
      #cat("\n")
      uM.tmp[uM.tmp< -(1-(1e-4))]=-(1-(1e-4))
      D1<-t(u)%*%diag(as.vector(ds/(1+uM.tmp)))%*%rep(1,n)
      DD<-(-t(u)%*%diag(as.vector(ds/(1+uM.tmp)^2))%*%u)
      #print(summary((as.vector((uM.tmp)))))
      #print(M)
      #cat("\n")
      #cat(DD,'\n')
      if(prod(abs(eigen(DD)$values))<1e-9)
      {
       M=F
       break
      }
      #print(DD)
      #print(eigen(DD)$values)
      D2<-solve(DD,D1,tol=1e-13)
      dif<-max(abs(D2))
      rule<-1
      while(rule>0){
         rule<-0
         if(min(1+t(M-D2)%*%t(u))<=0) rule<-rule+1
         if(rule>0) D2<-D2/2
         }
      D2[D2>10]=10
      D2[D2< (-10)]= -10
      #cat(max(abs(D2)),'\n')
      M<-M-D2
      #print(round(cbind(D2,M),6))
      #cat(u%*%M,'\n')

      #cat("\n")
   }
   return(M)
}
#PAEMLE.CS(rep(1,sum(n.v)),gen.design(rep(1:m,n.v),y.samp-Zmat.samp%*%mu.new),rep(1,sum(n.v)),rep(0,m),init=lambda.cur)
### Pseudo-empirical MLE.
### No need to include the vector of 1's in x matrix.
PEMLE.CS=function(y,x,wt,xbar,init)
{
  lambda=Lag2(x,wt,xbar,init.val=init)
  if(lambda[1]==F) return(list(weight=0, est=0, indic=F))
  wet=wt/(1+t(t(x)-xbar)%*%lambda)
  wet=wet/sum(wet)
  return(list(weight=wet,est=sum(wet*y),indic=T,lamb=lambda))
}
### Pseudo-empirical MLE, with two-point adjustments.
PAEMLE.CS=function(y,x,wt,xbar,init,scaling=1.9,unequal=F)
{
  #m=9; n.v=rep(20,m)
  #y=rep(1,sum(n.v))
  #x=gen.design(rep(1:m,n.v),y.samp-Zmat.samp%*%mu.cur)
  #wt=rep(1,sum(n.v))
  #xbar=rep(0,m)
  #init=lambda.cur
  adjust=F
  if(sum(!(apply(t(t(x)-xbar),2,checksign)))>0)
  {
   adjust=T
   y=c(y,mean(y),mean(y))
   wt=c(wt,1,1)
   mean.x=apply(x,2,mean)
   u.vec=(mean.x-xbar)/sqrt(sum((mean.x-xbar)^2))
   c.u=1/sqrt(t(u.vec)%*%solve(cov(x))%*%u.vec)*scaling
   x=rbind(x,t(xbar-c.u*u.vec))
   x=rbind(x,t(2*mean.x-xbar+c.u*u.vec))
  }
   lambda=Lag2(x,wt,xbar,init.val=init)
   if(lambda[1]==F) return(list(weight=0, est=0, indic=F))
   wet=wt/(1+t(t(x)-xbar)%*%lambda)
   wet=wet/sum(wet)
   return(list(weight=wet,est=sum(wet*y),indic=T,lamb=lambda,Adjust=adjust))
}
### Kim empirical likelihood
PEMLE.Kim=function(y,x,wt,xbar)
{
   n<-length(wt)
   u<-x-rep(1,n)%*%t(xbar)
   lambda1<-sum(wt)
   lambda2<-0*xbar
   dif<-1
   tol<-1e-08
   while(dif>tol){
      D1<-t(u)%*%diag(as.vector(ds*logderv.star(1+u%*%M)))%*%rep(1,n)
      DD<-t(u)%*%diag(as.vector(ds*logderv2.star(1+u%*%M)))%*%u
       if(prod(abs(eigen(DD)$values))<1e-12)
       {
        M=F
        break
       }
      D2<-solve(DD,D1,tol=1e-13)
      dif<-max(abs(D2))
      M<-M-D2
   }
  wet=wt/(1+t(t(x)-xbar)%*%M)
  wet=wet/sum(wet)
  return(list(weight=wet,est=sum(wet*y)))
}
### simulate from multivariate normal
sim.mvnorm=function(mu,varm)
{
  p=length(mu)
  return(t(chol(varm))%*%as.vector(rnorm(p))+mu)
}
### simulate from multivariate t-distribution
sim.mvt=function(mu,varm,degf)
{
  p=length(mu)
  y=(t(chol(varm))%*%as.vector(rnorm(p)))
  u=rchisq(1,degf)
  return(mu+y*sqrt(degf/u))
}
## test the mv.norm function
 #res=c()
 #res.norm=c()
 #for(i in 1:5000)
 # {
 # res=cbind(res, sim.mvt(c(1,2),matrix(c(1,2,2,10),2,2,byrow=T),6))
 # res.norm=cbind(res.norm, sim.mvnorm(c(1,2),matrix(c(1,2,2,10),2,2,byrow=T)))
 # }
 #cov(t(res))
 #cov(t(res.norm))
 #par(mfrow=c(2,2))
 #plot(t(res))
 #plot(t(res.norm))
### trace plot
traceplot=function(xmat,true=NA,startp=1,endp=nrow(xmat),thin=1,string="")
{
  index=seq(startp,endp,by=thin)
  for(j in 1:ncol(xmat))
  {
   if(j %% 9==1) {x11(); par(mfrow=c(3,3))}
   plot(index,xmat[index,j],type='l',main=string[j],ylab=string[j])
   abline(h=mean(xmat[index,j]),col='blue')
   abline(h=true[j],col='red')
  }
}
## Given a simulated chain (after burnin and thinning) in matrix form,
## calculate the posterior mean, s.d, lower and upper limit of 95% CI.
Bayes.summary=function(sim)
{
  res=c()
  p=dim(sim)[2]
  for(j in 1:p)
  {
  res=c(res, mean(sim[,j]),sd(sim[,j]), quantile(sim[,j],probs=c(0.025,0.975)))
  }
  return(res)
}
## print out summary given true value and Bayes summary (estimated value, s.d., and lower & upper bound of CI)
print.summary=function(true,Bayes,string='')
{
  cat(string,":", true, apply(Bayes[,1:2],2,mean), mean(Bayes[,3]< true & Bayes[,4]>true),'\n')
}
## Directly optimize the quasi-posterior density by iterative maximization and seek the mode of pi(theta|y) .
## obs denotes the observations, and theta.init denotes initial values of theta.
optim.quasi=function(obs,theta.init,Avar,Dvar,tolern=1e-6)
{
  m=length(theta.init)
  theta.cur=theta.init
  weight=rep(1,length(theta.cur))
  low=min(obs)-2*sd(obs)
  high=max(obs)+2*sd(obs)
  xmat.cur=cbind(obs-theta.cur,(obs-theta.cur)^2/Dvar-1)
  lambda.cur=Lag2(xmat.cur,rep(1,nrow(xmat.cur)),rep(0,ncol(xmat.cur)))
##############################################################################
  for(j in 1:500)
  {
  xmat.cur=cbind(obs-theta.cur,(obs-theta.cur)^2/Dvar-1)
  #xmat.mod=cbind(c(obs-theta.cur,-log(m)/2*mean(obs-theta.cur)),c((obs-theta.cur)^2/Dvar-1,-log(m)/2*mean((obs-theta.cur)^2/Dvar-1)))
  #derv=apply(xmat.cur/(1+xmat.cur%*%lambda.cur),2,sum)
  #cat((((1+xmat.cur%*%lambda.cur))),'\n')
  #cat(dim(t(xmat.cur)),'\n')
  cat(j,lambda.cur,prod(1/as.vector(1+xmat.cur%*%lambda.cur)),exp(-sum((theta.cur)^2)/2/Avar),theta.cur,'\n')
  Bmat=t(xmat.cur)%*%diag(1/as.vector(1+xmat.cur%*%lambda.cur))%*%rep(1,m)
  Amat=t(xmat.cur)%*%diag(1/as.vector(1+xmat.cur%*%lambda.cur)^2)%*%xmat.cur
  Bmat2=t(xmat.cur)%*%rep(1,m)
  Amat2=t(xmat.cur)%*%xmat.cur
  lambda.cur=solve(Amat2,Bmat2,tol=tolern)
  if(sum(lambda.cur==F)>0) return(F)
  #lambda.cur=as.vector(lambda.cur+solve(Amat,Bmat,tol=tolern))
  #Lag2(xmat.cur,rep(1,nrow(xmat.cur)),rep(0,ncol(xmat.cur)))*0.98+lambda.cur*0.02
  theta.new=theta.cur
  for(i in 1:m)
  {
   funk=function(thet)
   {
    #xmat.tmp=xmat.cur
    #xmat.tmp[i,]=c(obs[i]-thet,(obs[i]-thet)^2/Dvar[i]-1)
    #lambda.cur=solve(t(xmat.tmp)%*%xmat.tmp,t(xmat.tmp)%*%rep(1,m))
    return(exp(-(thet)^2/2/Avar)/(1+sum(lambda.cur*(c(y[i]-thet,(y[i]-thet)^2/Dvar[i]-1)))))
   }
   theta.new[i]=optimize(f=funk,interval=c(obs[i]-sd(obs),obs[i]+sd(obs)),maximum =T)$maximum
  }
  if(sum(abs(theta.cur-theta.new))<tolern) break;
  theta.cur=theta.new
  }
##############################################################################
return(theta.cur)
}
###############################################################################
#  R function for solving g^*_1(lambda)=0  #
#  where the g^*_1 () is modified so we dont need to check constraint each time.
##############################################################################
## Chen and Sitter empirical likelihood
Lag2.Mod<-function(u,ds,mu,init.val=c(0,0))
{
   n<-length(ds)
   u<-u-rep(1,n)%*%t(mu)
   M<-init.val
   dif<-1
   direction<-0
   tol<-1e-06
   while(dif>tol){
      D1<-t(u)%*%diag(as.vector(ds*logderv.star(1+u%*%M)))%*%rep(1,n)
      DD<-t(u)%*%diag(as.vector(ds*logderv2.star(1+u%*%M)))%*%u
       if(prod(abs(eigen(DD)$values))<1e-12)
       {
        M=F
        break
       }
      D2<-solve(DD,D1,tol=1e-13)
      dif<-max(abs(D2))
 #########################################################################
      ## Check whether the solution is in the ellipse or not
      rule<-1
      while(rule>0){
         rule<-0
         if(sum((((M-D2)-c(0,1/2))*c(1/sqrt(Dmax),1/2))^2)>1) rule<-rule+1
         if(min(1+t(M-D2)%*%t(u))<=0) rule<-rule+1
         if(rule>0) D2<-D2/2
        }
 #########################################################################
      M<-M-D2
 #########################################################################
 ## return if M is on the boundary and tangent line perpendicular to the
 ## direction of next step.
 direction<-D2/sqrt(sum(D2^2)+1e-10)      ## Direction on where to move.
 xixi=(M-c(0,1/2))*c(1/sqrt(Dmax),1/2)^2
 xixi=c(xixi[2],-xixi[1])/sum(xixi^2+1e-10)   ## Tangent direction on ellipse.
 #cat(sum(((M-c(0,1/2))*c(1/sqrt(Dmax),1/2))^2),'\n')
 if(sum(((M-c(0,1/2))*c(1/sqrt(Dmax),1/2))^2)>(1-1e-10) & sum(direction*xixi)<1e-10)
 {
  #cat("On the boundary",'\n')
  break
 }
}
 #if(sum(((M-c(0,1/2))*c(1/sqrt(Dmax),1/2))^2)<(1-1e-10))  cat("Not on the boundary",'\n')
 return(M)
}
## Return logstar. derivative as in page 62 of Owen
logderv.star=function(z,size=m)
{
  res=z-z+2*size-size^2*z
  res[z>1/size]=(1/z)[z>1/size]
  return(res)
}
## Return logstar. 2nd derivative as in page 62 of Owen
logderv2.star=function(z,size=m)
{
  res=z-z-size^2
  res[z>1/size]=(-1/z^2)[z>1/size]
  return(res)
}
### Pseudo-empirical MLE.
### Modified empirical likelihood that reguarlized lambda.
PEMLE.CS.Mod=function(y,x,wt,xbar,init=c(0,0))
{
  lambda=Lag2.Mod(x,wt,xbar,init.val=init)
  if(lambda[1]==F) return(list(weight=0, est=0, indic=F))
  wet=wt/(1+t(t(x)-xbar)%*%lambda)
  wet=wet/sum(wet)
  return(list(weight=wet,est=sum(wet*y),indic=T,lamb=lambda))
}
##############################################################################
## Return coordinates of ellipse boundary given center and semi-axis
ellipse=function(center, ax)
{
  theta=(0:360)/180*pi
  return(t(cbind(cos(theta),sin(theta)))*ax+center)
}
##############################################################################
## Estimate A and beta under FH model; FH JASA paper
est.FH=function(yvec,xmat,Dvec)
{
  bet.cur=solve(t(xmat)%*%xmat)%*%t(xmat)%*%yvec
  A.cur=sum((yvec-xmat%*%bet.cur)^2)/(length(yvec)-ncol(xmat))
  k=length(yvec)
  p=ncol(xmat)
  tolern=1e-8
  A.new=1
  for(cnt in 1:1000)
  {
    bet.cur=solve(t(xmat)%*%diag(1/(A.cur+Dvec))%*%xmat)%*%t(xmat)%*%diag(1/(A.cur+Dvec))%*%yvec
    yfit=xmat%*%bet.cur
    f.A=sum((yvec-yfit)^2/(A.cur+Dvec))
    g.A=-sum((yvec-yfit)^2/(A.cur+Dvec)^2)
    A.new=A.cur+ (k-p-f.A)/g.A
    if(abs(A.new-A.cur)<tolern)
    {
     A.cur=A.new
     bet.cur=solve(t(xmat)%*%diag(1/(A.cur+Dvec))%*%xmat)%*%t(xmat)%*%diag(1/(A.cur+Dvec))%*%yvec
     break;
    }
    A.cur=A.new
    #cat(cnt,",",A.cur,",",bet.cur,'\n')
  }
  A.cur[A.cur<0]=0
  return(list(A.est=A.cur,bet.est=bet.cur))
}
################################################################################
checksign=function(vec)
{
 n=length(vec)
 hehe=T
 if(sum(vec <= 0)==n | sum(vec>=0)==n) hehe=F
 return(hehe)
}
################################################################################
library(MASS)
## Estimating a mixed effect model, x and z have to be matrices.
## in hierarchical Bayes framework, using Gibbs sampling
sim.mixed.1factor=function(y,x,z,sig2.bet,sig2.eps,M=1e+6,unequal=T)
{
  #cat("Oh Yeah!",'\n')
  #y=y.m[,1]
  #x=Xmat
  #z=Zmat
  #sig2.bet=0.2
  #sig2.eps=rep(0.1,n)
  ########################
  n=nrow(x)
  px=ncol(x)
  pz=ncol(z)
  sig2.mat=diag(sig2.eps)
  sig2inv.mat=diag(1/sig2.eps)
  ##############################################################################
  xz=as.matrix(cbind(x,z))
  Amat=solve(diag(c(rep(0,px),rep(1,pz)))*(1/sig2.bet)+diag(c(rep(1,px),rep(0,pz)))*(1/M)+t(xz)%*%sig2inv.mat%*%xz)
  ### Generate alpha and beta, respectively.
  temp=sim.mvnorm(Amat%*%t(xz)%*%sig2inv.mat%*%y,Amat)
  alpcur=temp[1:px]
  betcur=temp[(px+1):(px+pz)]
  ### Generate sigma^2_epsilon and sigma^2_beta, respectively.
  sig2.betcur=1/rgamma(1,pz/2+0.01,sum(betcur^2)/2+0.01)
  if(unequal)
  {
  sig2.epscur=1/rgamma(pz,apply(z,2,sum)/2+0.01,t(z)%*%((y-x%*%alpcur-z%*%betcur)^2)/2+0.01)
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=rep(sig2.epscur,apply(z,2,sum))))
  }
  if(!unequal)
  {
  sig2.epscur=1/rgamma(1,sum(apply(z,2,sum))/2+0.01,sum((y-x%*%alpcur-z%*%betcur)^2)/2+0.01)
  #cat(sig2.epscur,'\n')
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=rep(sig2.epscur,sum(apply(z,2,sum)))))
  }
}
##############################################################################
## Estimating a mixed effect model, x and z have to be matrices.
## in hierarchical Bayes framework, using Gibbs sampling
sim.mixed.1factor.chol=function(y,x,z,sig2.bet,sig2.eps,M=1e+6)
{
  #y=y.m[,1]
  #x=Xmat
  #z=Zmat
  #sig2.bet=0.2
  #sig2.eps=rep(0.1,n)
  ########################
  n=nrow(x)
  px=ncol(x)
  pz=ncol(z)
  sig2.mat=diag(sig2.eps)
  sig2inv.mat=diag(1/sig2.eps)
  ##############################################################################
  xz=as.matrix(cbind(x,z))
  mat=(diag(c(rep(0,px),rep(1,pz)))*(1/sig2.bet)+diag(c(rep(1,px),rep(0,pz)))*(1/M)+t(xz)%*%(sig2inv.mat)%*%xz)
  mat.chol=solve(chol(mat))
  Amat=mat.chol%*%t(mat.chol)
  ### Generate alpha and beta, respectively.
  temp=sim.mvnorm(Amat%*%t(xz)%*%(sig2inv.mat)%*%y,Amat)
  alpcur=temp[1:px]
  betcur=temp[(px+1):(px+pz)]
  ### Generate sigma^2_epsilon and sigma^2_beta, respectively.
  sig2.betcur=1/rgamma(1,pz/2-1/2+0.01,sum(betcur^2)/2+0.01)
  sig2.epscur=1/rgamma(pz,apply(z,2,sum)/2+0.01,t(z)%*%((y-x%*%alpcur-z%*%betcur)^2)/2+0.01)
  ##############################################################################
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=rep(sig2.epscur,apply(z,2,sum))))
}
################################################################################
## Sum up one variable at each value of the categorical variable, and return a list
## same length as the original x, with each element representing the mean
group.fn=function(cate,x,fn.name,sub.set=rep(T,length(cate)))
{
 cate=cate[sub.set]
 x=x[sub.set]
 if(fn.name=="sum")
  fn.res=tapply(x,as.factor(cate),sum)
 if(fn.name=="mean")
  fn.res=tapply(x,as.factor(cate),mean)
 if(fn.name=="var")
  fn.res=tapply(x,as.factor(cate),var)
 distinct=sort(unique(cate))
 res=x-x
 for(ca in 1:length(distinct))
 {
  res[cate==distinct[ca]]=fn.res[ca]
 }
 return(list(results=res,cates=distinct))
}
############################################################################
## stratified sample given the categorical variable of strata indicator.
strat.samp=function(indic,n=10)
{
  nlevel=length(unique(indic))
  unik=sort(unique(indic))
  index=1:length(indic)
  index.samp=c()
  for(ilevel in 1:nlevel)
  {
    index.cur=sample(index[indic==unik[ilevel]],n)
    index.samp=c(index.samp,index.cur)
  }
  return(index.samp)
}
################################################################################
## Using stratified SRS to draw a sample from univariate population.
## return two columns: first column indicates stratum, and second column contains observations.
ST.SRS=function(pop,nh,Nh,H)
{
  samp=c()
  for(h in 1:H)
  {
   temp=sample((1:dim(pop)[1])[pop[,1]==h],nh[h])
   samp=rbind(samp, cbind(pop[temp,],Nh[h]/nh[h]))
  }
  return(samp)
}
################################################################################
#### Rescale the second variable so that it matches the mean and variance of first variable.
match.meanvar=function(var1,var2)
{
  return(scale(var2)*sd(var1,na.rm=T)+mean(var1,na.rm=T))
}
################################################################################
## Solve2 finds the inverse of a matrix via eigen decomposition.
solve2=function(mat)
{
  vectors=eigen(mat)$vectors
  values=diag(1/eigen(mat)$values)
  return(vectors %*% values %*% t(vectors))
}
################################################################################
## Sum up one variable at each value of the categorical variable.
cate.sum=function(cate,x,sub.set=rep(T,length(cate)))
{
 cate=cate[sub.set]
 x=x[sub.set]
 distinct=sort(unique(cate))
 res=c()
 for(ca in 1:length(distinct))
 {
  res=c(res,sum(x[cate==distinct[ca]]))
 }
 return(list(counts=res,cates=distinct))
}
################################################################################
## Sum up one variable at each value of the categorical variable, and return a list
## same length as the original x, with each element representing the mean
group.fn=function(cate,x,fn.name,sub.set=rep(T,length(cate)))
{
 cate=cate[sub.set]
 x=x[sub.set]
 if(fn.name=="sum")
  fn.res=tapply(x,as.factor(cate),sum)
 if(fn.name=="mean")
  fn.res=tapply(x,as.factor(cate),mean)
 if(fn.name=="var")
  fn.res=tapply(x,as.factor(cate),var)
 distinct=sort(unique(cate))
 res=x-x
 for(ca in 1:length(distinct))
 {
  res[cate==distinct[ca]]=fn.res[ca]
 }
 return(list(results=res,cates=distinct))
}
##
n=20
aa=c(rep(1,n),rep(10,n))
group.fn(aa,rnorm(n*2),'sum')
################################################################################
dim.reduction.svd=function(xmat, delta)
{
  x.svd=svd(xmat)
  d.mat=diag(x.svd$d)
  v.mat=x.svd$v[,abs(x.svd$d)>delta]
  d.mat[abs(d.mat)<delta]=0
  return(x.svd$u %*% d.mat %*% v.mat)
}
################################################################################
## In the list "index", shuffle every k observations
shuffle=function(indx,kk)
{
  new.index=c()
  len=length(indx)
  j=1
  if(floor(len/kk)>0)
  {
  for(j in 1:floor(len/kk))
  {
   new.index=c(new.index,sample(indx[((j-1)*kk+1):(j*kk)],kk))
  }
  }
  if(floor(len/kk)==0) j=0
  if(len %%kk !=0)
   new.index=c(new.index,sample(indx[(j*kk+1):len],length((j*kk+1):len)))
  return(new.index)
}
################################################################################
### Given a training set and leaf indicator
### Create a table with each row representing a leaf.
summarize.tree=function(x,y,leaves)
{
  unik.leaves=unique(leaves)
  nleaves=length(unik.leaves)
  x.name=colnames(x)
  p=ncol(x)
  #cat(p,'\n')
  y.est=c()
  x.est=c()
  count=c()
  y.std=c()
  res.name=c()
  for(column in 1:p)
  {
   if(is.factor(x[,column]))
      res.name=c(res.name,x.name[column])
   if(is.numeric(x[,column]))
      res.name=c(res.name,paste(x.name[column],c("min","max"),sep='.'))
  }
  for(leaf in 1:nleaves)
  {
    #cat(leaf,'\n')
    sub.set=(leaves==unik.leaves[leaf])
    count=c(count,sum(sub.set))
    y.est=c(y.est,mean(y[sub.set]))
    y.std=c(y.std,sd(y[sub.set]))
    x.est.cur=c()
    for(column in 1:p)
    {
     if(is.factor(x[sub.set,column]))
        x.est.cur=c(x.est.cur,paste(as.character(unique(x[sub.set,column])),collapse=','))
     if(is.numeric(x[sub.set,column]))
        x.est.cur=c(x.est.cur,min(x[sub.set,column]),max(x[sub.set,column]))
    }
   x.est=rbind(x.est,t(x.est.cur))
  }
  res=cbind(x.est,y.est,y.std,count)
  #cat(c(x.name,"Fitted"),'\n')
  #cat(length(colnames(res)),'\n')
  colnames(res)=c(res.name,"Fitted","St.Dev","Size")
  return(res)
}
################################################################################
### Given a training set and leaf indicator
### Create a table with each row representing a leaf, and
### Robust regression of  log(y) on log(z) within each leaf.
### Const= whether we constrain the fit to be non-increasing.
sumReg.tree=function(x,y,z,leaves,Log=F,Const=T,Robs=F)
{
  unik.leaves=unique(leaves)
  nleaves=length(unik.leaves)
  x.name=colnames(x)
  p=ncol(x)
  x.est=c()
  count=c()
  alp=c()
  bet=c()
  res.name=c()
  for(column in 1:p)
  {
   if(is.factor(x[,column]))
      res.name=c(res.name,x.name[column])
   if(is.numeric(x[,column]))
      res.name=c(res.name,paste(x.name[column],c("min","max"),sep='.'))
  }
  for(leaf in 1:nleaves)
  {
    sub.set=(leaves==unik.leaves[leaf])
    count=c(count,sum(sub.set))
    if(Log)
    {
    logy.sub=log(y[sub.set])
    logz.sub=log(z[sub.set])
    coefs=rq(logy.sub~logz.sub,.5)$coefficients
    if(!Robs) coefs=lm(logy.sub~logz.sub)$coefficients
    if(Const & coefs[2]>0) {coefs=c(median(logy.sub),0)
    if(!Robs) coefs=c(mean(logy.sub),0)
    }
    }
    if(!Log)
    {
    coefs=rq(y[sub.set]~z[sub.set],.5)$coefficients
    if(!Robs) coefs=lm(y[sub.set]~z[sub.set])$coefficients
    if(Const & coefs[2]>0) {coefs=c(median(y[sub.set]),0)
    if(!Robs) coefs=c(mean(y[sub.set]),0)}
    }
    alp=c(alp,coefs[1])
    bet=c(bet,coefs[2])
    x.est.cur=c()
    for(column in 1:p)
    {
     if(is.factor(x[sub.set,column]))
        x.est.cur=c(x.est.cur,paste(as.character(unique(x[sub.set,column])),collapse=','))
     if(is.numeric(x[sub.set,column]))
        x.est.cur=c(x.est.cur,min(x[sub.set,column]),max(x[sub.set,column]))
    }
   x.est=rbind(x.est,t(x.est.cur))
  }
  res=cbind(x.est,alp,bet,count)
  colnames(res)=c(res.name,"Intercept","Slope","Size")
  rownames(res)=c()
  return(res)
}
################################################################################
### Given a training set and leaf indicator
### Create a table with each row representing a leaf, and
### monotone and convex regression on each leaf.
sumMonoConvex.tree=function(x,y,z,leaves,nknots=4)
{
  unik.leaves=unique(leaves)
  nleaves=length(unik.leaves)
  x.name=colnames(x)
  p=ncol(x)
  x.est=c()
  count=c()
  MonoConvex=c()
  res.name=c()
  for(column in 1:p)
  {
   if(is.factor(x[,column]))
      res.name=c(res.name,x.name[column])
   if(is.numeric(x[,column]))
      res.name=c(res.name,paste(x.name[column],c("min","max"),sep='.'))
  }
  for(leaf in 1:nleaves)
  {
    sub.set=(leaves==unik.leaves[leaf])
    count=c(count,sum(sub.set))
    cat(sum(sub.set),'\n')
    #if(sum(sub.set)==85) {
    #cat(y[sub.set],"\n"); cat(z[sub.set],"\n")
    #}
    coefs=as.vector(t(const.fit(y[sub.set],z[sub.set],k=nknots)$reg))
    #cat("Finish",'\n')
    MonoConvex=rbind(MonoConvex,t(coefs))
    rm(coefs)
    x.est.cur=c()
    for(column in 1:p)
    {
     if(is.factor(x[sub.set,column]))
        x.est.cur=c(x.est.cur,paste(as.character(unique(x[sub.set,column])),collapse=','))
     if(is.numeric(x[sub.set,column]))
        x.est.cur=c(x.est.cur,min(x[sub.set,column]),max(x[sub.set,column]))
    }
   x.est=rbind(x.est,t(x.est.cur))
  }
  res=cbind(x.est,MonoConvex,count)
  rm(MonoConvex)
  #colnames(res)=c(res.name,"Intercept","Slope","Size")
  rownames(res)=c()
  return(res)
}
################################################################################
## Retrieve leaf information based on a prediction data set and an association rule.
retrieve.leaf=function(rule.tree,dat)
{
  predict.leaf=rep(nrow(rule.tree),nrow(dat))
 for(ileaf in 1:(nrow(rule.tree)-1))
 {
   #cat(ileaf,'\n')
   ## stepwise ellimination to find the tree leaf corresponding to each observation
   indx=1:nrow(dat)
   indx=indx[as.numeric(dat[indx,1])>= as.numeric(rule.tree[ileaf,1]) & as.numeric(dat[indx,1]) <= as.numeric(rule.tree[ileaf,2])]
   for(j in 2:ncol(dat))
   {
    #cat(length(indx),'\n')
    indx=indx[(dat[indx,j] %in% unlist(strsplit(rule.tree[ileaf,j+1],',')))]
   }
   predict.leaf[indx]=ileaf
 }
  return(predict.leaf)
}
################################################################################
## Stratified and shuttled cross validation.
## The stratum variable starts from 1 to nstratum.
strat.CV=function(stratum, k)
{
 label=stratum
 nstratum=length(unique(stratum))
 strat=unique(stratum)
 for(i in 1:nstratum)
 {
  st=strat[i]
  hehe=(shuffle(1:sum(stratum==st),k)) %% k +1
  label[stratum==st]=hehe
 }
 return(label)
}
################################################################################
## Expit transformation
logit.inv=function(x)
{
  x[x>200]=200
  return(exp(x)/(1+sum(exp(x))))
}
expit=function(x)
{
  x[x>200]=200
  return(exp(x)/(sum(exp(x))))
}
## logistic function
logit=function(x)
{
  return(1/(1+exp(-x)))
}
exp.rob=function(x)
{
  x[x>100]=100
  x[x <(-50)]=(-50)
  return(exp(x))
}
## Vector version of logistic transformation
## with probability vector as inputs
veclogit=function(x)
{
  outside=1-sum(x)
  outside=max(c(outside,1e-200))
  return(log(x/outside))
}
################################################################################
#### k-fold cross validation for linear regression model.
#### indx provides the cross-validation labels.
#### Returns out-of-sample prediction error.
####  The R2 is computed by comparing the full model with reduced model
CV.LinReg=function(y,x,reduced=matrix(1,nrow=length(y),byrow=T),indx)
{
  CV.error=c()
  CV.R2=c()
  CV.AbsDev=c()
  index=indx
  k=length(unique(index))
  #cat(index)
  bet=solve(t(x)%*%x)%*%t(x)%*%y
  fit.reduced=reduced%*%solve(t(reduced)%*%reduced)%*%t(reduced)%*%y
  R2=1-mean((y-x%*%bet)^2)/mean((y-fit.reduced)^2)
  for(fold in 1:k)
  {
    y.train=y[index!=fold]
    x.train=x[index!=fold,]
    x.reduced.train=reduced[index!=fold,]
    y.test=y[index==fold]
    x.test=x[index==fold,]
    x.reduced.test=reduced[index==fold,]
    ## Regression.
    beta.train=solve(t(x.train)%*%x.train)%*%t(x.train)%*%y.train
    beta.reduced.train=solve(t(x.reduced.train)%*%x.reduced.train)%*%t(x.reduced.train)%*%y.train
    ## Compute prediction error.
    CV.error=c(CV.error,mean((y.test-x.test%*%beta.train)^2))
    CV.R2=c(CV.R2,1-mean((y.test-x.test%*%beta.train)^2)/mean((y.test-x.reduced.test%*%beta.reduced.train)^2))
    CV.AbsDev=c(CV.AbsDev,1-mean(abs(y.test-x.test%*%beta.train))/mean(abs(y.test-x.reduced.test%*%beta.reduced.train)))
  }
  return(list(error=CV.error,out.R2=CV.R2,AbsDev=CV.AbsDev,in.R2=R2))
}
################################################################################
#### k-fold cross validation for log-linear regression model.
#### Returns out-of-sample prediction error.
#### indx provides the cross-validation labels.
#### x and y are already log-transformed.
CV.logLinReg=function(y,x,reduced=matrix(1,nrow=length(y),byrow=T),indx)
{
  CV.error=c()
  CV.R2=c()
  CV.AbsDev=c()
  index=1:length(y)
  index=indx
  k=length(unique(indx))
  bet=solve(t(x)%*%x)%*%t(x)%*%y
  fit.reduced=exp(reduced%*%solve(t(reduced)%*%reduced)%*%t(reduced)%*%y)
  R2=1-mean((exp(y)-exp(x%*%bet))^2)/mean((exp(y)-fit.reduced)^2)
  for(fold in 1:k)
  {
    y.train=y[index!=fold]
    x.train=x[index!=fold,]
    x.reduced.train=reduced[index!=fold,]
    y.test=y[index==fold]
    x.test=x[index==fold,]
    x.reduced.test=reduced[index==fold,]
    ## Regression.
    beta.train=solve(t(x.train)%*%x.train)%*%t(x.train)%*%y.train
    beta.reduced.train=exp(x.reduced.test%*%solve(t(x.reduced.train)%*%x.reduced.train)%*%t(x.reduced.train)%*%y.train)
    ## Compute prediction error.
    CV.error=c(CV.error,mean((exp(y.test)-exp(x.test%*%beta.train))^2))
    CV.R2=c(CV.R2,1-mean((exp(y.test)-exp(x.test%*%beta.train))^2)/mean((exp(y.test)-beta.reduced.train)^2))
    CV.AbsDev=c(CV.AbsDev,1-mean(abs(exp(y.test)-exp(x.test%*%beta.train)))/mean(abs(exp(y.test)-beta.reduced.train)))
  }
  return(list(error=CV.error,out.R2=CV.R2,AbsDev=CV.AbsDev,in.R2=R2))
}
#Given a matrix, unique.ext returns
# the distinct row vectors
#and their counts
unique.ext=function(x)
{
 unique.v=c()
 p=dim(x)[2]
 n=dim(x)[1]
 unique.v=unlist(x[1,])
 cnt=c(1)
 for(i in 2:n)
  {
   xcur=unlist(x[i,])
   if(length(unique.v)/p >1) equ.indc=apply(unique.v==xcur,2,sum)
   else equ.indc=sum(unique.v==xcur)
   if(sum(equ.indc==p)>=1) cnt[equ.indc==p]=cnt[equ.indc==p]+1
   else
   {
   unique.v=cbind(unique.v,xcur)
   cnt=c(cnt,1)
   }
  }
  return(list(vectors=t(unique.v),counts=cnt))
}
## The function changes an NA value to zero
## But returns the original quantity if non-na
na0=function(x)
{
  x[is.na(x)]=0
  return(x)
}
na.del=function(x)
{
  return(x[!is.na(x)])
}
### Generate a correlation matrix given pho, using AR(1)
ar.cor=function(pho,p)
{
  mat=diag(1,p)
  for(i in 1:p)
   {
    for(j in 1:p) mat[i,j]=pho^(abs(i-j))
   }
   return(mat)
}
## simulate one draw of the beta in
## y=beta x + e
## where e~ N(0, 1-beta^2)
fn.corr=function(betarg, yarg, xarg)
{
  n=length(yarg)
  return(log(1-betarg^2)*(-n/2)-sum((yarg-betarg*xarg)^2)/2/(1-betarg^2))
}
## fisher transfomation and inverse fisher transformation
fisher.trans=function(x)
{
 x[x<-0.999]=0.999
 x[x>0.999]=0.999
 return(log((1+x)/(1-x))/2)
}
fisher.inv=function(x)
{
 res=(exp(2*x)-1)/(exp(2*x)+1)
 res[res>0.999]=0.999
 res[res<-0.999]=-0.999
 return(res)
}
### Prior distribution on the correlation coefficient
prior.fisher=function(beta2,nu)
  {
   ## prior sd is nu.
   beta2[beta2<-0.999]=0.999
   beta2[beta2>0.999]=0.999
   return(exp(-(log((1+beta2)/(1-beta2)))^2/2/nu^2)/(1+beta2)/(1-beta2))
  }
  #nu=seq(0,3,length.out=30)
  #bet=seq(-1,1,length.out=200)
  #par(mfrow=c(3,3))
  #for(i in 1:30)
  #{
  # plot(bet,prior.fisher(bet,nu[i]),main=round(nu[i],2))
  # if((i %% 9)==0) { x11(); par(mfrow=c(3,3))}
  #}
## uses uniform proposal, and metropolis algorithm.
sim.corr.unif=function(ys,xs,beta.old=0)
{
  beta.new=runif(1,-0.999,0.999)
  resu=beta.new
  runifm=runif(1)
  if(runifm>min(exp(fn.corr(beta.new,ys,xs)-fn.corr(beta.old,ys,xs)),1)) resu=beta.old
  return(resu)
}
## uses fisher transformation and normal proposal after transformation.
sim.corr.fisher=function(ys,xs,beta.old=0,inform=F,nu0=1,fisher.sd=0.25)
{
  rat=1
  beta.new=fisher.inv(rnorm(1,fisher.trans(beta.old),fisher.sd))
  if(inform==T) rat=prior.fisher(beta.new,nu=nu0)/prior.fisher(beta.old,nu=nu0)
  resu=beta.new
  runifm=runif(1)
  rat=rat*exp(fn.corr(beta.new,ys,xs)-fn.corr(beta.old,ys,xs))
  rat=rat*exp(fisher.trans(beta.new))/exp(fisher.trans(beta.old))
  if(runifm>min(rat,1)) resu=beta.old
  return(resu)
}
##################
# test the functions
#n=50
#beta2=0.5
#x=rnorm(n)
#y=beta2*x+rnorm(n,0,sqrt(1-beta2^2))
#corm=c()
#corc=0
#par(mfrow=c(2,2))
#for(j in 1:10000)
#{
#  corm=c(corm,corc)
#  corc=sim.corr.fisher(y,x,beta.old=corc)
#}
#seql=seq(1001,10000,by=10)
#plot(corm[seql],type='l')
#plot(density(corm[seql]))
#abline(v=beta2)
#cat("posteior mean:",mean(corm[seql]),"post sd:",sd(corm[seql]),'\n')
#corm=c()
#for(j in 1:10000)
#{
#  corm=c(corm,corc)
#  corc=sim.corr.fisher(y,x,beta.old=corc,inform=T)
#}
#seql=seq(1001,10000,by=10)
#plot(corm[seql],type='l')
#plot(density(corm[seql]))
#abline(v=beta2)
#cat("posteior mean:",mean(corm[seql]),"post sd:",sd(corm[seql]),'\n')
###########################################################################
## convert a frame into a vector and delete the NA's
vectorize=function(mat)
{
 hehe=as.vector(as.matrix(mat))
 hehe=hehe[!is.na(hehe)]
 return(hehe)
}
###################################################################
## calculate log-multivariate density
mv.logdensity=function(x,muv,sigmat)
{
  p=length(x)
  choles=chol(sigmat)
  return(-p/2*log(2*pi)-log(det(choles))-t(x-muv)%*%solve(choles)%*%t(t(x-muv)%*%solve(choles))/2)
}
## calculate the log-inverse gamma density with certain parameters
ig.logdensity=function(arg,alpha,bet)
{
  return(alpha*log(bet)-log(gamma(alpha))-(alpha+1)*log(arg)-bet/arg)
}
### to simulate from double-truncated normal with mean, variance, truncation point.
norm.truncate=function(mu,variance,theta)
{
  trans1=pnorm((theta[,1]-mu)/sqrt(variance))
  #trans1[trans1<1/M]=1/M
  trans2=pnorm((theta[,2]-mu)/sqrt(variance))
  #trans2[trans2>(1-1/M)]=1-1/M
  #trans1[trans1>trans2]=trans2[trans1>trans2]
  hehe=trans1+(trans2-trans1)*runif(length(mu))
  #hehe=runif(length(mu),trans1,trans2)
  res=qnorm(hehe)*sqrt(variance)+mu
  res[res<theta[,1]]=theta[res<theta[,1],1]
  res[res>theta[,2]]=theta[res>theta[,2],2]
  return(res)
}
### trace plot
traceplot=function(x,startp,endp,thin,string="")
{
  index=seq(startp,endp,by=thin)
  plot(index,x[index],type='l',main=string,ylab=string)
}
## Given a simulated chain (after burnin and thinning) in matrix form,
## calculate the posterior mean, s.d, lower and upper limit of 95% CI.
Bayes.summary=function(sim)
{
  res=c()
  p=dim(sim)[2]
  for(j in 1:p)
  {
  res=c(res, mean(sim[,j]),sd(sim[,j]), quantile(sim[,j],probs=c(0.025,0.975)))
  }
  return(res)
}
## print out summary given true value and Bayes summary (estimated value, s.d., and lower & upper bound of CI)
print.summary=function(true,Bayes,string='')
{
  cat(string,":", true, apply(Bayes[,1:2],2,mean), mean(Bayes[,3]< true & Bayes[,4]>true),'\n')
}
###################################################################
## calculate the conditional mean and variance given joint normal mean and variances.
## index denotes the index of y1, where we want to evaluate the conditional density
cond.norm=function(mu,covar,y2,index1)
{
  p=length(mu)
  index2=setdiff(1:p,index1)
  mu1=mu[index1]
  mu2=mu[index2]
  covar11=covar[index1,index1]
  covar12=t(as.matrix(covar[index1,index2]))
  covar21=t(covar12)
  covar22=covar[index2,index2]
  mm=mu1+covar12%*%solve(covar22)%*%(y2-mu2)
  cc=covar11-covar12%*%solve(covar22)%*%covar21
  return(list(mu.cond=mm,var.cond=cc))
}
### simulate from multivariate normal from a full-rank covariance matrix.
sim.mvnorm=function(mu,varm)
{
  hehe=as.matrix(chol2(varm))
  p=dim(hehe)[2]
  return((hehe)%*%as.vector(rnorm(p))+mu)
}
### simulate from a constrained multivariate normal
## with sum of the elements=constraint
sim.mvnorm.cons=function(mu,varm,constraint=0)
{
  Amat=rbind(diag(rep(1,length(mu))),1)
  mu.full=Amat%*%mu
  varm.full=Amat%*%varm%*%t(Amat)
  ## partition mean and variance.
  mu1=mu
  mu2=mu.full[length(mu.full)]
  sig11=varm
  sig12=varm.full[1:length(mu),length(mu.full)]
  sig22=varm.full[length(mu.full),length(mu.full)]
  return(sim.mvnorm(mu1+sig12/sig22*(constraint-mu2),sig11-sig12%*%t(sig12)/sig22))
}
## simulate multivariate normal data in the presence of missing data
sim.normNA=function(arg1,arg2)
{
  temp=arg1
  hehe=arg1[!is.na(arg1)]
  xixi=arg2[!is.na(arg1),!is.na(arg1)]
  temp[!is.na(temp)]=sim.mvnorm(ginv(xixi)%*%hehe, ginv(xixi))
  return(temp)
}
#### calculate the square root matrix for a nonnegative definite matrix.
matsqrt.nnd=function(mat,tolern=1e-24)
{
  eigens=eigen(mat)$values
  eigens[eigens<tolern]=tolern
  eigens=sqrt(eigens)
  return((eigen(mat)$vector)%*%diag(eigens)%*%t(eigen(mat)$vector))
}
#### Calculate the square root matrix using SVD.
matsqrt.svd=function(mat)
{
  mat.svd=svd(mat)
  xixi=diag(sqrt(mat.svd$d))
  if(length(mat.svd$d)==1) xixi=sqrt(mat.svd$d)
  return(mat.svd$u%*%xixi%*%t(mat.svd$v))
}
### another function for taking matrix inverse
### when the diagonal has missing values, disregard and calculate inverse
### and return a matrix with same dimension
ginv2=function(mat)
{
 res=mat
 naind=is.na(diag(mat))
 res[!naind,!naind]=ginv(mat[!naind,!naind])
 res[naind,]=0
 res[,naind]=0
 return(res)
}
# test the function
# a=diag(rep(2,4))
# a[2,1]=a[1,2]=1
# a[3,1]=a[1,3]=1
# a[2,2]=na
## calculate "cholesky decomposition" when the matrix is not pd.
chol2=function(mat,tolern=1e-6)
{
  temp=eigen(mat)
  evals=Re(temp$values)
  vals=evals[evals>tolern]
  p=length(vals)
  vecs=Re(temp$vectors[,evals>tolern])
  return(vecs%*%diag(sqrt(vals),nrow=p,ncol=p))
}
## calculate matrix inverse via cholesky decomposition
## should never be used.
#solve2=function(mat)
#{
# xx=solve(chol(mat))
# return(t(xx)%*%xx)
#}
## calculate determinant via cholesky decomposition
det2=function(mat)
{
 xx=(chol(mat))
 return(prod(diag(xx))^2)
}
## calculate multiple regression R^2
Rsquare=function(yv,xm)
{
  bet=solve(t(xm)%*%xm)%*%t(xm)%*%yv
  fit=(xm%*%bet)
  return(sum((fit-mean(fit))^2)/sum((yv-mean(yv))^2))
}
## calculate multiple regression adjusted R^2
Adj.Rsquare=function(yv,xm)
{
  n=length(yv)
  p=length(xm)/n
  bet=solve(t(xm)%*%xm)%*%t(xm)%*%yv
  fit=(xm%*%bet)
  SST=sum((yv-mean(yv))^2)
  SSE=SST-sum((fit-mean(fit))^2)
  return(1-SSE/SST*(n-1)/(n-p))
}
## MCMC for an unstructured correlation matrix.
## Input is the data set, previous corr. matrix,
## Output is the new correlation matrix after 1 iteration.
## Using jointly uniform prior and Beta prior with mean being previous value and
## s.d.=tuning* length of elligible interval for each r_{ij}
sim.corr.mat=function(dat, cor.old, tuning=1/20,scal=F)
{
  ## Location-scale transformation to the data set.
  if(scal) dat=scale(dat)
  cor.cur=cor.old
  p=nrow(cor.old)
  for(i in 1:(p-1))
  {
   for(j in (i+1):p)
   {
    ## the correlation coefficient that we want to update
    r.cur=cor.cur[i,j]
    ## figure out the interval for the (i, j)-th correlation coefficient.
    intvl=corr.interval(cor.cur,i,j)
    ## the updated value
    int=intvl[2]-intvl[1]
    para.cur=eBeta4(c(intvl,r.cur,(int*tuning)^2))
    ## Replace the old correlation with the new to form a new corr. matrix.
    r.new=rBeta4(1,para.cur)
    cor.new=cor.cur
    cor.new[i,j]=cor.new[j,i]=r.new
    #if(sum(para.cur[3:4]<0)>0)
    if(is.na(r.new))
    {
    return(cor.cur)
    }
    #cat('New corr:',r.new,para.cur,'\n')
    ## Calculate the reserved jump.
    para.new=eBeta4(c(intvl,r.new,(int*tuning)^2))
    ## Calculate the two pieces of ratios in MCMC: the ratio of llhd and that of the
    ## Jumping density
    rat.llhd=-sum(diag((dat)%*%solve(cor.new)%*%t(dat)/2))-log(det(cor.new))/2*nrow(dat)
    rat.llhd=exp(rat.llhd+sum(diag((dat)%*%solve(cor.cur)%*%t(dat))/2)+log(det(cor.cur))/2*nrow(dat))
    rat.jump=0
    if(sum(para.new[3:4]<0)==0) rat.jump=dBeta4(r.cur,para.new)/dBeta4(r.new,para.cur)
    rat=rat.llhd*rat.jump
    #cat(i,',',j,',',r.cur,',',r.new,para.cur,',',rat,'\n')
    ## accept all draws if beta parameters of current distribution are negative.
    #if(sum(para.cur[3:4]<0)>0) rat=1
    if(runif(1)<rat) cor.cur=cor.new
   }
  }
  return(cor.cur)
}
## calculate the elligible interval for a particular correlation coefficient.
corr.interval=function(corm,i,j)
{
  cor0=cor1=cor_1=corm
  cor0[i,j]=cor0[j,i]=0
  cor1[i,j]=cor1[j,i]=1
  cor_1[i,j]=cor_1[j,i]=-1
  f0=det(cor0)
  f1=det(cor1)
  f_1=det(cor_1)
  aa=(f1+f_1-2*f0)/2
  bb=(f1-f_1)/2
  cc=f0
  determ=sqrt(bb^2-4*aa*cc)
  return(sort(c((-bb-determ)/2/aa,(-bb+determ)/2/aa)))
}
## Estimate parameters of Beta 4 distribution,
## given min, max, mean, and variance.
eBeta4=function(param)
{
 rat1=(param[3]-param[1])/(param[2]-param[1])
 rat2=param[4]/(param[2]-param[1])^2
 alp=(1-rat1)*rat1^2/rat2-rat1
 bet=(1/rat1-1)*alp
 return(c(param[1:2],alp,bet))
}
## Randomly draw from the Beta4 distribution
## given min, max, alpha and beta.
rBeta4=function(n,param)
{
  return(rbeta(n,param[3],param[4])*(param[2]-param[1])+param[1])
}
## Return the density of Beta4 distribution
## given min, max, alpha and beta.
dBeta4=function(x,param)
{
  return(((x-param[1])^(param[3]-1))*((param[2]-x)^(param[4]-1))/beta(param[3],param[4])/((param[2]-param[1])^(param[3]+param[4]-1)))
}
## Test the above function of generating correlation matrix.
#n=100
#B=5000
#burnin=1000
#thin=2
#seql=seq(burnin,B,by=thin)
#covmat=matrix(c(1,-0.3,-0.3,1),2,2,byrow=T)
#cor.curr=covmat
#cor.estF=c()
#cor.estT=c()
#scal.ind=F
#tolern=1e-8
#for(sim in 1:50)
#{
# y=c()
# for(nsim in 1:n)
# {
# y=rbind(y,t(sim.mvnorm(0,covmat)))
# }
# corm=c()
# for(j in 1:B)
# {
#  corm=rbind(corm,vectorize(cor.curr))
#  cor.curr=sim.corr.mat(y,cor.curr,scal=F)
# }
# cor.estF=c(cor.estF,mean(corm[seql,2]))
# corm=c()
# for(j in 1:B)
# {
#  corm=rbind(corm,vectorize(cor.curr))
#  cor.curr=sim.corr.mat(y,cor.curr,scal=T)
# }
#  cor.estT=c(cor.estT,mean(corm[seql,2]))
#  cat(sim,'\n')
#}
# par(mfrow=c(1,2))
# hist(cor.estF)
# hist(cor.estT)
## Estimating a mixed effect model, x and z have to be matrices.
sim.mixed=function(y,x,alp,z,sig2.bet,sig2.eps,tune=0.01)
{
  n=nrow(x)
  px=ncol(x)
  pz=ncol(z)
  Sigma=z%*%t(z)*sig2.bet+diag(rep(1,n))*sig2.eps
  ######## Jump to a new set of variance parameters
  sig2.bet.new=exp(log(sig2.bet)+rnorm(1)*sqrt(tune))
  sig2.eps.new=exp(log(sig2.eps)+rnorm(1)*sqrt(tune))
  Sigma.new=z%*%t(z)*sig2.bet.new+diag(rep(1,n))*sig2.eps.new
  ######## Finish jumpimg to a new set of variance parameters
  Sigma.inv=solve2(Sigma)
  Sigma.new.inv=solve2(Sigma.new)
  Amat=solve(t(x)%*%Sigma.inv%*%x)
  alp.cur=sim.mvnorm(Amat%*%t(x)%*%Sigma.inv%*%y,Amat)
  ##############################################################################
  llhd=-sum(log(det2(Sigma))+t(y-x%*%alp.cur)%*%Sigma.inv%*%(y-x%*%alp.cur))/2
  llhd.new=-sum(log(det2(Sigma.new))+t(y-x%*%alp.cur)%*%Sigma.new.inv%*%(y-x%*%alp.cur))/2
  rat.llhd=exp(llhd.new-llhd)*(sig2.bet*sig2.eps/sig2.bet.new/sig2.eps.new)
  rat.jump=sig2.bet.new*sig2.eps.new/sig2.bet/sig2.eps
  rat=rat.jump*rat.llhd
  if(runif(1)<rat)
  {
   sig2.bet.cur=sig2.bet.new
   sig2.eps.cur=sig2.eps.new
  }
  ##############################################################################
  return(list(alp.new=alp.cur,sig2.bet.new=sig2.bet.cur,sig2.eps.new=sig2.eps.cur))
}
## Estimating a mixed effect model, x and z have to be matrices.
## in hierarchical Bayes framework, using Gibbs sampling
## We require that the combined matrix (x,z) is full column rank
sim.mixed.BHM=function(y,x,alp,z,bet,sig2.bet,sig2.eps)
{
  n=nrow(x)
  px=ncol(x)
  pz=ncol(z)
  ##############################################################################
  Amat=solve(diag(rep(1,pz))*(1/sig2.bet)+t(z)%*%z/sig2.eps)
  Bmat=solve(t(x)%*%x)
  ### Generate alpha and beta, respectively.
  alpcur=sim.mvnorm(Bmat%*%t(x)%*%(y-z%*%bet),Bmat*sig2.eps)
  betcur=sim.mvnorm(Amat%*%t(z)%*%(y-x%*%alpcur)/sig2.eps,Amat)
  ### Generate sigma^2_epsilon and sigma^2_beta, respectively.
  sig2.betcur=1/rgamma(1,pz/2-1/2,sum(betcur^2)/2)
  sig2.epscur=1/rgamma(1,n/2,sum((y-x%*%alpcur-z%*%betcur)^2)/2)
  ##############################################################################
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=sig2.epscur))
}
## Estimating a mixed effect model, x and z have to be matrices.
## in hierarchical Bayes framework, one Gibbs sampling step.
sim.mixed.2factor=function(y,x,alp,z,bet,sig2.bet,sig2.eps,dims)
{
  n=nrow(x)
  px=ncol(x)
  pz=sum(dims)
  ##############################################################################
  xz=as.matrix(cbind(x,z))
  Amat=solve(chol(diag(c(rep(0,px),rep(1,pz)))%*%diag(c(rep(0,px),rep(1/sig2.bet[1],dims[1]),rep(1/sig2.bet[2],dims[2])))+t(xz)%*%xz/sig2.eps))
  Amat=Amat%*%t(Amat)
  ### Generate alpha and beta, respectively.
  temp=sim.mvnorm(Amat%*%t(xz)%*%y/sig2.eps,Amat)
  alpcur=temp[1:px]
  betcur=temp[(px+1):(px+pz)]
  #cat(betcur,'\n')
  ### Generate sigma^2_epsilon and sigma^2_beta, respectively.
  sig2.epscur=1/rgamma(1,n/2,sum((y-x%*%alpcur-z%*%betcur)^2)/2)
  sig2.betcur=c(1,1)
  sig2.betcur[1]=1/rgamma(1,dims[1]/2-1/2,sum(betcur[1:dims[1]]^2)/2)
  sig2.betcur[2]=1/rgamma(1,dims[2]/2-1/2,sum(betcur[(dims[1]+1):pz]^2)/2)
  ##############################################################################
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=sig2.epscur))
}
################################################################################
## MLE estimate of parameters for random effects model with 2 factors.
MLE.mixed.2factor=function(y,xmat,zmat,dims)
{
  sig2.eps=sum(lm(y~xmat)$residuals^2)/lm(y~xmat)$df.residual
  sig2.bet=rep(sig2.eps,2)
  logll.profile=function(sig2)
  {
    sig2[sig2<=0]=1e-9
    vmat=solve(zmat%*%diag(as.vector(c(rep(sig2[1],dims[1]),rep(sig2[2],dims[2]))))%*%t(zmat)+diag(rep(sig2[3],nrow(zmat))))
    projmat=xmat%*%solve(t(xmat)%*%(vmat)%*%xmat)%*%t(xmat)%*%(vmat)
    return((-sum(log(eigen(vmat)$values))+t(y)%*%(vmat)%*%(diag(rep(1,nrow(zmat)))-projmat)%*%y)/2)
  }
  #sig2est=optim(c(sig2.bet,sig2.eps/5),logll.profile,method="BFGS",control=list(maxit=1))$par
  #sig2est=c(580.8064,580.7948,124.5377)
  vinv=solve(zmat%*%diag(c(rep(sig2est[1],dims[1]),rep(sig2est[2],dims[2])))%*%t(zmat)+diag(rep(sig2est[3],nrow(zmat))))
  alp=solve(t(xmat)%*%vinv%*%xmat)%*%t(xmat)%*%vinv%*%y
  BLUP=diag(c(rep(sig2est[1],dims[1]),rep(sig2est[2],dims[2])))%*%t(zmat)%*%vinv%*%(y-xmat%*%alp)
  return(list(alp.init=alp,bet.init=BLUP,sig2.eps=sig2est[3],sig2.bet=sig2est[1:2]))
}
################################################################################
## Spatially varying coefficient simple linear regression.
Spat.varycoef=function(y,x,lonlat,nknots=max(20,min(length(y),150)))
{
##
}
##############################################################################
## Estimating a mixed effect model, x and z have to be matrices.
## in hierarchical Bayes framework, one Gibbs sampling step.
sim.mixed.multifactor=function(y,x,alp,z,bet,sig2.bet,sig2.eps,dims)
{
  n=nrow(x)
  px=ncol(x)
  pz=sum(dims)
  ##############################################################################
  #x=Xmat
  #alp=alp.cur
  #z=Zmat
  #bet=bet.cur
  #sig2.bet=sig2.bet.cur
  #sig2.eps=sig2.eps.cur
  ##############################################################################
  xz=as.matrix(cbind(x,z))
  tmp=c()
  for(i in 1:length(dims))
  {
  tmp=c(tmp,rep(1/sig2.bet[i],dims[i]))
  }
  Amat=solve2((diag(c(rep(0,px),tmp))+xz2mat/sig2.eps))
  ### Generate alpha and beta, respectively.
  temp=sim.mvnorm(Amat%*%t(xz)%*%y/sig2.eps,Amat)
  alpcur=temp[1:px]
  betcur=temp[(px+1):(px+pz)]
  ### Generate sigma^2_epsilon and sigma^2_beta, respectively.
  sig2.epscur=1/rgamma(1,n/2,sum((y-x%*%alpcur-z%*%betcur)^2)/2)
  sig2.betcur=rep(1,length(dims))
  for(i in 1:length(dims))
  {
   if(i==1)
   {
   sig2.betcur[i]=1/rgamma(1,dims[i]/2-1/2,sum(betcur[1:dims[1]]^2)/2)
   }
   if(i>1) sig2.betcur[i]=1/rgamma(1,dims[i]/2-1/2,sum(betcur[(sum(dims[1:(i-1)])+1):sum(dims[1:i])]^2)/2)
  }
  ##############################################################################
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=sig2.epscur))
}
## Given a factor variable, generate the design matrix (matrix of 1 and 0's).
## input is a factor variable.
gen.design=function(fact,unik=sort(unique(fact)),cont=matrix(1,nrow=length(fact),ncol=1,byrow=T))
{
  n=length(fact)
  p=length(unik)
  mat=matrix(0,nrow=n,ncol=p,byrow=T)
  for(j in 1:p)
  {
   mat[fact==unik[j],j]=1
  }
  #print(table(fact))
  #cat(dim(cont),"\n")
  fact.mat=mat
  mat=c()
  for(k in 1:ncol(cont))
   {
     mat=cbind(mat,fact.mat*cont[,k])
   }
  return(list(design=mat,nam=unik))
}
################################################################################
## generate cuts on a continuous variable; 10 cuts by default.
gen.cut=function(contin,ncut=10)
{
  #ncut=10
  #cont=rnorm(1000)
  #cat(unique(c(-Inf,quantile(cont,probs=(1:ncut)/(ncut))+1e-10)),'\n')
  return(as.factor(cut(contin,unique(c(-Inf,quantile(contin,probs=(1:ncut)/(ncut))+1e-10)))))
}
################################################################################
## Matrix orthogonalization.
orthogonalize=function(X1,X2)
{
  n=nrow(X2)
  Imat=diag(rep(1,n))
  return((Imat-X1%*%solve(t(X1)%*%X1)%*%t(X1))%*%X2)
}
################################################################################
## Initial estimates of parameters for random effects model with 2 factors.
Init.mixed.multifactor=function(y,xmat,zmat,dims,ridge=F,lambda=0)
{
  p1=ncol(xmat)
  p2=ncol(zmat)
  if(!ridge) alpbetcur=solve2(xz2mat)%*%t(cbind(xmat,zmat))%*%y
  if(ridge) alpbetcur=solve2(xz2mat+diag(c(rep(0,p1),rep(lambda,p2))))%*%t(cbind(xmat,zmat))%*%y
  alp=alpbetcur[1:p1]
  betcur=alpbetcur[(p1+1):(p2+p1)]
  sig2.betcur=rep(1,length(dims))
  for(i in 1:length(dims))
  {
   if(i==1)
   {
   sig2.betcur[i]=median(betcur[1:dims[1]]^2)
   next
   }
   sig2.betcur[i]=median(betcur[(sum(1:(i-1))+1):sum(dims[1:i])]^2)
  }
  sig2est=sum(lm(y~xmat)$residuals^2)/lm(y~xmat)$df.residual
  return(list(alp.init=alp,bet.init=betcur,sig2.eps=sig2est,sig2.bet=sig2.betcur))
}
################################################################################
## Replicate a matrix l-times depending on the value of a certain factor.
replikat=function(mat,fact)
{
  unik=unique(fact)
  nrep=length(unik)
  p=ncol(mat)
  res=c()
  for(i in 1:(nrep))
  {
   xixi=mat
   xixi[fact!=unik[i],]=0
   res=cbind(res,xixi)
  }
  return(res)
}
################################################################################
## Calculate matrix outer product for two matrices of same # of rows.
mat.out=function(mat1, mat2)
{
  res=c()
  p1=ncol(mat1)
  p2=ncol(mat2)
  for(i in 1:p1)
  {
    res=cbind(res,mat1[,i]*mat2)
  }
  return(res)
}
################################################################################
## selection of the first few pcs that explain at least certain percent of total variation.
## Return rotation matrix and pc score.
prcomp.sel=function(x,perc=.95)
{
  pc=prcomp(x)
  sd2=cumsum(pc$sdev^2)/sum(pc$sdev^2)
  index=1:length(sd2)
  index.last=min((1:length(sd2))[sd2>perc])
  rot=pc$rotation[,1:index.last]
  return(list(num=index.last,rotation=rot,score=x%*%rot))
}
##############################################################################
## Estimating a mixed effect model, x and z have to be matrices.
## via MLE version of EM algorithm.
EM.mixed.multifactor=function(y,x,alp,z,bet,sig2.bet,sig2.eps,dims)
{
  ##############################################################################
  #x=Xmat
  #alp=alp.cur
  #z=Zmat
  #bet=bet.cur
  #sig2.bet=sig2.bet.cur
  #sig2.eps=sig2.eps.cur
  ##############################################################################
  n=nrow(x)
  px=ncol(x)
  pz=sum(dims)
  ##############################################################################
  xz=as.matrix(cbind(x,z))
  tmp=c()
  for(i in 1:length(dims))
  {
  tmp=c(tmp,rep(1/sig2.bet[i],dims[i]))
  }
  ### Generate alpha and beta, respectively.
  #vinv=solve2(z%*%diag(1/tmp)%*%t(z)+diag(rep(sig2.eps,n)))
  #alpcur=solve2(t(x)%*%vinv%*%x)%*%t(x)%*%vinv%*%y
  #betcur=diag(1/tmp)%*%t(z)%*%vinv%*%(y-x%*%alpcur)
  ### Generate alpha and beta, respectively.
  Amat=solve2((diag(c(rep(0,px),tmp))+xz2mat/sig2.eps))
  temp=Amat%*%t(xz)%*%y/sig2.eps
  alpcur=temp[1:px]
  betcur=temp[(px+1):(px+pz)]
  ### Generate sigma^2_epsilon, respectively.
  T1.epscur=sum((y-x%*%alpcur-z%*%betcur)^2)
  tempz=solve2(t(z)%*%z)
  T2.epscur=sum(diag(rep(sig2.eps,n)))-sig2.eps*sum(diag(tempz%*%solve2(tempz+diag(1/tmp)/sig2.eps)))
  sig2.epscur=(T1.epscur+T2.epscur)/n
  ## fix sig2.epscur
  #sig2.epscur=sig2.eps.arg
  ### Generate other variance components
  sig2.betcur=rep(1,length(dims))
  tempu=solve2(diag(tmp)+(t(z)%*%z)/sig2.eps)
  for(i in 1:length(dims))
  {
   colm=c()
   if(i==1)
   {
   colm=1:dims[1] #sig2.betcur[i]=1/rgamma(1,dims[i]/2-1/2,sum(betcur[]^2)/2)
   }
   if(i!=1)
   {
   colm=(sum(dims[1:(i-1)])+1):sum(dims[1:i])  #sig2.betcur[i]=1/rgamma(1,dims[i]/2-1/2,sum(betcur[]^2)/2)
   }
   #sig2.betcur[i]=sum(diag(t(z[,colm])%*%z[,colm]/sig2.eps+diag(rep(1/sig2.bet[i],dims[i])))+ betcur[colm]^2)/dims[i]
   sig2.betcur[i]=sum(mean(diag(tempu[colm,colm])) + betcur[colm]^2)/dims[i]
  }
  ## fix sig2.betcur
  sig2.betcur=sig2.bet.arg
  ##############################################################################
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=sig2.epscur))
}

################################################################################
## concatenate a vector of strings by the symbol specified in conn.
## e.g. string=c("a","b"), conn='-', then return "a-b".
concatenate=function(string,conn)
{
 n=length(string)
 string[1:(n-1)]=paste(string[1:(n-1)],conn,sep='')
 res=string[1]
 for(i in 2:n)
  res=paste(res,string[i],sep='')
  return(res)
}
################################################################################
## concatenate a vector of strings by the symbol specified in conn.
## e.g. string=c("a","b"), conn='-', then return "a-b".
concatenate=function(string,conn)
{
 n=length(string)
 string[1:(n-1)]=paste(string[1:(n-1)],conn,sep='')
 res=string[1]
 for(i in 2:n)
  res=paste(res,string[i],sep='')
  return(res)
}
################################################################################
## Estimating a mixed effect model, x and z have to be matrices.
## in hierarchical Bayes framework, one Gibbs sampling step.
sim.mixed.multifactor=function(y,x,alp,z,bet,sig2.bet,sig2.eps,dims)
{
  n=nrow(x)
  px=ncol(x)
  pz=sum(dims)
  ##############################################################################
  #x=Xmat
  #alp=alp.cur
  #z=Zmat
  #bet=bet.cur
  #sig2.bet=sig2.bet.cur
  #sig2.eps=sig2.eps.cur
  ##############################################################################
  xz=as.matrix(cbind(x,z))
  tmp=c()
  for(i in 1:length(dims))
  {
  tmp=c(tmp,rep(1/sig2.bet[i],dims[i]))
  }
  Amat=solve2((diag(c(rep(0,px),tmp))+xz2mat/sig2.eps))
  ### Generate alpha and beta, respectively.
  temp=sim.mvnorm(Amat%*%t(xz)%*%y/sig2.eps,Amat)
  alpcur=temp[1:px]
  betcur=temp[(px+1):(px+pz)]
  ### Generate sigma^2_epsilon and sigma^2_beta, respectively.
  sig2.epscur=1/rgamma(1,n/2,sum((y-x%*%alpcur-z%*%betcur)^2)/2)
  sig2.betcur=rep(1,length(dims))
  for(i in 1:length(dims))
  {
   if(i==1)
   {
   sig2.betcur[i]=1/rgamma(1,dims[i]/2-1/2,sum(betcur[1:dims[1]]^2)/2)
   }
   if(i>1) sig2.betcur[i]=1/rgamma(1,dims[i]/2-1/2,sum(betcur[(sum(dims[1:(i-1)])+1):sum(dims[1:i])]^2)/2)
  }
  ##############################################################################
  return(list(alp.new=alpcur,bet.new=betcur,sig2.bet.new=sig2.betcur,sig2.eps.new=sig2.epscur))
}
## Given a factor variable, generate the design matrix (matrix of 1 and 0's).
## input is a factor variable.
gen.design2=function(fact,cont=matrix(1,nrow=length(fact),ncol=1,byrow=T))
{
  n=length(fact)
  p=nlevels(fact)
  mat=matrix(0,nrow=n,ncol=p,byrow=T)
  unik=unique(fact)
  for(j in 1:p)
  {
   mat[fact==unik[j],j]=1
  }
   fact.mat=mat
   mat=c()
   for(k in 1:ncol(cont))
     {
       mat=cbind(mat,fact.mat*cont[,k])
     }
  return(mat)
}
################################################################################
## Initial estimates of parameters for random effects model with 2 factors.
Init.mixed.multifactor=function(y,xmat,zmat,dims,ridge=F,lambda=0)
{
  p1=ncol(xmat)
  p2=ncol(zmat)
  if(!ridge) alpbetcur=solve2(xz2mat)%*%t(cbind(xmat,zmat))%*%y
  if(ridge) alpbetcur=solve2(xz2mat+diag(c(rep(0,p1),rep(lambda,p2))))%*%t(cbind(xmat,zmat))%*%y
  alp=alpbetcur[1:p1]
  betcur=alpbetcur[(p1+1):(p2+p1)]
  sig2.betcur=rep(1,length(dims))
  for(i in 1:length(dims))
  {
   if(i==1)
   {
   sig2.betcur[i]=median(betcur[1:dims[1]]^2)
   next
   }
   sig2.betcur[i]=median(betcur[(sum(1:(i-1))+1):sum(dims[1:i])]^2)
  }
  sig2est=sum(lm(y~xmat)$residuals^2)/lm(y~xmat)$df.residual
  return(list(alp.init=alp,bet.init=betcur,sig2.eps=sig2est,sig2.bet=sig2.betcur))
}
################################################################################
## Replicate a matrix l-times depending on the value of a certain factor.
replikat=function(mat,fact)
{
  unik=unique(fact)
  nrep=length(unik)
  p=ncol(mat)
  res=c()
  for(i in 1:(nrep))
  {
   xixi=mat
   xixi[fact!=unik[i],]=0
   res=cbind(res,xixi)
  }
  return(res)
}
################################################################################
## Calculate matrix outer product for two matrices of same # of rows.
mat.out=function(mat1, mat2)
{
  res=c()
  p1=ncol(mat1)
  p2=ncol(mat2)
  for(i in 1:p1)
  {
    res=cbind(res,mat1[,i]*mat2)
  }
  return(res)
}
################################################################################
## selection of the first few pcs that explain at least certain percent of total variation.
## Return rotation matrix and pc score.
prcomp.sel=function(x,perc=.95)
{
  pc=prcomp(x)
  sd2=cumsum(pc$sdev^2)/sum(pc$sdev^2)
  index=1:length(sd2)
  index.last=min((1:length(sd2))[sd2>perc])
  rot=pc$rotation[,1:index.last]
  return(list(num=index.last,rotation=rot,score=x%*%rot))
}
################################################################################
## Extended function of diag matrix: creat block diagonal matrices.
diag.ext=function(mat1,mat2)
{
 p=nrow(mat1)+nrow(mat2)
 res=diag(rep(1,p))
 res[1:nrow(mat1),1:nrow(mat1)]=mat1
 res[(nrow(mat1)+1):p,(1+nrow(mat1)):p]=mat2
 return(res)
}
################################################################################
## Convert a matrix to a numeric matrix
as.numericmat=function(mat)
{
  p=ncol(mat)
  res=matrix(0,nrow=nrow(mat),ncol=ncol(mat),byrow=F)
  for(j in 1:p) res[,j]=as.numeric(mat[,j])
  return(res)
}
################################################################################
## return the value of a piecewise linear function evaluated at a vector
plot.piecelinear=function(x,fn)
{
  npieces=length(fn)/4
  value=x-x
  for(ipiece in 1:npieces)
  {
   fn.cur=fn[((ipiece-1)*4+1):(ipiece*4)]
   value=value+(x*fn.cur[4]+(fn.cur[3]))*(x> fn.cur[1] & x < fn.cur[2])
  }
  return(value)
}
################################################################################
## For a vector, find the first kn non-NA elements, and then apply certain function
## like mean, or min.
apply.NA=function(x,kn,fn)
{
  xsub=as.numeric(x[!is.na(x)])
  xsub=xsub[1:kn]
  res=c()
  if(fn=="mean") res=mean(xsub,na.rm=T)
  if(fn=="min") res=min(xsub,na.rm=T)
  if(fn=="sum") res=sum(xsub,na.rm=T)
  return(res)
}
################################################################################
### Remove the quotes in a matrix
remove.quote=function(mat)
{
  n=nrow(mat)
  return(matrix(as.numeric(mat),nrow=n,byrow=F))
}
################################################################################
### Estimating a varying coefficient regression model with a single covariate.
### Truncated polynomial spline basis,
VCM.est=function(y.v,x.v,t.v,nknots,degree=2,t.eval)
{
design.fixt=c()
design.fixteval=c()
tknots=quantile(t.v,probs=(1:nknots)/(nknots+1))
for(j in 0:degree)
{
 design.fixt=cbind(design.fixt,t.v^j)
 design.fixteval=cbind(design.fixteval,t.eval^j)
}
 Xmat=cbind(design.fixt,design.fixt*x.v)
 design.rdm=c()
 design.rdmeval=c()
for(j in 1:nknots)
{
 xixi=t.v-tknots[j]
 xixi[xixi<0]=0
 design.rdm=cbind(design.rdm,xixi^degree)
 xixi.eval=t.eval-tknots[j]
 xixi.eval[xixi.eval<0]=0
 design.rdmeval=cbind(design.rdmeval,xixi.eval^degree)
}
 Zmat=cbind(design.rdm,design.rdm*x.v)
 dims=c(nknots,nknots)
 nam=c(paste("z",1,".",1:dims[2],sep=''),paste("z",2,".",1:dims[2],sep=''))
 colnames(Zmat)=nam
 dat=data.frame(y=y.v,Zmat)
 dat$x=Xmat
 dat$all=rep(1,nrow(dat))
 lst=c()
 for(j in 1:length(dims))
 {
  lst=c(lst,list(pdIdent(formula(paste("~",concatenate(paste("z",j,".",1:dims[j],sep=''),'+'),"-1",sep='')))))
 }
 fit=lme(y~x-1,data=dat,random=list(all=pdBlocked(lst)))

  alp.est=fit$coef$fixed
  bet.est=fit$coef$random$all
  sig2u.est=(as.numeric(VarCorr(fit)[,2])[-length(as.numeric(VarCorr(fit)[,2]))])^2
  sig2e.est=(as.numeric(VarCorr(fit)[,2])[length(as.numeric(VarCorr(fit)[,2]))])^2
  ##############################################################################
  ## plot the estimated yields as well as true yields.
  yv.est=Xmat%*%alp.est+Zmat%*%as.vector(bet.est)
  ################################################################################
  ## Calculate model degrees of freedom.
  xzmat=cbind(Xmat,Zmat)
  di2=c(svd(xzmat)$d^2,rep(0,max(ncol(xzmat)-nrow(Xmat),0)))
  lambda=c(rep(0,ncol(Xmat)),sig2e.est/sig2u.est)
  DF=sum(di2/(di2+lambda))
  INPT=design.fixteval%*% alp.est[1:(degree+1)]+design.rdmeval%*%bet.est[1:nknots]
  SLOPE=design.fixteval%*% alp.est[(degree+2):length(alp.est)]+ design.rdmeval%*%bet.est[(1+nknots):length(bet.est)]
  res=list(fits=yv.est,curves=cbind(INPT,SLOPE),degf=DF)
  }
  ## test the vcm function
  #library(nlme)
  #n=100
  #int.est=c()
  #slope.est=c()
  #for(sim in 1:20)
  #{
  #tm=runif(n)
  #xv=rnorm(n)
  #yv=tm^2+ xv*tm+rnorm(n,0,0.1)
  #xixi=VCM.est(yv,xv,tm,nknots=4,t.eval=seq(0,1,length.out=101))
  #int.est=cbind(int.est,xixi$curves[,1])
  #slope.est=cbind(slope.est, xixi$curves[,2])
  #}
  #plot(seq(0,1,length.out=101),apply(int.est,1,mean),type='l')
  #lines(seq(0,1,length.out=101),seq(0,1,length.out=101)^2,col='red')
  #x11(); plot(seq(0,1,length.out=101),apply(slope.est,1,mean),type='l')
  #lines(seq(0,1,length.out=101),seq(0,1,length.out=101),col='red')
################################################################################
### Estimating a varying intercept regression model with a single covariate.
### Truncated polynomial spline basis,
VInt.est=function(y.v,x.v,t.v,nknots,degree=2,t.eval)
{
################
#y.v=cross.cur[,"SALES.UNITS"]
#x.v=cross.cur[,"PRICE"]-mean(cross.cur[,"PRICE"])
#t.v=cross.cur[,"WEEK.NO"]
#nknots=n.knots
#t.eval=weeks
#degree=2
################
design.fixt=c()
design.fixteval=c()
tknots=quantile(t.v,probs=(1:nknots)/(nknots+1))
for(j in 0:degree)
{
 design.fixt=cbind(design.fixt,t.v^j)
 design.fixteval=cbind(design.fixteval,t.eval^j)
}
 Xmat=cbind(design.fixt,x.v)
 design.rdm=c()
 design.rdmeval=c()
for(j in 1:nknots)
{
 xixi=t.v-tknots[j]
 xixi[xixi<0]=0
 design.rdm=cbind(design.rdm,xixi^degree)
 xixi.eval=t.eval-tknots[j]
 xixi.eval[xixi.eval<0]=0
 design.rdmeval=cbind(design.rdmeval,xixi.eval^degree)
}
 Zmat=cbind(design.rdm)
 dims=c(nknots)
 nam=c(paste("z",1,".",1:dims[1],sep=''))
 colnames(Zmat)=nam
 dat=data.frame(y=y.v,Zmat)
 dat$x=Xmat
 dat$all=rep(1,nrow(dat))

 fit=lme(y~x-1,data=dat,random=list(all=pdIdent(formula(paste("~",concatenate(paste("z",1,".",1:dims,sep=''),'+'),"-1",sep='')))))

  alp.est=fit$coef$fixed
  bet.est=fit$coef$random$all
  sig2u.est=(as.numeric(VarCorr(fit)[,2])[-length(as.numeric(VarCorr(fit)[,2]))])^2
  sig2e.est=(as.numeric(VarCorr(fit)[,2])[length(as.numeric(VarCorr(fit)[,2]))])^2
  ##############################################################################
  ## plot the estimated yields as well as true yields.
  yv.est=Xmat%*%alp.est+Zmat%*%as.vector(bet.est)
  ################################################################################
  ## Calculate model degrees of freedom.
  xzmat=cbind(Xmat,Zmat)
  di2=c(svd(xzmat)$d^2,rep(0,max(ncol(xzmat)-nrow(Xmat),0)))
  lambda=c(rep(0,ncol(Xmat)),sig2e.est/sig2u.est)
  DF=sum(di2/(di2+lambda))
  INPT=design.fixteval%*%alp.est[1:(degree+1)]+design.rdmeval%*%bet.est[1:nknots]
  SLOPE=alp.est[length(alp.est)]#design.fixteval%*% alp.est[(degree+2):length(alp.est)]+ design.rdmeval%*%bet.est[(1+nknots):length(bet.est)]
  res=list(fits=yv.est,curves=cbind(INPT,SLOPE),degf=DF)
  }
  ## test the vcm function
  #library(nlme)
  #n=100
  #int.est=c()
  #slope.est=c()
  #for(sim in 1:20)
  #{
  #tm=runif(n)
  #xv=rnorm(n)
  #yv=tm^2+ xv*tm+rnorm(n,0,0.1)
  #xixi=VCM.est(yv,xv,tm,nknots=4,t.eval=seq(0,1,length.out=101))
  #int.est=cbind(int.est,xixi$curves[,1])
  #slope.est=cbind(slope.est, xixi$curves[,2])
  #}
  #plot(seq(0,1,length.out=101),apply(int.est,1,mean),type='l')
  #lines(seq(0,1,length.out=101),seq(0,1,length.out=101)^2,col='red')
  #x11(); plot(seq(0,1,length.out=101),apply(slope.est,1,mean),type='l')
  #lines(seq(0,1,length.out=101),seq(0,1,length.out=101),col='red')
################################################################################
 ## Given vectors A and B, for each element of B, find its position in the vector A.
 find.position=function(vec.A,vec.B)
 {
   nA=length(vec.A)
   nB=length(vec.B)
   mat=(matrix(rep(vec.A,nB),nrow=nB,byrow=T)==vec.B)
   indx=matrix(rep(1:nA,nB),nrow=nB,byrow=T)
   return(t(indx)[t(mat)])
 }
 #A=c("1","2","3","4")
 #B=c("5","3")
 #find.rank(A,B)
 ################################################################################
### Sequantial regression based stochastic imputation.
## Both inputs should be matrices.
## x0 already contains the column of 1s.
SeqImpute=function(x0,x1)
{
  p.imp=ncol(x1)
  ## sort the columns of x1.
  order.imp=rank(apply(is.na(x1),2,sum),ties.method="random")
  indx.sort=(1:p.imp)[order.imp]
  x1.sort=x1[,order.imp]
  x.pred=x0
  x1.imp=c()
  for(p in 1:p.imp)
  {
    ## if the column to impute is complete, then go to next column;
    if(sum(is.na(x1.sort[,p]))==0)
    {
     x.pred=cbind(x.pred,x1.sort[,p])
     x1.imp=cbind(x1.imp,x1.sort[,p])
     next;
    }
    fit.lm=lm(x1.sort[,p]~x.pred-1)
    fit.sig=sqrt(sum(fit.lm$residuals^2)/fit.lm$df)
    imp.cur=x.pred%*%fit.lm$coef + rnorm(nrow(x0),0,fit.sig)
    imp.cur[!is.na(x1.sort[,p])]=x1.sort[!is.na(x1.sort[,p]),p]
    x1.imp=cbind(x1.imp,imp.cur)
  }
   x1.imp[,indx.sort]=x1.imp
   #cbind(x1,x1.imp)
   return(x1.imp)
}
################################################################################
## Function for conducting linear regression and log-linear regression.
## method="linear"
## method="const.loglinear"
## given thresh.low and thresh.high
## Constant nest-level parameter.
regression=function(y,x,N.vec=0,n.vec=0,market=NA,method="linear",nEM=5000,tolern=1e-6,opt='exp',nests=NA,off.set=0,param.init=0)
{
  res=lm(y~x-1)
  if(method=="const.loglinear")
  {
   res.new=lm(log(y)~x-1)
   coefs=res.new$coef
   if(coefs[2]>thresh.high | coefs[2]<thresh.low)
   {
    #y.tmp=log(y)-x[,-1]*thresh
    #xmat.tmp=cbind(1,-x[,-1])
    #reg=constrp(xmat.tmp,y.tmp,t(c(0,1)))
    #coefs=reg$bhat
    #coefs=c(coefs[1],thresh-coefs[2])
    tmp.fn=function(bet,y.tmp,x.tmp)
    {
    return(sum((log(y.tmp)-x.tmp%*%bet)^2))
    }
    coefs=optim(c(0,0),tmp.fn,method="L-BFGS-B",lower=c(-Inf,thresh.low),upper=c(Inf,thresh.high),y.tmp=y,x.tmp=x)$par
   }
   resids=y-exp(x%*%coefs)
   res$residuals=resids
   res$coef=coefs
  }
  if(method=="const.linear")
  {
   res.new=lm(y~x-1)
   coefs=res.new$coef
   if(coefs[2]>0) coefs=c(mean(y),0)
   resids=y-x%*%coefs
   res$residuals=resids
   res$coef=coefs
  }
  if(method=="loglinear")
  {
   res.new=lm(log(y)~x-1)
   coefs=res.new$coef
   resids=y-exp(x%*%coefs)
   res$residuals=resids
   res$coef=coefs
  }
  ## choice model for a given market indicator.
  if(method=="choice")
  {
   gamm.cur=rep(0,ncol(x))
   crit.cur=NA
   ##seg,Nvec,nvec,zmat,gamm.param
   for(iEM in 1:nEM)
    {
    #cat(iEM,'\n')
    tmp.cur=exp(x%*%gamm.cur)
    p.cur=tapply(tmp.cur,market,sum)/(tapply(tmp.cur,market,sum)+0)
    xixi=mlogit.IRLS(market,N.vec,n.vec,x,gamm.cur,oo=F,GAM=opt)
    cat(iEM,xixi$gamm,'\n')
    gamm.new=xixi$gamm
    #gamm.new[1]=gamm.new[1]-mean(apply(x%*%gamm.new,1,sum))
    crit.cur=xixi$crit
    if(sum(abs(gamm.new-gamm.cur))<tolern) break;
    gamm.cur=gamm.new
   }
    res$residuals=NA
    res$coef=gamm.new
    res$crit=(-2)*crit.cur
    #if(opt=='linear')
    #{
    #gamm.cur=c(mean(n.vec/N.vec),rep(0,ncol(x)-1))
    ### Likelihood criterion for choice model.
    #crit.linear=function(gamm)
    #{
    # atr=Attraction(x%*%gamm,option=opt)$val
    # prob.vec=atr/(group.fn(as.factor(market),atr,"sum")$results)
    # crit=(-2)*sum(n.vec*log(prob.vec))
    # return(crit)
    #}
    # res$coef=optim(gamm.cur,crit.linear)$par
    # res$crit=optim(gamm.cur,crit.linear)$value
    #}
  }
  ## Nested choice model for a given market indicator.
  if(method=="nest")
  {
   #gamm.cur=c(rep(0,ncol(x)-1))
   #tau.cur=rep(0,lengthunique(as.character(nests))-1)
   gamm.cur=as.vector(param.init[1:(ncol(x))])
   tau.cur=as.vector(param.init[-(1:(ncol(x)))])
   #if(homo) tau.cur=0
   #cat(length(c(gamm.cur,tau.cur)),'\n')
   ## Likelihood criterion for nested logit model.
   crit.nest=function(param)
   {
     cat(param,'\n')
     gamm=as.vector(param[1:(ncol(x))])
     tau=rep(as.vector(c(param[-(1:(ncol(x)))])),lengthunique(as.character(nests)))
     #tau[length(tau)]=0
     #tau=c(tau,0)
     tau.vec=rep(0,nrow(x))
     tau.vec=tau[nests]
     #cat(lengthunique(as.character(nests)),'\n')
     atr=Attraction((x%*%gamm+off.set)*exp(-tau.vec),option=opt)$val
     xixi=group.fn(as.factor(paste(market,nests,sep="-")),atr,"sum")
     #cat("okay1",'\n')
     prob.vec=atr/(xixi$results)
     I.vec=log(xixi$results)
     Qtemp.vec=exp(exp(tau.vec)*I.vec)
     Qtemp2.vec=Qtemp.vec/group.fn(as.factor(paste(market,nests,sep="-")),rep(1,length(atr)),"sum")$results
     Q.vec=Qtemp.vec/(group.fn(as.factor(market),Qtemp2.vec,"sum")$results)
     cat(summary(prob.vec),";",summary(Q.vec),'\n')
     crit=(-2)*sum(n.vec*log(prob.vec*Q.vec))
     #cat(crit,param,'\n')
     return(crit)
   }
   #nseq=20
   #param.mat=expand.grid(1,seq(-0.75,-0.25,length.out=nseq),seq(-0.1,.1,length.out=nseq),seq(-0.1,.1,length.out=nseq),0)
   #param.mat=cbind(param.mat,param.mat[,ncol(param.mat)])#,seq(-0.5,0.5,length.out=nseq))
   #cat(lengthunique(as.character(nests))-2,"\n")
   #value=c()
   #for(iseq in 1:nrow(param.mat))
   #{
   # cat(iseq/nrow(param.mat),'\n')
   # heihei=c(as.vector(unlist(param.mat[iseq,])))
    #cat(lengthunique(as.character(nests)),"\n")
    #cat(heihei,'\n')
   # value=c(value,crit.nest(heihei))
   #}
   #par(mfrow=c(2,3))
   #for(i in 1:ncol(param.mat))
   #{
   # plot(param.mat[,i],value,type='p',pch=19,cex=.5)
   #}
   cat(c(gamm.cur,tau.cur),'\n')
   res.cur=optim(c(gamm.cur,tau.cur),crit.nest,control=list(maxit=1000))
   res$coef=res.cur$par
   #res.new=res.cur$par+as.vector(rnorm(length(res.cur$par)))
   #cat("New param:",res.new,crit.nest(res.new),"\n")
   res$crit=res.cur$value
   res$conv=res.cur$convergence
   #res$val=value
  }
  return(res)
}
## examine the regression().
#x=cbind(1,rgamma(100,1,1))
#y=exp(x%*%c(2,4)+rnorm(100,0,0.1))
#thresh.high=3
#thresh.low=1
#regression(y,x,method="const.loglinear")
################################################################################
## Return the number of unique values of x.
lengthunique=function(x)
{
 return(length(unique(unlist(x))))
}
################################################################################
### Conduct a single split, given the vector and x represents the variable
### maxsplit represents the maximum number of splits, and minimum size of each node.
### returns a matrix.
### Error message: error=0; error=1: single level; error=2, all splits result in small nodes;
### error=3: no split will give enough reduction in SSE.
### crit.parent: the criterion for the parent node.
### crit.imp: the minimum improvement on the criterion in order to partition.
### where xmat and covariate must be matrices.
### Number of categories can't be too large for setparts().
#library(partitions)
library(sfsmisc)
split.var=function(xmat,node.cur,covariate,y,crit.parent,var.names,crit.min=50,maxsplit=400,minsize=20,reg.method="linear")
{
  covariate=as.matrix(covariate)
  crit.min.cur=crit.min
  maxsplit.cur=maxsplit
  minsize.cur=minsize
  split.rule=c()
  split.est=c()
  ## number of columns for xmat.
  p=ncol(xmat)
  nx=nrow(xmat)
  cat(node.cur,"size",nx,'\n')
  ## The index of elements in the node in the current data set
  index.cur=rep(node.cur,nx)
  crit.imp=0
  crit.leftnode=c()
  crit.rightnode=c()
  split.crit=c()
  for(j in 1:p)
  {
  x=xmat[,j]
  #cat(var.names[j],'\n')
  ## decide the variable type; splitting on a numerical variable.
  ## return the split points.
  if(is.numeric(x))
  {
   #cat("Numeric",'\n')
   x.cut=sort(unique(x))
   if(length(x.cut)==1) next;
   x.cut=(x.cut[-1]+x.cut[-length(x.cut)])/2
   ## the vector of cutoff values.
   x.cut.crude=x.cut
   if(length(x.cut)>(3*maxsplit))
    {
    tmp=round(seq(1,length(x.cut),length.out=maxsplit))
    x.cut.crude=x.cut[tmp]
    #cat(x.cut.crude,'\n')
    }
    for(i in 1:length(x.cut.crude))
    {
      split.cur=(x <= x.cut.crude[i])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      y.left=y[split.cur]
      covariate.left=as.matrix(covariate[split.cur,])
      ## use regression SSE
      crit.left=sum((regression(y.left,covariate.left,method=reg.method)$residuals)^2)
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      crit.right=sum((regression(y.right,covariate.right,method=reg.method)$residuals)^2)
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat(crit.imp.cur,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"|<=|{",x.cut.crude[i],"}",sep='')
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
      }
    }
  }
  ## decide on the variable type; splitting on a categorical variable: no more than 15 categories;
  ## return the association matrix of category values.
  if(is.factor(x) & !is.ordered(x))
  {
   #cat("Factor",'\n')
   ## if the number of levels of x is 1, then next variable.
   x=as.factor(as.character(x))
   if(nlevels(x)==1) next;
   tmp=matrix(c(T,F),ncol=1,byrow=T)
   nleve=lengthunique(x)
   cat(nleve,"\n")
   if(nleve > 40 | nleve <= 5)
   {
    #cat("Wrong category!","\n")
    if(nleve > 2 & nleve <= 5)
    {
    hoho=(1:(2^(nleve-1)-1))-1
    tmp=(rbind(1,digitsBase(hoho,base=2,nleve-1))==1)
    }
   ## If the number of levels is larger than 40, treat it as ordinal.
   if(nleve > 40)
   {
    tmp=c()
    if(max(table(x))<= minsize) next;
    x.unik=names(sort(table(x),decreasing=T))
    covariate.mean=as.vector(apply(covariate,2,mean))
    crit.est=rep(0,length(x.unik))
   for(kk in 1:length(x.unik))
   {
    cat(x.unik[kk],'\n')
    if(sum(x==x.unik[kk])>minsize)
   {
    crit.cur=sum(regression(y[x==x.unik[kk]],covariate[x==x.unik[kk],],method=reg.method)$coef*covariate.mean)
   }
    if(sum(x==x.unik[kk])<= minsize) crit.cur=sample(crit.est[crit.est!=0],1)
    crit.est[unique(x)==x.unik[kk]]=crit.cur
   }
   crit.cut=(sort(crit.est)[-1]+sort(crit.est)[-length(crit.est)] )/2
   for(kk in 1:(length(x.unik)-1))
   {
     tmp=cbind(tmp,crit.est<crit.cut[kk])
   }
   }
   for(i in 1:ncol(tmp))
   {
      split.cur=(x %in% unique(x)[tmp[,i]])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      y.left=y[split.cur]
      covariate.left=as.matrix(covariate[split.cur,])
      ## use regression SSE
      crit.left=sum((regression(y.left,covariate.left,method=reg.method)$residuals)^2)
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      crit.right=sum((regression(y.right,covariate.right,method=reg.method)$residuals)^2)
      crit.imp.cur=crit.parent-crit.left-crit.right
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"| in |{",paste(unique(x)[tmp[,i]],collapse=","),"}",sep='')
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
      }
    }
   }
   if(nleve> 5 & nleve<= 40)
   {
    #hoho=unique(floor(runif(15,0,1)*(2^(nleve-1)-1)))[1:5]
    tmp=(rbind(1,matrix(rbinom((nleve-1)*5,1,1/2),nrow=(nleve-1),byrow=T))==1)
    for(i in 1:ncol(tmp))
    {
      cat(i,"th initial point","\n")
      split.cur=(x %in% unique(x)[tmp[,i]])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      ## use regression SSE
      crit.left=sum((regression(y[split.cur],covariate[split.cur,],method=reg.method)$residuals)^2)
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      crit.right=sum((regression(y.right,covariate.right,method=reg.method)$residuals)^2)
      #cat("steep.descent",'\n')
      logic.v=steep.descent(tmp[,i],x,y,covariate,crit.parent-crit.left-crit.right,crit.parent,reg.method,minsize,crit.min)
      #cat("steep.descent:",logic.v,'\n')
      rm(split.cur)
      split.cur=(x %in% unique(x)[logic.v])
      ## use regression SSE
      crit.left=sum((regression(y[split.cur],covariate[split.cur,],method=reg.method)$residuals)^2)
      ## use regression SSE
      crit.right=sum((regression(y[!split.cur],covariate[!split.cur,],method=reg.method)$residuals)^2)
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat(xixi,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"| in |{",paste(unique(x)[logic.v],collapse=","),"}",sep='')
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
      }
    }
   }
  }
  if(is.ordered(x))
  {
   #cat("Ordinal",'\n')
   x.unik=(levels(x)[levels(x) %in% x])
   nleve=length(x.unik)
   if(nleve==1) next;
   tmp=matrix(rep(x,nleve-1),nrow=(nleve-1),byrow=T) < (x.unik[-1])
   for(i in 1:nrow(tmp))
    {
      #cat("Ordinal row:",i,'\n')
      split.cur=(tmp[i,])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      y.left=y[split.cur]
      covariate.left=as.matrix(covariate[split.cur,])
      ## use regression SSE
      crit.left=sum((regression(y.left,covariate.left,method=reg.method)$residuals)^2)
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      crit.right=sum((regression(y.right,covariate.right,method=reg.method)$residuals)^2)
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat(crit.imp.cur,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"| in |{",paste(x.unik[1:i],collapse=","),"}",sep='')
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
      }
    }
  }
 }
 cat(split.crit,"Improvement:",crit.imp,'\n')
 split.rule=split.crit
 if(crit.imp<= crit.min)
 {
  cat("Terminal node:",node.cur,'\n');
  fit.cur=regression(y,covariate,method=reg.method)
  return(list(error=3,model.est=rbind(split.est,t(c(as.numeric(node.cur),length(y),fit.cur$coef,sum(fit.cur$residuals^2))))))
 }
 if(crit.imp>crit.min)
 {## split left child node.
  subsat=(index.cur==paste(unique(node.cur),"1",sep=''))
  #cat("Diff in SSE:",sum((lm(y[subsat]~covariate[subsat,]-1)$residuals)^2) - crit.leftnode,'\n')
  reg.method2=reg.method
  split.fit=split.var(xmat[subsat,],paste(unique(node.cur),"1",sep=''),covariate[subsat,],y[subsat],crit.leftnode,var.names,crit.min=crit.min.cur,maxsplit=maxsplit.cur,minsize=minsize.cur,reg.method=reg.method2)
  if(split.fit$error==3) split.est=rbind(split.est,split.fit$model.est)
  if(split.fit$error==0)
  {
   index.cur[subsat]=split.fit$leaf.assoc
   split.rule=paste(split.rule,split.fit$split.krit,sep=';')
   split.est=rbind(split.est,split.fit$model.est)
  }
  #return(list(split.krit=split.crit,node.child=index.cur,error=0));
  }
  #cat("split.est:",split.est[,1],'\n')
 ## split right child node.
  subsat=(index.cur==paste(unique(node.cur),"0",sep=''))
  #cat("Diff in SSE:",sum((lm(y[subsat]~covariate[subsat,]-1)$residuals)^2) - crit.rightnode,'\n')
  reg.method2=reg.method
  split.fit=split.var(xmat[subsat,],paste(unique(node.cur),"0",sep=''),covariate[subsat,],y[subsat],crit.rightnode,var.names,crit.min=crit.min.cur,maxsplit=maxsplit.cur,minsize=minsize.cur,reg.method=reg.method2)
  if(split.fit$error==3) split.est=rbind(split.est,split.fit$model.est)
  if(split.fit$error==0)
  {
   index.cur[subsat]=split.fit$leaf.assoc
   split.rule=paste(split.rule,split.fit$split.krit,sep=';')
   split.est=rbind(split.est,split.fit$model.est)
  }
  return(list(error=0,leaf.assoc=index.cur,split.krit=split.rule,model.est=split.est))
}
################################################################################
## Find the predicted responses given feature matrix xmat, predictor matrix covariate,
## the split rule split.rule, and summary of leaves (leaf ID and coefficients).
split.predict=function(xmat,covariate,split.rule,leaf.summary,cols=2:ncol(leaf.summary))
{
  #cat(split.rule,'\n')
  if(is.na(split.rule)) return(list(assoc=rep("1",nrow(covariate)),pred=0,coefs=0,spt.rules=NA))
  #leaf.summary=res$model.est[,c(1,3,4)]
  #xmat=xixi[,1:3]
  #covariate=cbind(1,xixi$z)
  #split.rule=res$split.krit
  ##############################################################################
  #split.rules=matrix(NA,ncol=4,nrow=length(split.rule),byrow=T)
  split.rules=matrix(unlist(lapply(unlist(strsplit(split.rule[!is.na(split.rule)],";")),strsplit,"\\|")),ncol=4,byrow=T)
  leaf.assoc=rep("1",nrow(xmat))
  leaf.pred=rep(0,nrow(xmat))
  coef.mat=matrix(0,nrow=nrow(xmat),ncol=ncol(covariate),byrow=T)
  for(i in 1:nrow(split.rules))
  {
    #cat(i,split.rules[i,2],'\n')
    logic.v=rep(F,nrow(xmat))
    if(is.numeric(xmat[,split.rules[i,2]])) logic.v[xmat[,split.rules[i,2]]<= as.numeric(substr(split.rules[i,4],2,nchar(split.rules[i,4])-1))]=T
    if(is.factor(xmat[,split.rules[i,2]]))
    {
     #cat(unlist(strsplit(substr(split.rules[i,4],2,nchar(split.rules[i,4])-1),'\\,')),'\n')
     logic.v[xmat[,split.rules[i,2]] %in% unlist(strsplit(substr(split.rules[i,4],2,nchar(split.rules[i,4])-1),'\\,'))]=T
    }
    if(is.character(xmat[,split.rules[i,2]]))
    {
     #cat(unlist(strsplit(substr(split.rules[i,4],2,nchar(split.rules[i,4])-1),'\\,')),'\n')
     logic.v[xmat[,split.rules[i,2]] %in% unlist(strsplit(substr(split.rules[i,4],2,nchar(split.rules[i,4])-1),'\\,'))]=T
    }
    leaf.assoc[substr(leaf.assoc,1,nchar(split.rules[i,1]))==split.rules[i,1] & logic.v]=paste(split.rules[i,1],"1",sep="")
    leaf.assoc[substr(leaf.assoc,1,nchar(split.rules[i,1]))==split.rules[i,1] & (!logic.v)]=paste(split.rules[i,1],"0",sep="")
   }
  #print(split.rules)
  #print(leaf.assoc[1:68])
  ## handle exceptions, assign to the leading category.
  leaf.assoc[(leaf.assoc %in% split.rules[,1])]=names(table(leaf.assoc)[table(leaf.assoc)==max(table(leaf.assoc))])
  for(i in 1:nrow(leaf.summary))
  {
   #cat(leaf.summary[i,1],'\n')
   logic.v=rep(F,nrow(xmat))
   logic.v[as.numeric(leaf.assoc)==leaf.summary[i,1]]=T
   if(sum(logic.v)==0) next;
   hehe=covariate[logic.v,]
   if(sum(logic.v)==1) hehe=as.matrix(t(hehe))
   if(length(leaf.pred[logic.v]) != nrow(as.matrix(hehe))) cat(length(leaf.pred[logic.v]),nrow(as.matrix(hehe)) ,"Wierd!",'\n')
   #cat(,dim(as.matrix(covariate[logic.v,])),'\n')
   leaf.pred[logic.v]=as.matrix(hehe) %*% as.vector(leaf.summary[i,cols])
   #cat("heihei",sum(logic.v),length(logic.v),'\n')
   #print(coef.mat[logic.v,])
   #cat("heihei",'\n')
   #print(matrix(rep(leaf.summary[i,cols],sum(logic.v)),nrow=sum(logic.v),byrow=T))
   coef.mat[logic.v,]=matrix(rep(leaf.summary[i,cols],sum(logic.v)),nrow=sum(logic.v),byrow=T)
  }
  return(list(assoc=leaf.assoc,pred=leaf.pred,coefs=coef.mat,spt.rules=split.rules))
}
##############################################################################
## Given the vector of leaf associations, and model estimates, provide coefficients,
## as well as the predicted values.
split.estimate=function(leaves,covar,modelest,cols)
{
  #leaves=res$leaf.assoc
  #covar=as.matrix(cbind(1,xixi$z))
  #modelest=res$model.est
  #cols=3:4
  coef.mat=matrix(0,nrow=nrow(covar),ncol=length(cols),byrow=T)
  for(imod in 1:nrow(modelest))
  {
    imod.tmp=(as.numeric(leaves)==modelest[imod,1])
    coef.mat[imod.tmp,]=matrix(rep(modelest[imod,cols],sum(imod.tmp)),ncol=2,byrow=T)
  }
  return(list(pred=apply(coef.mat*covar,1,sum),coefs=coef.mat))
}
##############################################################################
## minimum size of each splitted node: minsize2;
## minimum reduction of the criterion: crit.min2
steep.descent=function(logic,x.cate,y.v,covariate.mat,delta.SSE,crit.parent,reg.method2,minsize2,crit.min2)
{
    #cat("\n")
    logic.optimal=logic.cur=logic
    crit.imp=delta.SSE
    nx=length(y.v)
    indic=F
    for(j in 2:length(logic))
    {
      logic.cur=logic
      logic.cur[j]=(!logic.cur[j])
      split.cur=(x.cate %in% unique(x.cate)[logic.cur])
      if(sum(split.cur)< minsize2| sum(split.cur)>(nx-minsize2)) next;
      if(prod(logic[2:length(logic)])) next;
      ## use regression SSE
      crit.left=sum((regression(y.v[split.cur],as.matrix(covariate.mat[split.cur,]),method=reg.method2)$residuals)^2)
      ## use regression SSE
      crit.right=sum((regression(y.v[!split.cur],as.matrix(covariate.mat[!split.cur,]),method=reg.method2)$residuals)^2)
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat(logic.cur,crit.imp.cur,"\n")
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min2)
      {
        crit.imp=crit.imp.cur
        logic.optimal=logic.cur
        indic=T
      }
    }
    if(!indic) return(logic);
    if(indic) logic.opt=steep.descent(logic.optimal,x.cate,y.v,covariate.mat,crit.imp,crit.parent,reg.method2,minsize2,crit.min2);
    return(logic.opt);
}
################################################################################
## Use least squares instead of IRLS to estimate the gating parameters.
mix.expert.LS=function(yv,xmat,smat,res,tolern=1e-6,nloop=2000)
{
  #tolern=1e-6
  #nloop=500
  #yv=xixi$y
  #xmat=cbind(1,xixi$z)
  #smat=cbind(gen.design(xixi$x1.cate),gen.design(xixi$x2.ord)[,-1],xixi$x3.cont)
  part=res$leaf.assoc
  res.est=res$model.est
  K=length(unique(part))
  bet.param=t(res.est[,c(3,4)])
  sig2.param=res.est[,5]/res.est[,2]
  npart=ncol(smat)
  #### initial value of gating parameters
  gamm.param=matrix(rnorm(K*ncol(smat),0,0.1),nrow=ncol(smat),byrow=T)
  gmat=c()
  for(ileaf in 1:K)
  {
   gmat=cbind(gmat,as.numeric(part)==res.est[ileaf,1])
  }
  gmat=gmat*(1/apply(gmat,1,sum))
  for(iloop in 1:nloop)
  {
  cat(iloop,'\n')
  tmp=xmat%*%bet.param
  denmat=c()
  for(ileaf in 1:K)
    denmat=cbind(denmat,dnorm(yv,tmp[,ileaf],sqrt(sig2.param[ileaf])))
  hmat=gmat*denmat
  hmat[hmat<(tolern^2)]=tolern^2
  hmat=hmat*(1/apply(hmat,1,sum))
  #### weighted least squares to update regression parameters.
  bet.new=bet.param
  gamm.new=gamm.param
  for(ileaf in 1:K)
  {
   tmp=lm(yv~xmat-1,weights=hmat[,ileaf])
   bet.new[,ileaf]=tmp$coef
   sig2.param[ileaf]=sum(tmp$residuals^2*hmat[,ileaf])/sum(hmat[,ileaf])
   gamm.new[,ileaf]=lm(log(hmat[,ileaf])~smat-1)$coef
  }
  cat(iloop,",",bet.new,",",'\n')
  #### LS to estimate gating parameters.
  if(max(abs(bet.new-bet.param))< tolern & max(abs(gamm.new-gamm.param))< tolern) break;
  gamm.param=gamm.new
  bet.param=bet.new;
  gmat=exp(smat%*%gamm.param)
  gmat[is.na(gmat)]=1/tolern
  gmat=gmat*(1/apply(gmat,1,sum))
  }
}
################################################################################
## Summarize the HME/ME results, by plotting the estimated curves,
#library(fields)
summarize.ME=function(yv,xv,bet.para,gmat.para,method='linear')
{
  #xv=xixi$z
  #bet.para=bet.param
  #gmat.para=gmat
  n=length(yv)
  nleaf=ncol(bet.para)
  par(mfrow=c(3,3))
  plot(xv,yv,type='p',pch='.',cex=2)
  x.grid=seq(quantile(xv,probs=c(0.01)),quantile(xv,probs=c(0.99)),length.out=100)
  for(ileaf in 1:nleaf)
  {
   if(method=='linear' | method=="const.linear") lines(x.grid,x.grid*bet.para[2,ileaf]+bet.para[1,ileaf],type='l',col='red')
   if(method=='poisson' | method=="const.poisson") lines(x.grid,exp(x.grid*bet.para[2,ileaf]+bet.para[1,ileaf]),type='l',col='red')
  }
  if(method=='linear'| method=="const.linear") fits=apply(t(t(gmat.para)*bet.para[1,]),1,sum)+apply(t(t(gmat.para)*bet.para[2,]),1,sum)*xv
  if(method=='poisson' | method=="const.poisson")
  {
   fits=apply(exp(matrix(rep(bet.para[1,],n),nrow=n,byrow=T)+matrix(rep(bet.para[2,],n),nrow=n,byrow=T)*as.vector(xv))*gmat.para,1,sum)
  }
  fits[fits<0]=0
  plot(xv,fits,ylim=range(yv),type='p',pch='.',cex=2)
  R2=round(1-sum((yv-fits)^2)/sum((yv-mean(yv))^2),3)
  plot(fits,yv,xlim=range(yv),type='p',pch='.',cex=2,xlab="Fit",ylab="Obs",main=paste("R2 =",R2,sep=''))
  abline(0,1,col='red')
  plot(fits,yv-fits,xlim=range(yv),type='p',pch='.',cex=2,xlab="Fit",ylab="Residuals")
  abline(h=0,col='red')
  plot(xv,yv-fits,type='p',pch='.',cex=2,xlab="xv",ylab="Residuals")
  hist(fits,main='Fitted values',xlab='Fitted values')
  hist(apply(t(t(gmat.para)*bet.para[1,]),1,sum),main="Intercept",xlab='Intercept')
  hist(apply(t(t(gmat.para)*bet.para[2,]),1,sum),main="Slope",xlab='Slope')
  return(list(coef=cbind(apply(t(t(gmat.para)*bet.para[1,]),1,sum),apply(t(t(gmat.para)*bet.para[2,]),1,sum)),Rsquare=R2))
}
################################################################################
## Summarize the HME/ME results, by plotting the surface plot of the weights.
summarize.ME2=function(yv,xv,bet.para,gmat.para)
{
  par(mfrow=c(3,2))
  nleaf=ncol(gmat.para)
  for(ileaf in 1:nleaf)
  {
   hist(gmat.para[,ileaf])
  }
  index=sample(1:length(yv),min(1000,length(yv)))
  for(ileaf in 1:nleaf)
  {
  if(ileaf %% 4 ==1) {x11(); par(mfrow=c(2,2))}
  fit=Tps(cbind(xv,yv)[index,],gmat.para[index,ileaf])
  out.p<-predict.surface(fit)
  plot.surface(out.p, type="C")
  }
}
################################################################################
## Use a recursive pass upward the tree to compute the h weights.
recursive.hmat=function(y,xmat,smat,terminal,res,gamm.est,iloop=1,node="root")
{
   #y=xixi$y
   #node="root"
   #iloop=1
   #xmat=x.mat
   #smat=s.mat
   #terminal=unique(res$leaf.assoc)
################################################################################
 res.est=res$model.est
 if(node !="root")
 {
  probmat=read.table("probmat.csv",header=T,sep=',')
 }
 if(node =="root")
 {
  probmat=c()
  for(inode in 1:nrow(res$model.est))
  {
   probmat=cbind(probmat,dnorm(y,xmat%*%res.est[inode,3:4],sqrt(res.est[inode,ncol(res.est)]/res.est[inode,2])))
  }
  colnames(probmat)=paste("PROB",res$model.est[,1],sep='.')
 }
 terminal.unk=unique(terminal)
 hv=c()
 collapse.opt=c()
 for(inod in 1:length(terminal.unk))
  {
  ## if the current leaf is not collapseable, then next;
  cur=terminal.unk[inod]
  bro=paste(substr(cur,1,nchar(cur)-1),as.character(1-as.numeric(substr(cur,nchar(cur),nchar(cur)))),sep="")
  siblings=c(cur,bro)
  parent=substr(cur,1,nchar(cur)-1)
  #cat(as.numeric(bro) %in% terminal.unk[1:length(terminal.unk)],'\n')
  if((as.numeric(bro) %in% terminal.unk[1:length(terminal.unk)])) break;
  }
  #cat(inod,'\n')
  #cat(cur,",",bro,'\n')
  #cat(terminal.unk,'\n')
  gamm.cur=gamm.est[gamm.est[,1]==as.numeric(parent),2:ncol(gamm.est)]
  ## if the current leaf is collapseable, then compute both hmat and probmat, save them in the file,
  ## and collapse the two nodes.
  expt=smat%*%gamm.cur
  expt[expt>100]=100
  expt[expt<(-100)]=(-100)
  g.cur=exp(expt)/(exp(expt)+1)
  xix=substr(res$leaf.assoc,1,nchar(cur))
  if(iloop==1) g.cur=sum(xix==paste(parent,"1",sep=""))/(sum(xix==cur)+sum(xix==bro))
  #cat(paste("PROB.",parent,"1",sep=""),"\n")
  #cat(node,"\n")
  hoho=g.cur*probmat[,paste("PROB.",parent,"1",sep="")]+(1-g.cur)*probmat[,paste("PROB.",parent,"0",sep="")]
  hoho[hoho < 1e-200]=1e-200
  #cat(paste("PROB.",parent,"1",sep=""),":",sum(is.na(hoho)),'\n')
  #if(sum(is.na(hoho))) cat("min.hoho",min(hoho),',',max(hoho),'\n')
  probmat=cbind(probmat,hoho)
  colnames(probmat)[ncol(probmat)]=paste("PROB",parent,sep='.')
  write.table(probmat,"probmat.csv",sep=',',row.names=F)
  #if(parent=="11") g.cur=1/2  ##############
  hv=cbind(g.cur*probmat[,paste("PROB.",parent,"1",sep="")]/hoho,1-g.cur*probmat[,paste("PROB.",parent,"1",sep="")]/hoho)
  colnames(hv)=c(paste("h.",parent,"1",sep=""),paste("h.",parent,"0",sep=""))
  hmat=as.matrix(hv)
  #cat(colnames(hmat),'\n')
  #if(parent=="10") write.table(cbind(probmat[,paste("PROB.",parent,"1",sep="")],hoho),"tmp.csv",sep=',',row.names=F)
  #cat(paste("PROB.",parent,"1",sep=""),":",sum(is.na(probmat[,paste("PROB.",parent,"1",sep="")])),"\n")
  #cat("g.cur",g.cur,"\n")
  #cat(paste("h.",parent,"1",sep=""),":",sum(is.na(hv[,1])),'\n')
  #cat(paste("h.",parent,"0",sep=""),":",sum(is.na(hv[,2])),'\n')
  if(node!="root") hmat=cbind(read.table("hmat.csv",sep=',',header=T),hv)
  write.table(hmat,"hmat.csv",sep=',',row.names=F)
  rm(hmat)
  ## collapse the nodes.
  tmp=c(cur,bro)
  terminal[terminal %in% tmp]=parent;
  if(length(unique(terminal))>1) recursive.hmat(y,xmat,smat,terminal,res,gamm.est,node="intermediate")
  #break;
  #}
 return(terminal)
}
################################################################################
## Given a set of terminal nodes, find all nodes in the tree.
Find.nodes=function(terminal)
{
 #terminal=unique(res$leaf.assoc)
 nodes=terminal
 for(inode in 1:length(terminal))
 {
  node.cur=terminal[inode]
  if(substr(node.cur,1,nchar(node.cur)-1) %in% terminal) next;
  nodes=c(nodes,substr(rep(node.cur,nchar(node.cur)),1,1:nchar(node.cur)))
 }
 return(unique(nodes))
}
################################################################################
## Given a set of terminal nodes, collapse the tree to have k nodes.
## Based on the weakest-branch pruning principle.
Collapse.nodes=function(terminal,k,y,xmat,crit.increase=Inf,reg.method="linear")
{
  terminal.unk=unique(terminal)
  if(length(terminal.unk)<= k) return(terminal)
  terminal.collapse=c()
  collapse.opt=c()
  for(inode in 1:length(terminal.unk))
  {
  ## if the current leaf is not collapseable, then next;
  cur=terminal.unk[inode]
  bro=paste(substr(cur,1,nchar(cur)-1),as.character(1-as.numeric(substr(cur,nchar(cur),nchar(cur)))),sep="")
  siblings=c(cur,bro)
  if(!(bro %in% terminal.unk[inode:length(terminal.unk)])) next;
  SSE.L=sum(regression(y[terminal==siblings[1]],xmat[terminal==siblings[1],],method=reg.method)$residuals^2)
  SSE.R=sum(regression(y[terminal==siblings[2]],xmat[terminal==siblings[2],],method=reg.method)$residuals^2)
  SSE.Parent=sum(regression(y[terminal %in% siblings],xmat[terminal %in% siblings,],method=reg.method)$residuals^2)
  #cat(cur,(SSE.Parent-(SSE.L+SSE.R)),'\n')
  #SSE.Parent=sum(lm(y~xmat-1,subset=(terminal %in% siblings))$residuals^2)
  if((SSE.Parent-(SSE.L+SSE.R)) <crit.increase)
  {
   crit.increase=(SSE.Parent-(SSE.L+SSE.R))
   collapse.opt=siblings[1]
  }
 }
 tmp=collapse.opt
 tmp=c(tmp, paste(substr(tmp,1,nchar(tmp)-1),as.character(1-as.numeric(substr(tmp,nchar(tmp),nchar(tmp)))),sep=""))
 terminal[terminal %in% tmp]=substr(tmp[1],1,nchar(tmp[1])-1)
 terminal.unk=unique(terminal)
 if(length(terminal.unk)>k) terminal=Collapse.nodes(terminal,k,y,xmat,crit.increase=Inf)
 return(terminal)
}
################################################################################
## After collapsing the nodes, refit the model
## The x must be a matrix.
Refit.tree=function(y,x,nodes,reg.method='linear')
{
  resu=c()
  for(inode in unique(nodes))
  {
    y.cur=y[nodes==inode]
    x.cur=x[nodes==inode,]
    tmp.reg=regression(y.cur,x.cur,method=reg.method)
    resu.cur=c(as.numeric(inode),sum(nodes==inode),as.vector(tmp.reg$coef),sum(tmp.reg$residuals^2))
    resu=rbind(resu,t(resu.cur))
  }
  return(resu)
}
################################################################################
## Compute the g. weights
comp.gmat=function(smat,node,gamm)
{
  #smat=s.mat
  #node="110"
  #gamm=gamm.est
  interm=as.numeric(substr(rep(node,nchar(node)-1),1,1:(nchar(node)-1)))
  powers=as.numeric(substr(rep(node,nchar(node)-1),2:nchar(node),2:(nchar(node))))
  resu=rep(1,nrow(smat))
  for(tmp0 in 1:length(interm))
  {
   interm.cur=interm[tmp0]
   g.tmp=(smat%*%(gamm[gamm[,1]==interm.cur,-1]))
   g.tmp[g.tmp>100]=100
   g.tmp[g.tmp<(-100)]=(-100)
   resu=resu*((exp(g.tmp)/(1+exp(g.tmp)))^powers[tmp0])*((1/(1+exp(g.tmp)))^(1-powers[tmp0]))
  }
  return(resu)
}
################################################################################
## One iteration of the IRLS algorithm for two-class logistic regression.
logit.IRLS=function(smat,hi,gamm,wt=as.vector(rep(1,length(hi))),lambda=0,tolern=1e-5)
{
  #hi=y
  #smat=x
  #gamm=c(2,2)
  #wt=as.vector(rep(1,length(hi)))
  tmp=smat%*%gamm
  tmp[tmp>100]=100
  tmp[tmp<(-100)]=(-100)
  gradient=apply(smat*as.vector(wt*(hi-exp(tmp)/(1+exp(tmp)))),2,sum)
  Hessian=t(smat)%*%(as.vector(wt*exp(tmp)/((1+exp(tmp))^2))*smat)
  diag(Hessian)=diag(Hessian)+1e-10
  increment=solve(Hessian)%*%gradient
  crit.cur=sum((log(exp(tmp)/(1+exp(tmp)))*hi+(1-hi)*log(1/(1+exp(tmp))))*wt)-lambda*sum(abs(gamm))
  increment.opt=NA
  for(iter in 1:15)
  {
   #cat("iter:",iter,",",increment,'\n')
   increment.cur=increment*(2^(-iter))
   if(sum(abs(increment.cur))<tolern) break;
   gamm.new=gamm+increment.cur
   #gamm.new[-1]=lasso(gamm.new[-1],lambda)
   tmp.new=smat%*%gamm.new
   tmp.new[tmp.new>100]=100
   tmp.new[tmp.new<(-100)]=(-100)
   crit.new=sum((log(exp(tmp.new)/(1+exp(tmp.new)))*hi+(1-hi)*log(1/(1+exp(tmp.new))))*wt)-lambda*sum(abs(gamm.new))
   if(crit.new>crit.cur)
   {
    crit.cur=crit.new
    increment.opt=increment.cur
    #cat("Successful iter:",iter,'\n')
   }
  }
  if(!is.na(increment.opt[1]))
  {
   gamm.new=gamm+increment.opt
   #gamm.new[-1]=lasso(gamm.new[-1],lambda)
   return(gamm.new)
  }
  return(gamm)
}
## test data.
#n=500
#x=cbind(1,rnorm(n))
#y=x%*%c(1,2)
#y=rbinom(n,1,exp(y)/(1+exp(y)))
#plot(x[,2],y,type='p')
#xixi=c(0,0)
#logit.IRLS(x,y,xixi)
#for(ite in 1:500)
#{
# xixi=as.vector(logit.IRLS(x,y,xixi))
# cat(xixi,'\n')
#}
#glm(y~x[,2],family =binomial)
################################################################################
## One iteration of the IRLS algorithm for multi-logistic regression.
mlogit.IRLS.slow=function(smat,hmat,gmat,gamm,tolern=1e-5)
{
  npart=ncol(smat)
  K=ncol(gmat)
  gradient=c()
  Hessian=matrix(0,nrow=(npart*(K-1)),ncol=(npart*(K-1)),byrow=T)
  for(ileaf in 1:(K-1))
  {
  gradient=c(gradient,apply((hmat-gmat)[,ileaf]*smat,2,sum))
  for(ileaf2 in 1:(K-1))
   {
    tmpmat=(-1)*t(smat)%*%(smat*gmat[,ileaf]*gmat[,ileaf2])
    if(ileaf2==ileaf)
    {
     tmpmat=t(smat)%*%(smat*gmat[,ileaf]*(1-gmat[,ileaf]))
    }
    Hessian[((ileaf-1)*npart+1):(ileaf*npart),((ileaf2-1)*npart+1):(ileaf2*npart)]=tmpmat
   }
  }
  diag(Hessian)=diag(Hessian)+(1e-10)
  chol.H=solve(chol(Hessian))
  increment=c(chol.H%*%t(chol.H)%*%gradient,rep(0,npart))
  gamm.v=as.vector(gamm)
  crit.cur=sum(log(gmat)*hmat)
  #cat(crit.cur,'\n')
  increment.opt=NA
  for(iter in 1:15)
  {
    increment.cur=increment*(2^(-iter))
    if(sum(abs(increment.cur))<tolern) break;
    gamm.new=matrix(gamm.v+increment.cur,ncol=K,byrow=F)
    gmat.new=exp(smat%*%gamm.new)
    gmat.new[gmat.new> 1e+10]=1e+10
    gmat.new[gmat.new< 1e-10]=1e-10
    gmat.new=gmat.new*(1/apply(gmat.new,1,sum))
    crit.new=sum(log(gmat.new)*hmat)
    #cat(crit.new,'\n')
    if(crit.new>crit.cur)
    {
    crit.cur=crit.new
    increment.opt=increment.cur
    }
  }
  if(!is.na(increment.opt[1])) gamm=matrix(gamm.v+increment.opt,ncol=K,byrow=F)
  return(gamm)
}
################################################################################
## Apply lasso penalty to estimated regression parameters
lasso=function(bet,lambda)
{
 hehe=abs(bet)-lambda
 hehe[hehe<0]=0
 return(sign(bet)*hehe)
}
################################################################################
## truncate a vector at [-100, 100].
truncat=function(x)
{
 x[x>100]=100
 x[x< (-100)]=-100
 return(x)
}
################################################################################
## One iteration of the IRLS algorithm for multi-logistic regression.
mlogit.IRLS.lasso=function(smat,hmat,gamm,Chol.Hess=F,lambda=0,tolern=1e-5,approxy=F,wt=rep(1,nrow(hmat)))
{
  gmat=exp(truncat(smat%*%gamm))/apply(exp(truncat(smat%*%gamm)),1,sum)
  K=ncol(gmat)
  gamm.new=gamm
  increment=c()
  for(ileaf in 1:K)
  {
   #plot(hmat[,ileaf]-gmat[,ileaf],type='p',pch='.',cex=2)
   gradient=apply(((hmat-gmat)[,ileaf]*wt)*smat,2,sum)
   #cat(apply(((hmat-gmat)[,ileaf]*wt)*smat,2,sum),apply(((hmat-gmat)[,ileaf])*smat,2,sum),'\n')
   if(Chol.Hess[1]==F)
   {
   Hessian=t(smat)%*%(smat*gmat[,ileaf]*(1-gmat[,ileaf])*wt)
   diag(Hessian)=diag(Hessian)+(1e-10)
   chol.H=solve(chol(Hessian))
   }
   if(Chol.Hess[1]!=F) chol.H=Chol.Hess
   increment=cbind(increment,(chol.H%*%t(chol.H)%*%gradient))
   }
  #cat(gradient,'\n')
  increment=increment/sqrt(sum((increment)^2))*2
  crit.cur=sum(log(gmat)*hmat*wt)-lambda*sum(abs(gamm))
  increment.opt=NA
  for(iter in 1:10)
  {
   increment.cur=increment*(2^(-iter))
   if(max(abs(increment.cur))<tolern) break;
   gamm.new=gamm+increment.cur
   gamm.new=lasso(gamm.new,lambda)
   gamm.new=gamm.new-apply(gamm.new,1,median)
   hoho=smat%*%gamm.new
   hoho[hoho> 100]=100
   hoho[hoho< (-100)]=(-100)
   gmat.new=exp(hoho)
   gmat.new=gmat.new*(1/apply(gmat.new,1,sum))
   crit.new=sum(log(gmat.new)*hmat*wt)-lambda*sum(abs(gamm.new))
   cat(iter,",",max(abs(increment.cur)),",",crit.new,'\n')
   if(crit.new>crit.cur)
   {
    crit.cur=crit.new
    increment.opt=increment.cur
   }
  }
  if(!is.na(increment.opt[1]))
  {
   gamm.new=gamm+increment.opt
   gamm.new=lasso(gamm.new,lambda)
   gamm.new=gamm.new-apply(gamm.new,1,median)
   return(list(gamm=gamm.new,chol.Hess=chol.H))
  }
  return(list(gamm=gamm,chol.Hess=chol.H))
}
## test data.
#n=1000
#x=cbind(1,rnorm(n))
#y=x%*%c(2,0.1)
#x.grid=seq(-3,3,length.out=601)
#y=rbinom(n,1,exp(y)/(1+exp(y)))
#xixi=c(2,0.1)
#gamm=cbind(xixi,c(0,0))
#gamm=xixi
#for(j in 1:20)
#{
# cat(j,",",gamm,'\n')
#gamm=(mlogit.IRLS.lasso(x,cbind(y,1-y), exp(x%*%gamm)/apply(exp(x%*%gamm),1,sum),gamm,lambda=0))
#gamm=logit.IRLS(x,y,gamm,lambda=0.1)
#}
#gamm
#gamm.glm=glm(y~x[,2],family = binomial)$coef
#gamm.glm
#plot(x[,2],y,type='p')
#lines(x.grid,exp(cbind(1,x.grid)%*%gamm[,1])/(1+exp(cbind(1,x.grid)%*%gamm[,1])),type='l',col='red')
#lines(x.grid,exp(cbind(1,x.grid)%*%gamm.glm)/(1+exp(cbind(1,x.grid)%*%gamm.glm)),type='l',col='blue')
################################################################################
### Hierarchical mixture of experts.
Hmix.expert.IRLS=function(y,x.mat,s.mat,res,lambda=0,tolern=1e-6,nloop=500)
{
 num.parents=length(Find.nodes(unique(res$leaf.assoc)))-length(unique(res$leaf.assoc))
 terminal.unk=unique(res$leaf.assoc)
 gamm.est=cbind(as.numeric(setdiff(Find.nodes(terminal.unk),terminal.unk)),matrix(rnorm(num.parents*ncol(s.mat)),nrow=num.parents,byrow=T))
 llhd=c()
 for(iloop in 1:nloop)
 {
 ## recursively pass upward the tree to generate hmatrix.
 ilop=iloop
 recursive.hmat(y,x.mat,s.mat,unique(res$leaf.assoc),res,gamm.est,ilop,node='root')
 cat("Finished recursive.",'\n')
 ## estimate the regression and variance parameters.
 hmat=read.table("hmat.csv",header=T,sep=',')
 probmat=read.table("probmat.csv",header=T,sep=',')
 llhd=c(llhd,sum(log(probmat[,"PROB.1"])))
 h.1=1
 hmat=cbind(hmat,h.1)
 sig2=c()
 size=c()
 bet.new=bet.cur=res$model.est[,-c(1,2,ncol(res$model.est))]
 for(inode in 1:nrow(res$model.est))
 {
  node.cur=as.character(res$model.est[inode,1])
  wt=apply(hmat[,paste("h.",substr(rep(node.cur,nchar(node.cur)),1,1:nchar(node.cur)),sep="")],1,prod)
  wt[wt<1e-12]=1e-12
  bet.new[inode,]=lm(y~x.mat-1,weights=wt)$coef
  sig2=c(sig2,sum(lm(y~x.mat-1,weights=wt)$residuals^2*wt))
  size=c(size,sum(wt))
 }
 res$model.est[,3:4]=bet.new
 res$model.est[,ncol(res$model.est)]=sig2
 res$model.est[,2]=size
 ## estimate the gating parameters.
 interm.node=setdiff((Find.nodes(unique(res$leaf.assoc))),(unique(res$leaf.assoc)))
 gamm.cur=gamm.est[,2:ncol(gamm.est)]
 gamm.new=c()
 for(inode in 1:length(interm.node))
 {
   node.cur=interm.node[inode]
   wt=rep(1,nrow(hmat))
   if(nchar(node.cur)>1) wt=apply(as.matrix(hmat[,paste("h.",substr(rep(node.cur,nchar(node.cur)-1),1,1:(nchar(node.cur)-1)),sep="")]),1,prod)
   wt[wt<1e-12]=1e-12
   gamm.tmp=gamm.est[inode,2:ncol(gamm.est)]
   for(kk in 1:10)
   {
   gamm.tmp=logit.IRLS(s.mat,hmat[,paste("h.",node.cur,'1',sep="")],gamm.tmp,wt,lambda=lambda,tolern=1e-5)
   }
  gamm.new=rbind(gamm.new,t(gamm.tmp))
 }
 cat(iloop,":",bet.cur,'\n')
 cat(iloop,":",max(abs(gamm.new-gamm.cur)),'\n')
 if(max(abs(bet.new-bet.cur))<tolern & max(abs(gamm.new-gamm.cur))<tolern) break;
 gamm.est[,2:ncol(gamm.est)]=gamm.new
 bet.cur=bet.new
 }
 #### Organize the results.
 gmat=c()
 for(inode in 1:length(terminal.unk))
 {
  node.cur=terminal.unk[inode]
  gmat=cbind(gmat,comp.gmat(s.mat,node.cur,gamm.est))
 }
 return(list(gamm=gamm.est,bet=t(bet.cur),gmat=gmat,sig2=sig2))
}
################################################################################
### Apply mixture of expert model with random starting point for gamm matrix,
### instead of estimated from the tree.
mix.expert.IRLS.rand=function(yv,xmat,smat,res,lambda=0,const.var=F,method="linear",tolern=1e-6,nloop=2000)
{
  #yv=aus.sales$SALES.UNITS
  #xmat=cbind(1,aus.sales$PRICE)
  #smat=xreg.cate
  #lambda=0
  #const.var=F
  #method="poisson"
  #tolern=1e-6
  #nloop=500
  #########
  #yv=xixi$y
  #xmat=cbind(1,xixi$z)
  #smat=cbind(gen.design(xixi$x1.cate),gen.design(xixi$x2.ord)[,-1],xixi$x3.cont)
  part=res$leaf.assoc
  K=length(unique(part))
  res.est=res$model.est
  bet.param=t(res$model.est[,-c(1,2,ncol(res$model.est))])
  sig2.param=res$model.est[,ncol(res$model.est)]/res$model.est[,2]
  npart=ncol(smat)
  #### initial value of gating parameters
  gamm.param=matrix(rnorm(K*ncol(smat),0,0.5),nrow=ncol(smat),byrow=T)
  gmat=c()
  for(ileaf in 1:K)
  {
   gmat=cbind(gmat,as.numeric(part)==res.est[ileaf,1])
  }
  hoho=smat%*%gamm.param
  hoho[hoho> 100]=100
  tmp=exp(hoho)
  cholH=F
  for(iloop in 1:nloop)
  {
  tmp=xmat%*%bet.param
  denmat=c()
  for(ileaf in 1:K)
    {
     if(method=="linear" | method=="const.linear")  denmat=cbind(denmat,dnorm(yv,tmp[,ileaf],sqrt(sig2.param[ileaf])))
     if(method=="poisson" | method=="const.poisson")
     {
      tmp2=tmp[,ileaf]
      tmp2[tmp2>100]=100
      tmp2[tmp2< (-100)]=-100
      haha=-exp(tmp2)+yv*tmp2-lfactorial(yv)
      denmat=cbind(denmat,exp(haha))
      }
    }
  hmat=gmat*denmat
  hmat[hmat< 1e-40]=1e-40
  hmat=hmat*(1/apply(hmat,1,sum))
  #### weighted least squares to update regression parameters.
  bet.new=bet.param
  if(method=="linear" | method=="const.linear")
  {
   SSE=c()
   denom=c()
   for(ileaf in 1:K)
   {
    tmp=lm(yv~xmat-1,weights=hmat[,ileaf])
    if(method=="const.linear" & tmp$coef[2]>0)
    {
     tmp$coef[2]=0
     tmp$coef[1]=sum(hmat[,ileaf]*yv)/sum(hmat[,ileaf])
    }
    bet.new[,ileaf]=tmp$coef
    tmp$residuals=yv-xmat%*%(tmp$coef)
    SSE=c(SSE,sum(tmp$residuals^2*hmat[,ileaf]))
    denom=c(denom,sum(hmat[,ileaf]))
   }
   sig2.param=SSE/denom
   sig2.param[sig2.param<tolern]=tolern
   if(const.var) sig2.param=rep(sum(SSE)/sum(denom),K)
  }
  if(method=="poisson")
  {
   bet.new=pois.IRLS(yv,xmat,hmat,bet.param)
  }
  if(method=="const.poisson")
  {
   for(iter in 1:5)
   bet.new=pois.IRLS.const(yv,xmat,hmat,bet.new,as.matrix(t(c(0,-1))))
  }
  cat(bet.new[1,],'\n')
  #### Estimate the gating parameters.
  gamm.param=gamm.param-apply(gamm.param,1,median)
  if(iloop %% 20 == 1) cholH=F
  gamm.cur=gamm.param
  for(mlogit.iter in 1:1)
  {
   #cat(sum(abs(hmat-gmat)),'\n')
   mlogt=mlogit.IRLS.lasso(smat,hmat,gamm.cur,Chol.Hess=cholH,lambda)
   if(max(abs(gamm.cur-mlogt$gamm))<tolern) break;
   gamm.cur=mlogt$gamm
  }
  gamm.new=mlogt$gamm
  cholH=mlogt$chol.Hess
  cat(iloop,",",max(abs(bet.new-bet.param)),",",max(abs(gamm.new-gamm.param)),'\n')
  if(max(abs(bet.new-bet.param)) < tolern & max(abs(gamm.new-gamm.param))<tolern) break;
  bet.param=bet.new
  gamm.param=gamm.new
  hoho=smat%*%gamm.param
  hoho[hoho> 100]=100
  hoho[hoho< (-100)]=-100
  gmat=exp(hoho)
  gmat=gmat*(1/apply(gmat,1,sum))
  }
  return(list(gamm=gamm.param,gmat=gmat,bet=bet.param,sig2=sig2.param))
}
################################################################################
## Iteratively reweighted least squares for poisson regression.
pois.IRLS=function(y,xmat,wt,bet.param,Fix.last.col=F)
{
   tmp2=xmat%*%bet.param
   tmp2[tmp2>100]=100
   tmp2[tmp2< (-100)]=-100
   crit.cur=apply((tmp2*as.vector(y)-exp(tmp2))*wt,2,mean)
   increment=c()
   K=ncol(bet.param)
   for(ileaf in 1:K)
   {
    increment=cbind(increment,solve(t(xmat)%*%(xmat*as.vector(wt[,ileaf]*exp(tmp2[,ileaf]))))%*%apply(xmat*as.vector((y-exp(tmp2[,ileaf]))*wt[,ileaf]),2,sum))
   }
   if(Fix.last.col) increment[,K]=0
   bet.opt=bet.param
   for(ibet in 1:10)
   {
    increment.cur=increment*(2^(-ibet))
    bet.new=bet.param+increment.cur
    tmp2=xmat%*%bet.new
    tmp2[tmp2>100]=100
    tmp2[tmp2< (-100)]=-100
    crit.new=apply((tmp2*as.vector(y)-exp(tmp2))*wt,2,mean)
    bet.opt[,crit.cur <crit.new]=bet.new[,crit.cur <crit.new]
    crit.cur[crit.cur <crit.new]=crit.new[crit.cur <crit.new]
   }
   #cat("Poisson:",bet.param,crit.cur,'\n')
   return(bet.opt)
}
################################################################################
# quadratic programming code: hinge algorithm in R
#  find theta to minimize t(theta)%*%qmat%*%theta-2*theta%*%cvec
#  subject to amat%*%theta >= 0
# qmat must be positive definite, amat must be irreducible
quadprog=function(cvec,qmat,amat){
	n=length(cvec)
	m=length(amat)/n
	sm=1e-10;h=1:m<0;obs=1:m;check=0
	umat=chol(qmat);uinv=solve(umat)
	delta=-amat%*%uinv
	y=t(uinv)%*%cvec
	b2=delta%*%y
	if(max(b2)>sm){
		i=min(obs[b2==max(b2)])
		h[i]=TRUE
	}else{check=1;theta=1:n*0}
	while(check==0){
		xmat=matrix(delta[h,],ncol=n)
		a=solve(xmat%*%t(xmat))%*%xmat%*%y
		if(min(a)<(-sm)){
			avec=1:m*0;avec[h]=a
			i=min(obs[avec==min(avec)])
			h[i]=FALSE;check=0
		}else{
			check=1
			theta=t(xmat)%*%a
			b2=delta%*%(y-theta)
			if(max(b2)>sm){
				i=min(obs[b2==max(b2)])
				h[i]=TRUE;check=0
			}
		}
	}
	bhat=uinv%*%(y-theta)
	return(bhat)
}
################################################################################
## Iteratively reweighted least squares for costrained poisson regression.
pois.IRLS.const=function(y,xmat,wt,bet.param,const.mat,fix.last.col=F)
{
   tmp2=as.matrix(xmat%*%bet.param)
   tmp2[tmp2>100]=100
   tmp2[tmp2< (-100)]=-100
   #crit.cur=apply(((tmp2*as.vector(y)-exp(tmp2))*wt),2,mean)
   #cat("crit.cur",'\n')
   increment=c()
   bet.opt=bet.param
   K=ncol(bet.param)
   if(fix.last.col) K=K-1
   for(ileaf in 1:K)
   {
    #cat("xixi",'\n')
    Qmat=t(xmat)%*%(xmat*exp(tmp2[,ileaf])*wt[,ileaf])
    Cvec=t(xmat*exp(tmp2[,ileaf])*wt[,ileaf])%*%(tmp2[,ileaf]-1+y/exp(tmp2[,ileaf]))
    #cat(Cvec,'\n')
    #cat(Qmat,'\n')
    #cat(const.mat,'\n')
    bet.opt[,ileaf]=quadprog(Cvec,Qmat,const.mat)
   }
   return(bet.opt)
}
## Generate poisson data.
#set.seed(100)
#n=1000
#xmat=cbind(1,rnorm(n))
#y=rpois(n,exp(xmat%*%c(1,1)))
#bet=as.matrix(c(0,0))
#for(iter in 1:10)
#{
# bet=pois.IRLS.const(y,xmat,rep(1,n),bet,as.matrix(t(c(0,-1))))
# cat(bet,'\n')
#}
#quadprog(c(0,1),matrix(c(1,2,2,5),nrow=2,byrow=F),as.matrix(t(c(1,1))))
################################################################################
## Compute BIC for Mixture of experts model.
calcBIC.ME=function(y,xmat,smat,fit,tolern=1e-8)
{
  n=length(y)
  gmat=exp(smat%*%fit$gamm)/apply(exp(smat%*%fit$gamm),1,sum)
  denmat=c()
  for(ileaf in 1:ncol(fit$bet))
  {
   denmat=cbind(denmat,dnorm(y,xmat%*%fit$bet[,ileaf],sqrt(fit$sig2[ileaf])))
  }
  bic.tmp=(-2)*sum(log(apply(denmat*gmat,1,sum)),na.rm=T)+log(n)*(length(fit$bet)+length(fit$sig2)+sum(abs(fit$gamm)>tolern))
  fits.tmp=apply(gmat*(xmat%*%fit$bet),1,sum)
  #cat((length(fit$bet)+length(fit$sig2)+sum(abs(fit$gamm)>tolern)),'\n')
  cat((-2)*sum(log(apply(denmat*gmat,1,sum)),na.rm=T),",",log(n)*(length(fit$bet)+length(fit$sig2)+sum(abs(fit$gamm)>tolern)),'\n')
  aic.tmp=(-2)*sum(log(apply(denmat*gmat,1,sum)),na.rm=T)+2*(length(fit$bet)+length(fit$sig2)+sum(abs(fit$gamm)>tolern))
  return(list(fits=fits.tmp,bic=bic.tmp,aic=aic.tmp))
}
####################################################################################
## Compute R2 for given predicted and actual.
calc.R2=function(pred, act)
{
  return(list(R2=1-sum((act-pred)^2)/sum((act-mean(act))^2),R1=1-sum(abs(act-pred))/sum(abs(act-mean(act)))))
}
################################################################################
### Apply type-B mixture of expert model with random starting point for gamm matrix
###
MoE.IRLS.Rob.B=function(yv,xmat,smat,res,lambda=0,const.var=F,method="linear",tolern=1e-6,nloop=2000,sig2.0=1e+6)
{
  part=res$leaf.assoc
  K=length(unique(part))
  res.est=res$model.est
  bet.param=cbind(t(res$model.est[,-c(1,2,ncol(res$model.est))]),c(0,0))
  sig2.param=c(res$model.est[,ncol(res$model.est)]/res$model.est[,2],sig2.0)
  npart=ncol(smat)
  #### initial value of gating parameters
  gamm.param=matrix(rnorm((K+1)*ncol(smat),0,0.5),nrow=ncol(smat),byrow=T)
  gmat=c()
  for(ileaf in 1:K)
  {
   gmat=cbind(gmat,as.numeric(part)==res.est[ileaf,1])
  }
  gmat=cbind(gmat,0)
  cholH=F
  for(iloop in 1:nloop)
  {
  tmp=xmat%*%bet.param
  denmat=c()
  for(ileaf in 1:(K+1))
    {
     if(method=="linear" | method=="const.linear")  denmat=cbind(denmat,dnorm(yv,tmp[,ileaf],sqrt(sig2.param[ileaf])))
     if(method=="poisson" | method=="const.poisson")
     {
      tmp2=tmp[,ileaf]
      tmp2[tmp2>100]=100
      tmp2[tmp2< (-100)]=-100
      denmat=cbind(denmat,dpois(yv,exp(tmp2)))
      }
    }
   hmat=gmat*denmat
   hmat[hmat< 1e-40]=1e-40
   hmat=hmat*(1/apply(hmat,1,sum))
   #### weighted least squares to update regression parameters.
   bet.new=bet.param
   if(method=="linear" | method=="const.linear")
   {
    SSE=c()
    denom=c()
    for(ileaf in 1:K)
   {
    tmp=lm(yv~xmat-1,weights=hmat[,ileaf])
    if(method=="const.linear" & tmp$coef[2]>0)
    {
     tmp$coef[2]=0
     tmp$coef[1]=sum(hmat[,ileaf]*yv)/sum(hmat[,ileaf])
    }
    bet.new[,ileaf]=tmp$coef
    tmp$residuals=yv-xmat%*%(tmp$coef)
    SSE=c(SSE,sum(tmp$residuals^2*hmat[,ileaf]))
    denom=c(denom,sum(hmat[,ileaf]))
   }
   sig2.param=SSE/denom
   sig2.param[sig2.param<tolern]=tolern
   sig2.param=c(sig2.param,sig2.0)
   if(const.var) sig2.param=rep(sum(SSE)/sum(denom),K)
  }
  if(method=="poisson")
  {
   bet.new=pois.IRLS(yv,xmat,hmat,bet.param,Fix.last.col=T)
  }
  if(method=="const.poisson")
  {
   for(iter in 1:5)
   bet.new=pois.IRLS.const(yv,xmat,hmat,bet.new,as.matrix(t(c(0,-1))),fix.last.col=T)
  }
  cat(bet.new[1,],'\n')
  #### Estimate the gating parameters.
  gamm.param=gamm.param-apply(gamm.param,1,median)
  if(iloop %% 20 == 1) cholH=F
  gamm.cur=gamm.param
  for(mlogit.iter in 1:1)
  {
   mlogt=mlogit.IRLS.lasso(smat,hmat,gamm.cur,Chol.Hess=cholH,lambda)
   if(max(abs(gamm.cur-mlogt$gamm))<tolern) break;
   gamm.cur=mlogt$gamm
  }
  gamm.new=mlogt$gamm
  cholH=mlogt$chol.Hess
  cat(iloop,",",max(abs(bet.new-bet.param)),",",max(abs(gamm.new-gamm.param)),'\n')
  if(max(abs(bet.new-bet.param)) < tolern & max(abs(gamm.new-gamm.param))<tolern) break;
  bet.param=bet.new
  gamm.param=gamm.new
  hoho=smat%*%gamm.param
  hoho[hoho> 100]=100
  hoho[hoho< (-100)]=-100
  gmat=exp(hoho)
  gmat=gmat*(1/apply(gmat,1,sum))
  cat(iloop,(-2)*sum(log(apply(denmat*gmat,1,sum)),na.rm=T),"\n")
  }
  return(list(gamm=gamm.param,gmat=gmat,bet=bet.param,sig2=sig2.param))
}
### Apply type-A mixture of expert model with random starting point for gamm matrix
MoE.IRLS.Rob.A=function(yv,xmat,smat,res,lambda=0,const.var=F,method="linear",tolern=1e-6,nloop=2000,sig2.0=1e+6)
{
  #yv=aus.sales$SALES.UNITS
  #xmat=cbind(1,aus.sales$PRICE)
  #smat=xreg.cate
  #lambda=0
  #const.var=F
  #method=ME.method
  #tolern=1e-6
  #nloop=500
  #########
  #yv=xixi$y
  #xmat=cbind(1,xixi$z)
  #smat=cbind(gen.design(xixi$x1.cate),gen.design(xixi$x2.ord)[,-1],xixi$x3.cont)
  part=res$leaf.assoc
  K=length(unique(part))
  res.est=res$model.est
  bet.null=c(0,0)
  sig2.null=sig2.0
  pi.cur=0.05
  h.null=rbinom(length(yv),1,pi.cur)
  bet.param=cbind(t(res$model.est[,-c(1,2,ncol(res$model.est))]))
  sig2.param=c(res$model.est[,ncol(res$model.est)]/res$model.est[,2])
  npart=ncol(smat)
  #### initial value of gating parameters
  gamm.param=matrix(rnorm(K*ncol(smat),0,0.5),nrow=ncol(smat),byrow=T)
  gmat=c()
  for(ileaf in 1:K)
  {
   gmat=cbind(gmat,as.numeric(part)==res.est[ileaf,1])
  }
  cholH=F
  for(iloop in 1:nloop)
  {
  pi.cur=mean(h.null)
  tmp=xmat%*%bet.param
  denmat=c()
  for(ileaf in 1:K)
    {
     if(method=="linear" | method=="const.linear")  denmat=cbind(denmat,dnorm(yv,tmp[,ileaf],sqrt(sig2.param[ileaf])))
     if(method=="poisson" | method=="const.poisson")
     {
      tmp2=tmp[,ileaf]
      tmp2[tmp2>100]=100
      tmp2[tmp2< (-100)]=-100
      denmat=cbind(denmat,dpois(yv,exp(tmp2)))
      }
    }
   hmat=gmat*denmat
   hmat[hmat< 1e-40]=1e-40
   h.null=(pi.cur*apply(hmat,1,sum))/(pi.cur*apply(hmat,1,sum)+(1-pi.cur)*dnorm(yv,bet.null,bet.null))
   hmat=hmat*(1/apply(hmat,1,sum))
   #### weighted least squares to update regression parameters.
   bet.new=bet.param
   if(method=="linear" | method=="const.linear")
   {
    SSE=c()
    denom=c()
    for(ileaf in 1:K)
   {
    tmp=lm(yv~xmat-1,weights=hmat[,ileaf]*h.null)
    if(method=="const.linear" & tmp$coef[2]>0)
    {
     tmp$coef[2]=0
     tmp$coef[1]=sum(hmat[,ileaf]*yv)/sum(hmat[,ileaf])
    }
    bet.new[,ileaf]=tmp$coef
    tmp$residuals=yv-xmat%*%(tmp$coef)
    SSE=c(SSE,sum(tmp$residuals^2*hmat[,ileaf]))
    denom=c(denom,sum(hmat[,ileaf]))
   }
   sig2.param=SSE/denom
   sig2.param[sig2.param<tolern]=tolern
   if(const.var) sig2.param=rep(sum(SSE)/sum(denom),K)
  }
  if(method=="poisson")
  {
   bet.new=pois.IRLS(yv,xmat,hmat*h.null,bet.param,Fix.last.col=T)
  }
  if(method=="const.poisson")
  {
   for(iter in 1:5)
   bet.new=pois.IRLS.const(yv,xmat,hmat*h.null,bet.new,as.matrix(t(c(0,-1))),fix.last.col=T)
  }
  cat(bet.new[1,],'\n')
  #### Estimate the gating parameters.
  gamm.param=gamm.param-apply(gamm.param,1,median)
  if(iloop %% 20 == 1) cholH=F
  gamm.cur=gamm.param
  for(mlogit.iter in 1:1)
  {
   mlogt=mlogit.IRLS.lasso(smat,hmat,gamm.cur,wt=h.null,Chol.Hess=cholH,lambda)
   if(max(abs(gamm.cur-mlogt$gamm))<tolern) break;
   gamm.cur=mlogt$gamm
  }
  gamm.new=mlogt$gamm
  cholH=mlogt$chol.Hess
  cat(iloop,",",max(abs(bet.new-bet.param)),",",max(abs(gamm.new-gamm.param)),'\n')
  if(max(abs(bet.new-bet.param)) < tolern & max(abs(gamm.new-gamm.param))<tolern) break;
  bet.param=bet.new
  gamm.param=gamm.new
  hoho=smat%*%gamm.param
  hoho[hoho> 100]=100
  hoho[hoho< (-100)]=-100
  gmat=exp(hoho)
  gmat=gmat*(1/apply(gmat,1,sum))
  cat(iloop,(-2)*sum(log(apply(denmat*gmat,1,sum)),na.rm=T),",",pi.cur,"\n")
  }
  return(list(gamm=gamm.param,gmat=gmat,bet=bet.param,sig2=sig2.param))
}
################################################################################
## One iteration of the Newton-Raphson algorithm for multi-logistic choice model, with multiple market segments.
mlogit.choice.IRLS=function(seg,Nvec,nvec,zmat,gamm.param,fixed=0,tolern=1e-5,oo=T,GAM="exp")
{
 #seg=period
 #Nvec=as.vector(matrix(rep(N.vec,M),nrow=M,byrow=T))
 #nvec=as.vector(t(n.mat))
 #zmat=z.mat
 #gamm.param=gamm.cur
 #seg=aus.sales$PERIOD.REG
 #Nvec=as.vector(N.vec)
 #nvec=as.vector(aus.sales$SALES.UNITS)
 #zmat=xreg.cate
 #gamm.param=gamm.cur
  M=length(seg)/length(unique(seg))
  dim.z=ncol(zmat)
  gamm.new=gamm.param
  if(oo) prob.vec=Attraction(zmat%*%gamm.param+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.param+fixed,GAM)$val,"sum")$results+1)
  if(!oo) prob.vec=Attraction(zmat%*%gamm.param+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.param+fixed,GAM)$val,"sum")$results)
  #cat(as.vector(Attraction(zmat%*%gamm.param+fixed,GAM)$gradlog),'\n')
  gradient=apply(zmat*as.vector(nvec-Nvec*prob.vec)*as.vector(Attraction(zmat%*%gamm.param+fixed,GAM)$gradlog),2,sum)
  ### Find the Hessian matrix.
  Hessian=matrix(0,dim.z,dim.z,byrow=T)
  for(iperiod in sort(unique(seg)))
  {
   period.cur=(seg==iperiod)
   temp.attr=Attraction(zmat%*%gamm.param+fixed,GAM)
   temp.vec=(nvec-Nvec*prob.vec)*(temp.attr$grad2log-(temp.attr$gradlog)^2)-(Nvec*prob.vec-Nvec*prob.vec^2)*(temp.attr$gradlog)^2
   Hessian=Hessian-t(zmat[period.cur,]) %*% diag(temp.vec[period.cur]) %*% zmat[period.cur,]
  }
  diag(Hessian)=diag(Hessian)+(1e-6)
  #print(eigen(Hessian))
  chol.H=solve(chol(Hessian))
  increment=chol.H%*%t(chol.H)%*%gradient
  #increment=increment/sqrt(sum(increment^2))
  if(oo) crit.cur=sum(nvec*log(prob.vec))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.vec,seg,sum)))
  if(!oo) crit.cur=sum(nvec*log(prob.vec))
  increment.opt=NA
  for(iter in 1:13)
  {
   increment.cur=increment*(2^(-iter+3))
   if(max(abs(increment.cur))<tolern) break;
   gamm.new=gamm.param+increment.cur
   if(oo) prob.new=Attraction(zmat%*%gamm.new+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.new+fixed,GAM)$val,"sum")$results+1)
   if(!oo) prob.new=Attraction(zmat%*%gamm.new+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.new+fixed,GAM)$val,"sum")$results)
   if(oo) crit.new=sum(nvec*log(prob.new))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.new,seg,sum)))
   if(!oo) crit.new=sum(nvec*log(prob.new))
   if(is.nan(crit.new)) cat(prob.new,'\n')
   #cat("mlogit",crit.new,crit.cur,'\n')
   if(crit.new> crit.cur)
   {
    crit.cur=crit.new
    increment.opt=increment.cur
    #cat(increment.cur,'\n')
   }
  }
  if(!is.na(increment.opt[1]))
  {
   gamm.new=gamm.param+increment.opt
   return(list(gamm=gamm.new,chol.Hess=chol.H))
  }
  #cat(mean(zmat%*%gamm.param),'\n')
  #gamm.param[1]=gamm.param[1]-mean(zmat%*%gamm.param)
  #cat("mlogit:",gamm,",",crit.cur,'\n')
  return(list(gamm=gamm.param,crit=crit.cur))
}
################################################################################
## One iteration of the IRLS algorithm for multi-logistic choice model, with multiple market segments.
## with shrinkage.
library(glmnet)
glmnetlogit.IRLS=function(seg,Nvec,nvec,zmat,gamm.param,fixed=0,tolern=1e-5,oo=T,GAM="exp",slopes=rep(0,nrow(zmat)),shrink.pen=0,shrink.val=(-0.002))
{
  #cat(shrink.pen,'\n')
  #M=length(seg)/length(unique(seg))
  #dim.z=ncol(zmat)
  #gamm.new=gamm.param
  gamm0=shrink.val-lm(slopes~zmat[,1:(ncol(zmat)/2)]-1)$coef
  gamm.param=gamm.param-gamm0
  #cat(gamm0,'\n')
  fixed=fixed+zmat%*%c(rep(0,ncol(zmat)/2),gamm0)
  if(oo) prob.vec=Attraction(zmat%*%gamm.param+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.param+fixed,GAM)$val,"sum")$results+1)
  if(!oo) prob.vec=Attraction(zmat%*%gamm.param+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.param+fixed,GAM)$val,"sum")$results)
  if(oo) crit.cur=(-2)*(sum(nvec/Nvec*log(prob.vec))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.vec,seg,sum))))
  if(!oo) crit.cur=(-2)*sum(nvec/Nvec*log(prob.vec))+shrink.pen*sum((slopes-shrink.val)^2)
  #cat((-2)*sum(nvec/Nvec*log(prob.vec)),sum((slopes-shrink.val)^2),'\n')
  pseudo=as.numeric(zmat%*%gamm.param+(nvec/Nvec-prob.vec)/(prob.vec*(1-prob.vec)))
  weights.cur=(prob.vec*(1-prob.vec))
  weights.cur=weights.cur/sum(weights.cur)
  xixi=glmnet(zmat,pseudo,weights=weights.cur,lambda=seq(1000,shrink.pen,length.out=50),penalty.factor=c(rep(1e-8,ncol(zmat)/2),apply(zmat[,1:(ncol(zmat)/2)],2,sum)),standardize=F,alpha=0)#$coef
  gamm.target=xixi$beta[,ncol(xixi$beta)]
  #cat(mean((pseudo-zmat%*%gamm.target)^2),'\n')
  #cat(crit.cur,'\n')
  for(itera in 1:10)
  {
  increment=(gamm.target-gamm.param)*(2^(1-itera))
  gamm.new=gamm.param + increment#(gamm.target-gamm.param)*(2^(1-itera))
  if(oo) prob.new=Attraction(zmat%*%gamm.new+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.new+fixed,GAM)$val,"sum")$results+1)
  if(!oo) prob.new=Attraction(zmat%*%gamm.new+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.new+fixed,GAM)$val,"sum")$results)
  slopes.new=slopes + zmat[,1:(ncol(zmat)/2)] %*% increment[(ncol(zmat)/2+1):ncol(zmat)]
  if(oo) crit.new=(-2)*(sum(nvec/Nvec*log(prob.new))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.new,seg,sum))))
  if(!oo) crit.new=(-2)*sum(nvec/Nvec*log(prob.new))+shrink.pen*sum((slopes.new-shrink.val)^2)
  if(crit.new< crit.cur | sum(abs((gamm.target-gamm.param)*(2^(1-itera))))< tolern)
  {
    gamm.param=gamm.new
    crit.cur=crit.new
    break;
  }
  }
  return(list(gamm=gamm.param+gamm0,crit=crit.cur))
}
################################################################################
## One iteration of the IRLS algorithm for multi-logistic choice model, with multiple market segments.
## with no shrinkage.
mlogit.IRLS=function(seg,Nvec,nvec,zmat,gamm.param,fixed=0,tolern=1e-5,oo=T,GAM="exp",slopes=rep(0,nrow(zmat)))#,shrink.pen=0,shrink.val=(-0.002))
{
  #cat(shrink.pen,'\n')
  #M=length(seg)/length(unique(seg))
  #dim.z=ncol(zmat)
  #gamm.new=gamm.param
  #cat(gamm0,'\n')
  fixed=fixed#+zmat%*%c(rep(0,ncol(zmat)/2),gamm0)
  if(oo) prob.vec=Attraction(zmat%*%gamm.param+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.param+fixed,GAM)$val,"sum")$results+1)
  if(!oo) prob.vec=Attraction(zmat%*%gamm.param+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.param+fixed,GAM)$val,"sum")$results)
  if(oo) crit.cur=(-2)*(sum(nvec/Nvec*log(prob.vec))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.vec,seg,sum))))
  if(!oo) crit.cur=(-2)*sum(nvec/Nvec*log(prob.vec))#+shrink.pen*sum((slopes-shrink.val)^2)
  pseudo=as.numeric(zmat%*%gamm.param+(nvec/Nvec-prob.vec)/(prob.vec*(1-prob.vec)))
  weights.cur=(prob.vec*(1-prob.vec))
  weights.cur=weights.cur/sum(weights.cur)
  xixi=lm(pseudo~zmat-1,weights=weights.cur)
  gamm.target=xixi$coef#beta[,ncol(xixi$beta)]
  #cat(gamm.target,'\n')
  #cat(mean((pseudo-zmat%*%gamm.target)^2),'\n')
  #cat(crit.cur,'\n')
  #cat(gamm.param,";",gamm.target,'\n')
  for(itera in 1:10)
  {
  increment=(gamm.target-gamm.param)*(2^(1-itera))
  gamm.new=gamm.param + increment#(gamm.target-gamm.param)*(2^(1-itera))
  if(oo) prob.new=Attraction(zmat%*%gamm.new+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.new+fixed,GAM)$val,"sum")$results+1)
  if(!oo) prob.new=Attraction(zmat%*%gamm.new+fixed,GAM)$val/(group.fn(as.factor(seg),Attraction(zmat%*%gamm.new+fixed,GAM)$val,"sum")$results)
  #slopes.new=slopes + zmat[,1:(ncol(zmat)/2)] %*% increment[(ncol(zmat)/2+1):ncol(zmat)]
  if(oo) crit.new=(-2)*(sum(nvec/Nvec*log(prob.new))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.new,seg,sum))))
  if(!oo) crit.new=(-2)*sum(nvec/Nvec*log(prob.new))#+shrink.pen*sum((slopes.new-shrink.val)^2)
  if(crit.new< crit.cur | sum(abs((gamm.target-gamm.param)*(2^(1-itera))))< tolern)
  {
    gamm.param=gamm.new
    crit.cur=crit.new
    break;
  }
  }
  return(list(gamm=gamm.param,crit=crit.cur))
}
################################################################################
## Convert the string-typed months to numbers, where 9-Jan is month 1.
month.convert=function(month)
{
 #month=c("9-FEB","10-JAN","8-DEC")
 months.order=c("JAN","FEB","MAR","APR","MAY","JUN","JUL","AUG","SEP","OCT","NOV","DEC")
 month=as.character(month)
 mon3=toupper(substr(month,nchar(month)-2,nchar(month)))
 year=(as.numeric(substr(month,1,nchar(month)-4))-9)*12
 mon=year-year
 for(imonth in 1:length(months.order))
 {
   mon[mon3==months.order[imonth]]=imonth
 }
  return(mon+year)
}
month.convert2=function(month)
{
 #month=c("FEB-09","JAN-10","DEC-08")
 months.order=c("JAN","FEB","MAR","APR","MAY","JUN","JUL","AUG","SEP","OCT","NOV","DEC")
 month=as.character(month)
 mon3=toupper(substr(month,1,3))
 year=(as.numeric(substr(month,nchar(month)-1,nchar(month)))-9)*12
 mon=year-year
 for(imonth in 1:length(months.order))
 {
   mon[mon3==months.order[imonth]]=imonth
 }
  return(mon+year)
}
#month=c("1/11/2012","12/20/2012")
month.convert3=function(month)
{
  #month=c("11/12/2012")
  res=unlist(lapply(month,strsplit,split="/"))
  mon3=as.numeric(res[(1:length(res)) %% 3==1])
  year=(as.numeric(res[(1:length(res)) %% 3==0])-2009)*12
  return(mon3+year)
}
## month="X11.26.2011"
month.convert4=function(month)
{
  #month=c("X11.26.2011","X1.2.2012")
  res=unlist(strsplit(month,split="\\."))
  months.order=c("JAN","FEB","MAR","APR","MAY","JUN","JUL","AUG","SEP","OCT","NOV","DEC")
  res[(1:length(res))%% 3 ==1 & substr(res,1,1)=="X"]=months.order[as.numeric(replace.char(res[(1:length(res))%% 3 ==1 & substr(res,1,1)=="X"],"X",""))]
  return(apply(matrix(res,ncol=3,byrow=T),1,paste,collapse="."))
}
## Assuming that each year has 12 months.
month.convert5=function(month)
{
  #month="2011-02"
  year=(as.numeric(substr(month,1,4))-min(as.numeric(substr(month,1,4))))*12
  month=as.numeric(substr(month,6,7))
  return(month+year+1)
}
#week=c("201102","201210")
## Assuming that each year has 53 weeks.
week.convert=function(week)
{
  #week="201102"
  year=(as.numeric(substr(week,1,4))-min(as.numeric(substr(week,1,4))))*53
  week=as.numeric(substr(week,5,6))
  return(week+year)
}
################################################################################
## For an arbitrary matrix, return the column name of the first and last non-NA entries.
## mat=Envy[,col.weeks]
avail=function(mat)
{
  mat.index=matrix(rep(1:ncol(mat),nrow(mat)),nrow=nrow(mat),byrow=T)
  xixi=mat.index*(mat/mat)
  mat[is.na(mat)]=0
  mat[mat.index< apply(xixi,1,min,na.rm=T)]=NA
  mat[mat.index> apply(xixi,1,max,na.rm=T)]=NA
  return(list(imputed=mat,mini=colnames(mat)[apply(xixi,1,min,na.rm=T)],maxm=colnames(mat)[apply(xixi,1,max,na.rm=T)]))
}
################################################################################
### Creat a tree with a fixed number of terminal nodes.
### Error message: error=0; error=1: single level; error=2, all splits result in small nodes;
### error=3: no split will give enough reduction in SSE.
### crit.parent: the criterion for the parent node.
### crit.imp: the minimum improvement on the criterion in order to partition.
### where xmat and covariate must be matrices.
### Number of categories can't be too large for setparts().
split.node=function(xmat,node.cur,covariate,y,crit.parent,var.names,crit.min=0,maxsplit=8,minsize=20,reg.method="linear",ncate.low=5,ncate.high=40,F_test=F,ncate.cont=10)
{
  covariate=as.matrix(covariate)
  maxsplit.cur=maxsplit
  minsize.cur=minsize
  split.rule=c()
  split.est=c()
  split.var=c()
  ## number of columns for xmat.
  p=ncol(xmat)
  nx=nrow(xmat)
  ## The index of elements in the node in the current data set
  index.cur=rep(node.cur,nx)
  crit.imp=0
  crit.leftnode=c()
  crit.rightnode=c()
  coef.leftnode=c()
  coef.rightnode=c()
  split.crit=c()
  imp.v=rep(0,p)
  split.opt=var.names[1]
  #cat(p,nx,split.opt,'\n')
  if(F_test & ncol(xmat)>1)
  {
   #cat(dim(xmat),'\n')
   #cat(is.numeric(xmat[,1]),is.numeric(xmat[,2]),is.numeric(xmat[,3]),'\n')
   split.opt=F.stat(y,xmat,covariate,Lcont=ncate.cont)$opt
  }
  #cat(split.opt,":",var.names,'\n')
  #cat(node.cur,"size",nx,'\n')
  for(j in 1:p)
  {
  #cat(var.names[j],'\n')
  if(F_test) {if(split.opt!=var.names[j]) next}
  x=xmat[,j]
  #cat(var.names[j],is.numeric(x),is.factor(x),is.ordered(x),'\n')
  ## decide the variable type; splitting on a numerical variable.
  ## return the split points.
  if(is.numeric(x))
  {
   x.cut=sort(unique(x))
   if(length(x.cut)==1) next;
   x.cut=(x.cut[-1]+x.cut[-length(x.cut)])/2
   ## the vector of cutoff values.
   x.cut.crude=x.cut
   if(length(x.cut)>(3*maxsplit))
    {
    tmp=round(seq(1,length(x.cut),length.out=maxsplit))
    x.cut.crude=x.cut[tmp]
    }
    for(i in 1:length(x.cut.crude))
    {
      split.cur=(x <= x.cut.crude[i])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      y.left=y[split.cur]
      covariate.left=as.matrix(covariate[split.cur,])
      ## use regression SSE
      reg.left=regression(y.left,covariate.left,method=reg.method)
      crit.left=sum((reg.left$residuals)^2)
      coef.left=reg.left$coef
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      reg.right=regression(y.right,covariate.right,method=reg.method)
      crit.right=sum((reg.right$residuals)^2)
      coef.right=reg.right$coef
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat("hehe",crit.imp.cur,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"|<=|{",x.cut.crude[i],"}",sep='')
        split.var=var.names[j]
        imp.v=rep(0,p)
        imp.v[j]=crit.imp.cur
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
        coef.leftnode=coef.left
        coef.rightnode=coef.right
      }
    }
  }
  ## decide on the variable type; splitting on a categorical variable: no more than 15 categories;
  ## return the association matrix of category values.
  if((!is.numeric(x)) & (!is.ordered(x)))
  {
   ## if the number of levels of x is 1, then next variable.
   x=as.character(x)
   tmp=matrix(c(T,F),ncol=1,byrow=T)
   nleve=lengthunique(x)
   if(nleve==1) next;
   #cat(nleve, ncate.high, ncate.low,'\n')
   if(nleve > ncate.high | nleve <= ncate.low)
   {
    #cat("Wrong category!","\n")
   if(nleve > 2 & nleve <=  ncate.low)
   {
    hoho=(1:(2^(nleve-1)-1))-1
    tmp=(rbind(1,digitsBase(hoho,base=2,nleve-1))==1)
   }
   ## If the number of levels is larger than 40, treat it as ordinal.
   if(nleve > ncate.high)
   {
    #cat(nleve, ncate.high, ncate.low,"second time",'\n')
    tmp=c()
    #if(max(table(x))<= minsize) next;
    x.unik=names(sort(table(x),decreasing=T))
    covariate.mean=as.vector(apply(covariate,2,mean))
    crit.est=rep(0,length(x.unik))
    #print(table(x))
    for(kk in 1:length(x.unik))
    {
    crit.cur=(-Inf)
    if(sum(x==x.unik[kk])>= minsize)
     {
     crit.cur=sum(regression(y[x==x.unik[kk]],covariate[x==x.unik[kk],],method=reg.method)$coef*covariate.mean)
     }
    #if(sum(x==x.unik[kk])< minsize) crit.cur=0#sample(crit.est[crit.est!=0],1)
    crit.est[unique(x)==x.unik[kk]]=crit.cur
   }
   ## Take a random ordering if all the crit.est are -Inf.
   if(sum(crit.est != (-Inf)) <1) {crit.est=sort(rnorm(length(crit.est))); }# cat("Random ordering",'\n')}
   crit.cut=(sort(crit.est)[-1]+sort(crit.est)[-length(crit.est)])/2
   #cat(minsize,":",crit.est,'\n')
   #cat(crit.cut,'\n')
   for(kk in 1:(length(x.unik)-1))
   {
     tmp=cbind(tmp,crit.est<crit.cut[kk])
   }
     #cat("Factor, ordered",'\n')
   }
   for(i in 1:ncol(tmp))
   {
      split.cur=(x %in% unique(x)[tmp[,i]])
      #cat(unique(x)[tmp[,i]],sum(split.cur),'\n')
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      y.left=y[split.cur]
      covariate.left=as.matrix(covariate[split.cur,])
      ## use regression SSE
      reg.left=regression(y.left,covariate.left,method=reg.method)
      crit.left=sum((reg.left$residuals)^2)
      coef.left=reg.left$coef
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      reg.right=regression(y.right,covariate.right,method=reg.method)
      crit.right=sum((reg.right$residuals)^2)
      coef.right=reg.right$coef
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat("xixi:",i,crit.imp.cur,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"| in |{",paste(unique(x)[tmp[,i]],collapse=","),"}",sep='')
        split.var=var.names[j]
        imp.v=rep(0,p)
        imp.v[j]=crit.imp.cur
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
        coef.leftnode=coef.left
        coef.rightnode=coef.right
      }
    }
   }
   if(nleve> ncate.low & nleve<= ncate.high)
   {
    #hoho=unique(floor(runif(15,0,1)*(2^(nleve-1)-1)))[1:5]
    tmp=(rbind(1,matrix(rbinom((nleve-1)*5,1,1/2),nrow=(nleve-1),byrow=T))==1)
    #cat(tmp,'\n')
    for(i in 1:ncol(tmp))
    {
      #cat(i,"th initial point","\n")
      split.cur=(x %in% unique(x)[tmp[,i]])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      ## use regression SSE
      #cat("steep.descent",'\n')
      #cat(unique(x)[tmp[,i]],'\n')
      #cat(sum(split.cur),'\n')
      crit.left=sum((regression(y[split.cur],covariate[split.cur,],method=reg.method)$residuals)^2)
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      crit.right=sum((regression(y.right,covariate.right,method=reg.method)$residuals)^2)
      logic.v=steep.descent(tmp[,i],x,y,covariate,crit.parent-crit.left-crit.right,crit.parent,reg.method,minsize,crit.min)
      #cat("steep.descent:",logic.v,'\n')
      rm(split.cur)
      split.cur=(x %in% unique(x)[logic.v])
      ## use regression SSE
      reg.left=regression(y[split.cur],covariate[split.cur,],method=reg.method)
      crit.left=sum((reg.left$residuals)^2)
      coef.left=reg.left$coef
      ## use regression SSE
      reg.right=regression(y[!split.cur],covariate[!split.cur,],method=reg.method)
      crit.right=sum((reg.right$residuals)^2)
      coef.right=reg.right$coef
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat(xixi,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"| in |{",paste(unique(x)[logic.v],collapse=","),"}",sep='')
        split.var=var.names[j]
        imp.v=rep(0,p)
        imp.v[j]=crit.imp.cur
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
        coef.leftnode=coef.left
        coef.rightnode=coef.right
      }
    }
   }
  }
  if(is.ordered(x))
  {
   cat("Ordinal",'\n')
   x.unik=(levels(x)[levels(x) %in% x])
   nleve=length(x.unik)
   if(nleve==1) next;
   tmp=matrix(rep(x,nleve-1),nrow=(nleve-1),byrow=T) < (x.unik[-1])
   for(i in 1:nrow(tmp))
    {
      #cat("Ordinal row:",i,'\n')
      split.cur=(tmp[i,])
      if(sum(split.cur)< minsize| sum(split.cur)>(nx-minsize)) next;
      y.left=y[split.cur]
      covariate.left=as.matrix(covariate[split.cur,])
      ## use regression SSE
      reg.left=regression(y.left,covariate.left,method=reg.method)
      crit.left=sum((reg.left$residuals)^2)
      coef.left=reg.left$coef
      y.right=y[!split.cur]
      covariate.right=as.matrix(covariate[!split.cur,])
      ## use regression SSE
      reg.right=regression(y.right,covariate.right,method=reg.method)
      crit.right=sum((reg.right$residuals)^2)
      coef.right=reg.right$coef
      crit.imp.cur=crit.parent-crit.left-crit.right
      #cat(crit.imp.cur,'\n')
      if(crit.imp.cur>crit.imp & crit.imp.cur>crit.min)
      {
        crit.imp=crit.imp.cur
        split.crit=paste(node.cur,"|",var.names[j],"| in |{",paste(x.unik[1:i],collapse=","),"}",sep='')
        split.var=var.names[j]
        imp.v=rep(0,p)
        imp.v[j]=crit.imp.cur
        index.cur[split.cur]=paste(unique(node.cur),"1",sep='')
        index.cur[!split.cur]=paste(unique(node.cur),"0",sep='')
        crit.leftnode=crit.left
        crit.rightnode=crit.right
        coef.leftnode=coef.left
        coef.rightnode=coef.right
      }
    }
  }
 }
 left.node=paste(unique(node.cur),"1",sep='')
 right.node=paste(unique(node.cur),"0",sep='')
 #cat(imp.v,'\n')
 modelest=rbind(t(c(as.numeric(left.node),sum(index.cur==left.node),coef.leftnode,crit.leftnode)),t(c(as.numeric(right.node),sum(index.cur==right.node),coef.rightnode,crit.rightnode)))
 return(list(imp.vec=imp.v,leaf.assoc=index.cur,split.krit=split.crit,Krit.imp=crit.imp,model.est=modelest,var.split=split.var))
}
################################################################################
## Fitting a simple tree with M nodes.
Simple.tree=function(x.mat,nodecur="1",covariate,y,crit.root,var.names,M=4,crit.min=0,maxsplit=8,minisize=20,regmethod="linear",ncate_low=5,ncate_high=40,F.test=F,L.cont=10)
{
 leaves.cur=nodecur
 split.rule=c()
 Leaf=rep(nodecur,nrow(x.mat))
 #cat(length(y),dim(x.mat),'\n')
 #cat(Leaf,'\n')
 node.summary=c()
 split.rule=c()
 spt.var=c()
 spt.imp=c()
 spt.impvec=rep(0,length(var.names))
 #cat(y,'\n')
 for(m in 1:(M-1))
 {
   crit.opt=0
   res.opt=c()
   spt.leaf=c()
   crit.opt=0
   for(ileaf in leaves.cur)
   {
    #cat(ileaf,'\n')
    crit.parent=node.summary[node.summary[,1]==as.numeric(ileaf),ncol(node.summary)]
    if(ileaf=="1") crit.parent=crit.root
    #cat("hehe",ileaf,'\n')
    #cat(ileaf,sum(Leaf==ileaf),'\n')
    #cat(ileaf,"before split",'\n')
    heihei=x.mat[Leaf==ileaf,]
    if(ncol(x.mat)<2)
     {
      heihei=as.matrix(heihei)
      #cat("Simple.tree",'\n')
      }
    #cat(ileaf,'\n')
    #cat(Leaf,'\n')
    #print(cbind(y,Leaf))
    res.tmp=split.node(heihei,ileaf,covariate[Leaf==ileaf,],y[Leaf==ileaf],crit.parent,var.names,minsize=minisize,reg.method=regmethod,ncate.low=ncate_low,ncate.high=ncate_high,F_test=F.test,ncate.cont=L.cont)
    #Krit.imp
    #cat(ileaf,"after split",'\n')
    #print(res.tmp$model.est)
    #cat(ileaf,'\n')
    #print(res.tmp$imp.vec)
    #cat("simple tree",ileaf,",",res.tmp$Krit.imp,'\n')
    if(res.tmp$Krit.imp> crit.opt)
      {
       spt.leaf=ileaf
       crit.opt=res.tmp$Krit.imp
       res.opt=res.tmp
      }
   }
   #cat(m,is.null(res.opt$split.krit),res.opt$split.krit,'\n')
   if(is.null(res.opt$split.krit)) break;
   Leaf[Leaf==spt.leaf]=res.opt$leaf.assoc
   if(m>1) split.rule=paste(split.rule,res.opt$split.krit,sep=";")
   if(m==1) split.rule=res.opt$split.krit
   spt.var=c(spt.var,res.opt$var.split)
   spt.imp=c(spt.imp,res.opt$Krit.imp)
   if(is.null(res.opt$imp.vec)) res.opt$imp.vec=0
   #cat(m,"nodes,",,res.opt$imp.vec,'\n')
   spt.impvec=spt.impvec+res.opt$imp.vec
   #cat("here:")
   #print(res.opt$model.est)
   node.summary=rbind(node.summary,res.opt$model.est)
   leaves.cur=unique(Leaf)
 }
 mode.est=node.summary[node.summary[,1] %in% as.numeric(leaves.cur),]
 if(sum(node.summary[,1] %in% as.numeric(leaves.cur))==0)
 {
  #cat("no splits,",'\n')
  split.rule=NA
  mode.est=matrix(c(1,length(y),0,0),nrow=1,byrow=T)
 }
 #cat("Finished M nodes",'\n')
 return(list(split.impvec=spt.impvec,split.imp=spt.imp,split.var=spt.var,leaf.assoc=Leaf,split.krit=split.rule,model.est=mode.est))
}
################################################################################
## Fitting a simple tree with M nodes.
 #res=Simple.tree(xixi[,1:3],"1",as.matrix(cbind(1,xixi$z)),xixi$y,crit.root,name.split,M,crit.min=0,minsize=20,reg.method="linear")
 #res.pred=split.predict(xixi[,1:3],as.matrix(cbind(1,xixi$z)),res$split.krit,res$model.est,3:4)
Boost.Tree=function(varPart,varReg,varY,name.split,M=4,nBoost=100,nu=0.1,fit.method="linear",mini.size=20,ncatelow=5,ncatehigh=20)
{
 resid.cur=varY-mean(varY)
 coef.Boost=varReg-varReg
 coef.Boost[,1]=mean(varY)
 #cat(mean(varY))
 #print(coef.Boost)
 var.imp=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 for(iBoost in 1:nBoost)
 {
  if(iBoost %% 9==1) {par(mfrow=c(3,3))}
  crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  res=Simple.tree(varPart,"1",varReg,resid.cur,crit.root,name.split,M,crit.min=0,minisize=mini.size,regmethod=fit.method,ncate_low=ncatelow,ncate_high=ncatehigh)
  res.pred=split.estimate(res$leaf.assoc,varReg,res$model.est,3:(ncol(varReg)+2))
  coef.increment=nu*res.pred$coefs
  var.imp=var.imp+(nu^2)*res$split.impvec
  coef.Boost=coef.Boost+coef.increment
  resid.cur=varY-apply(varReg*coef.Boost,1,sum)
  splt.modelest[[iBoost]]=res$model.est
  splt.crit=c(splt.crit,res$split.krit)
  plot(varY,apply(varReg*coef.Boost,1,sum),type='p',pch='.',cex=2,main=iBoost,ylab='fits')
  abline(0,1,col='red')
  R2.in=rbind(R2.in,t(unlist(calc.R2(apply(varReg*coef.Boost,1,sum),varY))))
  error.in=rbind(error.in,t(c(mean((apply(varReg*coef.Boost,1,sum)-varY)^2),mean(abs(apply(varReg*coef.Boost,1,sum)-varY)))))
  cat(iBoost,":",sum(resid.cur^2),'\n')
 }
 colnames(error.in)=c("L2","L1")
 return(list(variable.imp=var.imp,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,R2in=R2.in,f0.hat=mean(varY),error.train=error.in))
}
################################################################################
## Summary of the boosting results.
summary.Boost=function(boost,var.names,len=12,nam="Variable Importance")
{
 var.imp=matrix(boost$variable.imp/max(boost$variable.imp),nrow=1,byrow=T)
 colnames(var.imp)=substr(var.names,1,len)[order(var.imp,decreasing=T)]
 var.imp[1,]=sort(var.imp,decreasing=T)
 par(mfrow=c(1,2))
 hist(boost$coefs[,1],main='Intercept',freq=F,prob=T,xlab="Intercept")
 hist(boost$coefs[,2],main='Slope',freq=F,prob=T,xlab="Slope")
 #x11()
 pdf("VarImp.pdf")
 barplot(var.imp[,var.imp>0.1]*100,horiz =F,main=nam,col="red",las=2,cex.axis=0.8)
 #barplot(var.freq[,var.imp>0.1]*100,horiz =F,main=nam,col="red",las=2,cex.axis=0.8)
 dev.off()
 return(list(imp.vec=var.imp))
}
################################################################################
## Out-sample prediction based on boosting.
pred.Boost=function(varPart,varReg,varY,obj.Boost,cols.coef=3:(2+ncol(varReg)),nBoost,nu)
{
  pred.mat=rep(obj.Boost$f0.hat,length(varY))
  pred=obj.Boost$f0.hat
  R2.out=c()
  errorL2.test=c()
  errorL1.test=c()
  coef.mat=varReg-varReg
  coef.mat[,1]=obj.Boost$f0.hat
  for(iBoost in 1:nBoost)
  {
   spt.pred=split.predict(varPart,varReg,obj.Boost$split.krit[iBoost],obj.Boost$model.est[[iBoost]],cols=cols.coef)
   pred=pred+spt.pred$pred*nu
   pred.mat=cbind(pred.mat,pred)
   R2.out=c(R2.out,calc.R2(pred,varY)$R2)
   coef.mat=coef.mat+spt.pred$coefs*nu
   errorL2.test=c(errorL2.test,mean((pred-varY)^2))
   errorL1.test=c(errorL1.test,mean(abs(pred-varY)))
  }
  return(list(predictions=pred.mat,R2=R2.out,coefs=coef.mat,L2.error=errorL2.test,L1.error=errorL1.test))
  #return(list(R2=R2.out,coefs=coef.mat,L2.error=errorL2.test,L1.error=errorL1.test))
}
################################################################################
## Partial dependence plot.
 Partial.Boost=function(varPart,varReg,varY,name.var,var.levels=sort(unique(varPart[,name.var])))
  {
   #var.levels=sample(var.levels,min(length(var.levels),100),replace=F)
   #if(is.numeric(var.levels))
   #var.levels=as.factor(sort(var.levels))
   fits=matrix(varY-varY,nrow=length(varY),ncol=length(var.levels),byrow=T)
   varPart.cur=varPart
   for(ilevels.no in 1:length(var.levels))
   {
    ilevels=var.levels[ilevels.no]
    varPart.cur[,name.var]=ilevels
    cat(ilevels,'\n')
    pred.cur=pred.Boost(varPart.cur,cbind(1,varReg),varY,res,3:4,nBoost.param,nu=nu.param)
    cat(apply(pred.cur$coefs,2,mean),'\n')
    fits[,ilevels.no]=cbind(1,varReg)[,]%*%apply(pred.cur$coefs,2,mean)
  }
  return(fits)
  }
## Partial dependence plot.
 Partial.Boost.old=function(varPart,varReg,varY,name.var,var.levels=sort(unique(varPart[,name.var])))
  {
   #var.levels=sample(var.levels,min(length(var.levels),100),replace=F)
   #if(is.numeric(var.levels))
   #var.levels=as.factor(sort(var.levels))
   fits=varY-varY
   varPart.cur=varPart
   for(ilevels.no in 1:length(var.levels))
   {
    ilevels=var.levels[ilevels.no]
    varPart.cur[,name.var]=ilevels
    cat(ilevels,'\n')
    pred.cur=pred.Boost(varPart.cur,cbind(1,varReg),varY,res,3:4,nBoost.param,nu=nu.param)
    cat(apply(pred.cur$coefs,2,mean),'\n')
    fits[varPart[,name.var]==ilevels]=cbind(1,varReg)[varPart[,name.var]==ilevels,]%*%apply(pred.cur$coefs,2,mean)
  }
  return(fits)
  }
Partial.Boost.oldold=function(varPart,varReg,varY,name.var)
  {
   var.levels=unique(varPart[,name.var])
   var.levels=sample(var.levels,min(length(var.levels),100),replace=F)
   if(is.numeric(var.levels)) var.levels=sort(var.levels)
   tex=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P")
   resu=c()
   for(ilevels.no in 1:length(var.levels))#length(var.levels))#length(var.levels))
   {
    if(ilevels.no %% 16==1) {pdf("Partial.pdf"); par(mfrow=c(4,4))}
    ilevels=var.levels[ilevels.no]
    varPart.cur=varPart
    #cat(length(varReg[varPart.cur[,name.var]==ilevels,2]),",",length(varY[varPart.cur[,name.var]==ilevels]),'\n')
    #plot(varReg[varPart.cur[,name.var]==ilevels,2],varY[varPart.cur[,name.var]==ilevels],type='p',pch='.',cex=2,main=paste(name.var,"=",ilevels,sep=''),xlab='price',ylab='sales')
    plot(c(min(varReg),max(varReg)),c(min(varY),max(varY)),type='n',pch='.',cex=2,main=paste(name.var," ",tex[ilevels.no],sep=''),xlab='price',ylab='log(sales)')
    rug(varReg[varPart[,name.var]==var.levels[ilevels.no]],side=1)
    varPart.cur[,name.var]=ilevels
    pred.cur=pred.Boost(varPart.cur,cbind(1,varReg),varY,res,3:4,nBoost.param,nu=nu.param)
    abline(apply(pred.cur$coefs,2,mean),col='red',lwd=2)
    cat(c(ilevels,apply(pred.cur$coefs,2,mean)),'\n')
    if(ilevels.no %% 16==0) dev.off()
    resu=rbind(resu,t(c(ilevels,apply(pred.cur$coefs,2,mean))))
  }
  #dev.off()
  return(resu)
  }
################################################################################
## Given a tree object, find the next best cut.
Successive.tree=function(x.mat,tree,covariate,y,crit.root,var.names,crit.min=0,minisize=20,regmethod="linear")
{
 #x.mat=s.mat[index.train,]
 #tree=PartReg
 #covariate=as.matrix(cbind(1,x.vec)[index.train,])
 #y=y.vec[index.train]
 #var.names=names.part
 #crit.min=0
 #minisize=20
 #reg.method='linear'
 ### Retrieve information of the previous tree.
 node.summary=tree$model.est
 split.rule=tree$split.krit
 spt.var=tree$split.var
 spt.imp=tree$split.imp
 spt.impvec=tree$split.impvec
 Leaf=tree$leaf.assoc
 leaves.cur=unique(Leaf)
 ### Cycle thru the leaf nodes for best cut.
 crit.opt=0
 res.opt=c()
 spt.leaf=c()
 for(ileaf in leaves.cur)
   {
    crit.parent=node.summary[node.summary[,1]==as.numeric(ileaf),5]
    if(ileaf=="1") crit.parent=crit.root
    res.tmp=split.node(x.mat[Leaf==ileaf,],ileaf,covariate[Leaf==ileaf,],y[Leaf==ileaf],crit.parent,var.names,minsize=minisize,reg.method=regmethod)
    if(res.tmp$Krit.imp>crit.opt)
      {
       spt.leaf=ileaf
       crit.opt=res.tmp$Krit.imp
       res.opt=res.tmp
      }
   }
   Leaf[Leaf==spt.leaf]=res.opt$leaf.assoc
   split.rule=paste(split.rule,res.opt$split.krit,sep=";")
   spt.var=c(spt.var,res.opt$var.split)
   spt.imp=c(spt.imp,res.opt$Krit.imp)
   spt.impvec=spt.impvec+res.opt$imp.vec
   node.summary=rbind(node.summary,res.opt$model.est)
   leaves.cur=unique(Leaf)
 return(list(split.impvec=spt.impvec,split.imp=spt.imp,split.var=spt.var,leaf.assoc=Leaf,split.krit=split.rule,model.est=node.summary[node.summary[,1] %in% as.numeric(leaves.cur),]))
}
################################################################################
## In a vector of strings, replace char1 by char2
 replace.char=function(strvec,char1,char2)
  {
    lenvec=length(strvec)
    for(ivec in 1:lenvec)
     {
      strcur=strvec[ivec]
      charcur=unlist(strsplit(strcur,""))
      charcur[charcur==char1]=char2
      strvec[ivec]=paste(charcur,collapse="")
     }
    return(strvec)
  }
################################################################################
## Circle product of two matrices:
circle.prod=function(matA,matB)
{
 #matA=matrix(c(rnorm(4)),2,2,byrow=T)
 #matB=kronecker(matA,matA)
 n=nrow(matA)
 mat1=matA-matA+1
 mat=kronecker(matA,mat1)*matB
 res=matA-matA
 for(i in 1:n)
 {
  for(j in 1:n)
  {
    res=res+mat[((i-1)*n+1):(i*n),((j-1)*n+1):(j*n)]
  }
 }
 return(res)
}
################################################################################
## The sum of two SSEs in the splitted regression.
SSE.split=function(yvec,Xmat,svec,tauvec=rep(0.5,length(unique(svec))))
{
  ny=length(svec)
  sdesign=gen.design(svec)$design
  Wmat.comp=Wmat=matrix(0,ny,ny,byrow=T)
  #print(dim(Wmat))
  diag(Wmat)=sdesign%*%tauvec
  diag(Wmat.comp)=1-sdesign%*%tauvec
  #print(dim(Wmat))
  resid1=yvec-Xmat%*%solve(t(Xmat)%*%Wmat%*%Xmat)%*%t(Xmat)%*%Wmat%*%yvec
  resid2=yvec-Xmat%*%solve(t(Xmat)%*%Wmat.comp%*%Xmat)%*%t(Xmat)%*%Wmat.comp%*%yvec
  return(sum(resid1^2)+sum(resid2^2))
}
################################################################################
## The derivative of the sum of two SSEs in the splitted regression.
derv.split=function(yvec,Xmat,svec,tauvec=rep(0.5,length(unique(svec))))
{
  nx=length(svec)
  sdesign=gen.design(svec)$design
  Wmat.comp=Wmat=matrix(0,nx,nx,byrow=T)
  diag(Wmat)=sdesign%*%tauvec
  diag(Wmat.comp)=1-sdesign%*%tauvec
  solv=solve(t(Xmat)%*%Wmat%*%Xmat)
  solv.comp=solve(t(Xmat)%*%Wmat.comp%*%Xmat)
  T1=2*t(yvec-Xmat%*%solv%*%t(Xmat)%*%Wmat%*%yvec)%*%Xmat
  T1.comp=(-2)*t(yvec-Xmat%*%solv.comp%*%t(Xmat)%*%Wmat.comp%*%yvec)%*%Xmat
  T2=kronecker(solv,solv)
  T2.comp=kronecker(solv.comp,solv.comp)
  gradient=c()
  cate=sort(unique(svec))
  ncate=length(cate)
  for(icate in 1:ncate)
  {
    Xmat.cur=Xmat[svec==cate[icate],]
    yvec.cur=yvec[svec==cate[icate]]
    cons=T1%*%((solv%*%t(Xmat.cur)%*%Xmat.cur%*%solv)%*%t(Xmat)%*%Wmat%*%yvec-solv%*%apply(Xmat.cur*yvec.cur,2,sum))
    cons.comp=T1.comp%*%((solv.comp%*%t(Xmat.cur)%*%Xmat.cur%*%solv.comp)%*%t(Xmat)%*%Wmat.comp%*%yvec-solv.comp%*%apply(Xmat.cur*yvec.cur,2,sum))
    gradient=c(gradient,cons+cons.comp)
  }
  return(gradient)
}
################################################################################
## One iteration of the IRLS algorithm for multi-logistic choice model, with multiple market segments.
mlogit.GAM.IRLS=function(seg,Nvec,nvec,zmat,gamm.param,tolern=1e-6)
{
##
 #seg=period
 #Nvec=as.vector(matrix(rep(N.vec,M),nrow=M,byrow=T))
 #nvec=as.vector(t(n.mat))
 #zmat=z.mat
 #gamm.param=gamm.cur
 #seg=aus.sales$PERIOD.REG
 #Nvec=as.vector(N.vec)
 #nvec=as.vector(aus.sales$SALES.UNITS)
 #zmat=xreg.cate
 #gamm.param=gamm.cur
##
  M=length(seg)/length(unique(seg))
  dim.z=ncol(zmat)
  gamm.new=gamm.param
  prob.vec=exp(zmat%*%gamm.param)/(group.fn(as.factor(seg),exp(zmat%*%gamm.param),"sum")$results+1)
  gradient=apply(zmat*as.vector(nvec-Nvec*prob.vec),2,sum)
  ### Find the Hessian matrix.
  Hessian=matrix(0,dim.z,dim.z,byrow=T)
  for(iperiod in sort(unique(seg)))
  {
   period.cur=(seg==iperiod)
   Hessian=Hessian+unique(Nvec[period.cur])[1]*(t(zmat[period.cur,])%*%(zmat[period.cur,]*prob.vec[period.cur])-apply(zmat[period.cur,]*as.vector(prob.vec[period.cur]),2,sum)%*%t(apply(zmat[period.cur,]*as.vector(prob.vec[period.cur]),2,sum)))
  }
  diag(Hessian)=diag(Hessian)+(1e-6)
  #cat(eigen(Hessian)$values,'\n')
  chol.H=solve(chol(Hessian))
  increment=chol.H%*%t(chol.H)%*%gradient
  crit.cur=sum(nvec*log(prob.vec))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.vec,seg,sum)))
  increment.opt=NA
  for(iter in 1:10)
  {
   increment.cur=increment*(2^(-iter))
   if(max(abs(increment.cur))<tolern) break;
   gamm.new=gamm.param+increment.cur
   prob.new=exp(zmat%*%gamm.new)/(group.fn(as.factor(seg),exp(zmat%*%gamm.new),"sum")$results+1)
   crit.new=sum(nvec*log(prob.new))+sum((tapply(Nvec,seg,unique)-tapply(nvec,seg,sum))*log(1-tapply(prob.new,seg,sum)))
   #cat(iter,",",max(abs(increment.cur)),",",crit.new,'\n')
   if(crit.new>crit.cur)
   {
    crit.cur=crit.new
    increment.opt=increment.cur
   }
  }
  if(!is.na(increment.opt[1]))
  {
   gamm.new=gamm.param+increment.opt
   return(list(gamm=gamm.new,chol.Hess=chol.H))
  }
  #cat("mlogit:",gamm,",",crit.cur,'\n')
  return(list(gamm=gamm.param))
}
################################################################################
## Generate the matrix of directed links based on the Z matrix and coefficients.
Gen.link=function(z,alpha)
{
  nz=nrow(z)
  links=matrix(0, nrow=nz,ncol=nz,byrow=T)
  tmp=rbinom(nz*nz,1,as.vector(logit.inv(alpha+(z) %*% t(z/sqrt(apply(z^2,1,sum))))))
  return(matrix(tmp,nrow=nz,ncol=nz,byrow=F))
}
################################################################################
## Attraction function.
Attraction=function(util,option="exp")
{
  atr=exp.rob(util)
  derv=exp.rob(util)
  derv2=exp.rob(util)
  if(option=="linear")
   {
     atr=util
     atr[atr< 1e-10]=1e-10
     derv=derv-derv+1
     derv[atr< 0]=0
  }
  if(option=="piecewise")
   {
     atr=util
     atr[util< 0]=exp.rob(util[util< 0])-1
     atr=atr+1
     derv=derv-derv+1
     derv[util< 0]=exp.rob(util[util< 0])
     derv2=derv-derv
     derv2[util< 0]=exp.rob(util[util< 0])
  }
  return(list(val=atr,grad=derv,gradlog=derv/atr,grad2log=derv2/atr))
}
################################################################################
Boost.ChoiceSlow=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),M=4,nBoost=100,nu=0.1,fit.method="linear",mini.size=20,ncatelow=5,ncatehigh=20,opt="exp")
{
##############################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
###########################
 n=nrow(varReg)
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
 #cat("heihei")
 if(opt=="linear")
 {
  grad=rep(0,nrow(varReg))
  grad[apply(varReg*coef.Boost,1,sum)>0]=1
  resid.cur=resid.cur*grad/Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
 }
 #cat(sum(abs(resid.cur)),'\n')
 #cat("heihei")
 var.imp=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 for(iBoost in 1:nBoost)
 {
  #if(iBoost %% 9==1) {par(mfrow=c(3,3))}
  crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  cat(iBoost,crit.root,'\n')
  res=Simple.tree(varPart,"1",varReg,resid.cur,crit.root,name.split,M,crit.min=0,minisize=mini.size,regmethod=fit.method,ncate_low=ncatelow,ncate_high=ncatehigh,F.test=Ftest)
  res.pred=split.estimate(res$leaf.assoc,varReg,res$model.est,3:4)
  coef.increment=nu*res.pred$coefs
  var.imp=var.imp+(nu^2)*res$split.impvec
  coef.Boost=coef.Boost+coef.increment
  coef.Boost[,1]=coef.Boost[,1]-group.fn(market,apply(varReg*coef.Boost,1,sum),"mean")$results
  coefs.mat[[iBoost]]=coef.Boost
  resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  splt.modelest[[iBoost]]=res$model.est
  splt.crit=c(splt.crit,res$split.krit)
  #plot(log(var.n/var.N),apply(varReg*coef.Boost,1,sum),type='p',pch='.',cex=2,main=iBoost,ylab='fits')
  #abline(0,1,col='red')
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  #R2.in=rbind(R2.in,t(unlist(calc.R2(apply(varReg*coef.Boost,1,sum),varY))))
  #error.in=rbind(error.in,t(c(mean((apply(varReg*coef.Boost,1,sum)-varY)^2),mean(abs(apply(varReg*coef.Boost,1,sum)-varY)))))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
  cat(iBoost,":",crit.temp,sum((resid.cur)^2),'\n')
 }
 #R2.in=unlist(R2.in)
 #colnames(error.in)=c("L2","L1")
 return(list(variable.imp=var.imp,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,coefs.pred=coefs.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
## Out-sample prediction based on boosting, additive=F means bagging.
pred.BoostChoice=function(varPart,varReg,varY,obj.Boost,cols.coef,nBoost,nu,market=NA,var.n=0,var.N=0,thin=1,opt='exp',additive=T,initial='Param',inits=(varReg-varReg)[,1])
{
  if(initial=='Param') pred=apply(varReg %*% as.vector(obj.Boost$f0.hat),1,sum)
  if(initial=='fit') pred=inits#(varReg-varReg)[,1]#as.vector(obj.Boost$f0.hat)
  pred.mat=Attraction(pred,opt)$val/group.fn(market,Attraction(pred,opt)$val,"sum")$results*var.N
  R2.out=c()
  coef.mat=varReg-varReg
  if(initial=='Param')
  {
   for(j in 1:length(obj.Boost$f0.hat))
   {
    coef.mat[,j]=obj.Boost$f0.hat[j]
   }
  }
  if(initial=='fit') coef.mat[,1]=inits#0#obj.Boost$f0.hat
  for(iBoost in 1:nBoost)
  {
   cat(iBoost,'\n')
   spt.pred=split.predict(varPart,varReg,obj.Boost$split.krit[iBoost],obj.Boost$model.est[[iBoost]],cols=cols.coef)
   pred=pred+spt.pred$pred*nu
   coef.mat=coef.mat+spt.pred$coefs*nu
   coef.mat[,1]=coef.mat[,1]#-mean(coef.mat[,1])
   pred.units=Attraction(pred,opt)$val/group.fn(market,Attraction(pred,opt)$val,"sum")$results*var.N
   if(!additive)
    {
     pred.units=Attraction(pred/iBoost,opt)$val/group.fn(market,Attraction(pred/iBoost,opt)$val,"sum")$results*var.N
    }
   R2.out=c(R2.out,calc.R2(pred.units,var.n)$R2)
   if(iBoost %% thin ==0) pred.mat=cbind(pred.mat,pred.units)
  }
  if(!additive)
   {
    coef.mat=coef.mat/iBoost
   }
  return(list(predictions=pred.mat,R2=R2.out,coefs=coef.mat))
}
pred.BoostChoiceSimp=function(varPart,obj.Boost,cols.coef,nBoost,nu)
{
  varReg=cbind(rep(1,nrow(varPart)),rep(0,nrow(varPart)))
  R2.out=c()
  coef.mat=varReg-varReg
  for(j in 1:length(obj.Boost$f0.hat))
  {
    coef.mat[,j]=obj.Boost$f0.hat[j]
  }
  for(iBoost in 1:nBoost)
  {
   spt.pred=split.predict(varPart,varReg,obj.Boost$split.krit[iBoost],obj.Boost$model.est[[iBoost]],cols=cols.coef)
   coef.mat=coef.mat+spt.pred$coefs*nu
   coef.mat[,1]=coef.mat[,1]-mean(coef.mat[,1])
  }
  return(list(coefs=coef.mat))
}
################################################################################
## Data aggregation.
## dat: the whole data set.
## var.ident: the identifiers
## var.agg: the variables to be aggregated over.
Aggregate.old=function(dat,var.ident,var.agg,nam="temp")
{
  dat$Label=apply(dat[,var.ident],1,paste,collapse="\\")
  dat.temp=sort(unique(dat$Label))#cbind(CONFIG=cate.sum(dat[,"Label"],dat[,var.agg[1]])$cates)
  for(icol in 1:length(var.agg))
  {
   dat.temp=cbind(dat.temp,tapply(dat[,var.agg[icol]],dat$Label,sum,na.rm=T))
  }
  colnames(dat.temp)=c("Label",var.agg)
  dat.agg=cbind(matrix(unlist(lapply(dat.temp[,"Label"],strsplit,"\\\\")),nrow=nrow(dat.temp),byrow=T),dat.temp[,2:(length(var.agg)+1)])
  colnames(dat)=c(colnames(var.ident),colnames(var.agg))
  write.table(dat.agg,paste(nam,".csv",sep=""),sep=',',col.names=T,row.names=F)
  return(dat.agg)
}
Aggregate=function(dat,var.ident,var.agg,nam="temp")
{
  dat$Label=apply(dat[,var.ident],1,paste,collapse="\\")
  dat.temp=c()#sort(unique(dat$Label))#cbind(CONFIG=cate.sum(dat[,"Label"],dat[,var.agg[1]])$cates)
  #dat.agg=dat
  for(icol in 1:length(var.agg))
  {
   dat.temp=cbind(dat.temp,as.vector(tapply(dat[,var.agg[icol]],dat$Label,sum,na.rm=T)))
  }
  colnames(dat.temp)=c(var.agg)
  #matrix(unlist(lapply(sort(unique(dat$Label)),strsplit,"\\\\")),nrow=nrow(dat.temp),byrow=T)
  dat.agg=as.data.frame(matrix(unlist(lapply(sort(unique(dat$Label)),strsplit,"\\\\")),nrow=nrow(dat.temp),byrow=T))
  dat.agg=cbind(dat.agg,dat.temp)#cbind(,dat.temp[,2:(length(var.agg)+1)])
  colnames(dat.agg)=c((var.ident),(var.agg))
  return(dat.agg)
}
################################################################################
SemiBoost.ChoiceRpart=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),M=4,nBoost=100,nu=0.1,toln=1e-6,n.iter=500,opt='exp')
{
##############################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
 n=nrow(varReg)
 p=ncol(varReg)-1
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
 var.imp=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 tree.vec=c()
 slope.vec=c()
 intercept.mat=c()
 leaf.mat=c()
 for(iBoost in 1:nBoost)
 {
  dat.cur=cbind(resid.cur,varPart)
  colnames(dat.cur)=c("RESID",colnames(varPart))
  tree.cur=rpart(as.formula(paste("RESID~",paste(colnames(varPart),collapse="+"),sep="")),data=dat.cur,cp=-1,maxdepth=2)
  res.pred=predict(tree.cur,varPart)
  design.cur=cbind(gen.design(tree.cur$where)$design,varReg[,-1])
  gamm.cur=rep(0,ncol(design.cur))
  for(iter in 1:n.iter)
  {
   gamm.new=mlogit.choice.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F,GAM=opt)$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  coef.increment=cbind(design.cur[,-((ncol(design.cur)-p+1):ncol(design.cur))]%*%gamm.cur[-((ncol(design.cur)-p+1):ncol(design.cur))],gamm.cur[((ncol(design.cur)-p+1):ncol(design.cur))])#coef.Boost+coef.increment
  coef.Boost=coef.Boost+nu*coef.increment
  intercept.mat[[iBoost]]=gamm.cur[-length(gamm.cur)]
  coefs.mat[[iBoost]]=coef.Boost
  slope.vec=c(slope.vec,coef.Boost[1,2])
  resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
  tree.cur$frame$yval=as.numeric(row.names(tree.cur$frame))
  xixi=tree.cur$frame
  leaf.mat[[iBoost]]=sort(xixi[xixi$var=="<leaf>","yval"])
  tree.vec[[iBoost]]=tree.cur
  ### variable importance.
  # Code to  calculate variable effectiveness from output of rpart
  # convert rpart output to a data.frame
  df<-data.frame(Variable=rownames(tree.cur$splits), tree.cur$splits[,1:ncol(tree.cur$splits)], row.names=1:nrow(tree.cur$splits))
  # aggregate the improvement for each variable
  attach(df)
  var.scores<-aggregate(as.data.frame(improve), by=list(Variable=Variable), sum)
  detach(df)
  print(var.scores)
  cat(iBoost,":",crit.temp,xixi[xixi$var=="<leaf>","yval"],'\n')
 }
 return(list(res.tree=tree.vec,coefs=coef.Boost,leaves=leaf.mat,slopes=slope.vec,intercepts=intercept.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
SemiBoost.Choice=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),fit.method="linear",M=4,nBoost=100,nu=0.1,toln=1e-6,n.iter=500,opt='exp',mini.size=20,ncatelow=5,ncatehigh=20,mtry=ncol(varPart))
{
################################################################################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
 n=nrow(varReg)
 p=ncol(varReg)-1
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
 var.imp=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 tree.vec=c()
 slope.vec=c()
 intercept.mat=c()
 leaf.mat=c()
 for(iBoost in 1:nBoost)
 {
  cat(iBoost,"\n")
  crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  #dat.cur=cbind(resid.cur,varPart)
  #colnames(dat.cur)=c("RESID",colnames(varPart))
  tree.cur=Simple.tree(varPart,"1",as.matrix(rep(1,nrow(varReg))),resid.cur,crit.root,name.split,M,crit.min=0,minisize=mini.size,regmethod=fit.method,ncate_low=ncatelow,ncate_high=ncatehigh)
  #print(tree.cur$model.est)
  #rpart(as.formula(paste("RESID~",paste(colnames(varPart),collapse="+"),sep="")),data=dat.cur,cp=-1,maxdepth=2)
  #res.pred=predict(tree.cur,varPart)
  design.cur=cbind(gen.design(tree.cur$leaf.assoc)$design,varReg[,-1])
  gamm.cur=rep(0,ncol(design.cur))
  for(iter in 1:n.iter)
  {
   gamm.new=mlogit.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F,GAM=opt)$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  coef.increment=cbind(design.cur[,-((ncol(design.cur)-p+1):ncol(design.cur))]%*%gamm.cur[-((ncol(design.cur)-p+1):ncol(design.cur))],gamm.cur[((ncol(design.cur)-p+1):ncol(design.cur))])#coef.Boost+coef.increment
  coef.Boost=coef.Boost+nu*coef.increment
  intercept.mat[[iBoost]]=gamm.cur[-length(gamm.cur)]
  coefs.mat[[iBoost]]=coef.Boost
  slope.vec=c(slope.vec,coef.Boost[1,2])
  resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
  param.est=gamm.cur[-length(gamm.cur)]
  tree.cur$model.est[,3]=param.est[match(tree.cur$model.est[,1],gen.design(tree.cur$leaf.assoc)$nam)]#gamm.cur[-length(gamm.cur)]
  #cat(gen.design(tree.cur$leaf.assoc)$nam,gamm.cur[-length(gamm.cur)],'\n')
  splt.modelest[[iBoost]]=tree.cur$model.est
  splt.crit=c(splt.crit,tree.cur$split.krit)
  #leaf.mat[[iBoost]]=#sort(xixi[xixi$var=="<leaf>","yval"])
  #tree.vec[[iBoost]]=tree.cur
  }
 return(list(model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,slopes=slope.vec,intercepts=intercept.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
## Out-sample prediction based on boosting.
pred.SemiBoostChoice=function(varPart,varReg,varY,obj.Boost,nBoost,nu=0.1,market=NA,var.n=0,var.N=0,thin=1,opt='exp')
{
  pred=apply(varReg %*% as.vector(obj.Boost$f0.hat),1,sum)
  pred.mat=Attraction(pred,option=opt)$val/group.fn(market,Attraction(pred,option=opt)$val,"sum")$results*var.N
  R2.out=c()
  coef.mat=varReg-varReg
  coef.mat[,1]=obj.Boost$f0.hat[1]
  coef.mat[,2]=obj.Boost$f0.hat[2]
  leaves=c()
  #cat(nBoost,'\n')
  for(iBoost in 1:nBoost)
  {
   #cat("xixi",'\n')
   spt.predict=split.predict(varPart,as.matrix(varReg[,1]),obj.Boost$split.krit[iBoost],obj.Boost$model.est[[iBoost]],cols=3)
   spt.pred=spt.predict$coefs
   leaves=c(leaves,spt.predict$assoc)
   #cat("xixi",'\n')
   #print(spt.pred)
   #cat(dim(spt.pred),length(spt.pred),'\n')
   #cat(iBoost,unique(predict(obj.Boost$res.tree[[iBoost]],varPart)),":",dim(spt.pred),length(obj.Boost$intercepts[[iBoost]]),'\n')
   coef.mat[,1]=coef.mat[,1]+(spt.pred)*nu
   coef.mat[,2]=obj.Boost$slopes[iBoost] ## did we multiply by nu yet?
   pred=apply(varReg*coef.mat,1,sum)

   pred.units=Attraction(pred,option=opt)$val/group.fn(market,Attraction(pred,option=opt)$val,"sum")$results*var.N
   R2.out=c(R2.out,calc.R2(pred.units,var.n)$R2)
   if(iBoost %% thin ==0) pred.mat=cbind(pred.mat,pred.units)
  }
  return(list(predictions=pred.mat,R2=R2.out,coefs=coef.mat,assoc=leaves))
}
################################################################################
## Out-sample prediction based on boosting.
pred.SemiBoostChoiceRpart=function(varPart,varReg,varY,obj.Boost,nBoost,nu=0.1,market=NA,var.n=0,var.N=0,thin=1,opt='exp')
{
  #varPart=(aus.sales[index.pred,names.part])
  #varReg=as.matrix(cbind(1,aus.sales$PRICE)[index.pred,])
  #varY=aus.sales$SALES.UNITS[index.pred]
  #obj.Boost=res
  #nBoost=nBoost.param
  #market=aus.sales$SEGMENT[index.pred]
  #var.n=aus.sales$SALES.UNITS[index.pred]
  #var.N=aus.sales$size.periodreg[index.pred]
  #thin=n.thin
  pred=apply(varReg %*% as.vector(obj.Boost$f0.hat),1,sum)
  pred.mat=Attraction(pred,option=opt)$val/group.fn(market,Attraction(pred,option=opt)$val,"sum")$results*var.N
  R2.out=c()
  coef.mat=varReg-varReg
  coef.mat[,1]=obj.Boost$f0.hat[1]
  coef.mat[,2]=obj.Boost$f0.hat[2]
  for(iBoost in 1:nBoost)
  {
   spt.pred=gen.design(predict(obj.Boost$res.tree[[iBoost]],varPart),unik=obj.Boost$leaves[[iBoost]])$design
   #cat(iBoost,unique(predict(obj.Boost$res.tree[[iBoost]],varPart)),":",dim(spt.pred),length(obj.Boost$intercepts[[iBoost]]),'\n')
   coef.mat[,1]=coef.mat[,1]+(spt.pred%*%obj.Boost$intercepts[[iBoost]])*nu
   coef.mat[,2]=obj.Boost$slopes[iBoost]
   pred=apply(varReg*coef.mat,1,sum)
   pred.units=Attraction(pred,option=opt)$val/group.fn(market,Attraction(pred,option=opt)$val,"sum")$results*var.N
   R2.out=c(R2.out,calc.R2(pred.units,var.n)$R2)
   if(iBoost %% thin ==0) pred.mat=cbind(pred.mat,pred.units)
  }
  return(list(predictions=pred.mat,R2=R2.out,coefs=coef.mat))
}
################################################################################
## Return the number of distinct values in a character vector.
nlevels.ext=function(vec)
{
 return(length(unique(as.character(vec))))
}
## Kernel smoothing on categorical variables.
kern.nominal=function(dat,vec,scales,lamb.val,what="lambda")
{
 #dat=aus.sales[index.train,names.nominal]
 #vec=as.vector(vec.pred[names.nominal])
 #scales=scale.nominal
 #lamb.val=tune.cur[2]
 if(what=="alpha")
 {
  lamb.val=lamb.val+(1-lamb.val)/scales
  #cat(lamb.val,'\n')
 }
 vec.mat=(dat)
 vec.mat[]=as.vector(vec)
 vec.mat[]=matrix(as.vector(vec.mat),nrow=nrow((dat)),byrow=T)
 xixi=apply((vec.mat==dat)*lamb.val+t(t(vec.mat!=dat)/(scales-1))*(1-lamb.val),1,prod)
 return(xixi/max(xixi))
}
## Kernel smoothing on numerical variables.
kern.numerical=function(dat,vec,scales,h.val)
{
 #dat=as.matrix(aus.sales[index.train,names.numerical])
 #vec=as.numeric(vec.pred[names.numerical])
 #scales=scale.numerical
 #h.val=as.numeric(unlist(tune.cur[2]))
 diff.temp=t((t(dat)-vec)/scales)/h.val
 #cat(dim(diff.temp),'\n')
 #xixi=apply(3/4*(1-diff.temp^2)*(diff.temp<1 & diff.temp> (-1)),1,prod)
 xixi=apply(dnorm(diff.temp),1,prod)
 return(xixi)
}
## Distance calculation on categorical variables.
dist.nominal=function(dat,vec,scales,alp.val)
{
 #dat=aus.sales[index.train,names.nominal]
 #vec=as.vector(vec.pred[names.nominal])
 #scales=scale.nominal
 #lamb.val=tune.cur[2]
 vec.mat=(dat)
 vec.mat[]=as.vector(vec)
 vec.mat[]=matrix(as.vector(vec.mat),nrow=nrow((dat)),byrow=T)
 lamb.val=as.vector(alp.val+(1-alp.val)*(1-1/scales))
 #cat(lamb.val,'\n')
 #cat((vec.mat[1,]),'\n')
 #cat((dat[1,]),'\n')
 hoho=apply((vec.mat==dat)*(1-lamb.val)*(scales-1)+t(t(vec.mat!=dat))*lamb.val,1,sum,na.rm=T)
 return(hoho)
}
## Distance calculation on numerical variables.
dist.numerical=function(dat,vec,scales)
{
 #dat=as.matrix(aus.sales[index.train,names.numerical])
 #vec=as.numeric(vec.pred[names.numerical])
 #scales=scale.numerical
 #h.val=as.numeric(unlist(tune.cur[2]))
 diff.temp=t((t(dat)-vec)/scales/sqrt(2))
 #cat(dim(diff.temp),'\n')
 xixi=apply(diff.temp^2/qchisq(.95,1),1,sum)#apply(3/4*(1-diff.temp^2)*(diff.temp<1 & diff.temp> (-1)),1,prod)
 return(xixi)
}
## Epanechnikov
Epanechnikov=function(x)
{
  res=x-x
  res[abs(x) <= 1]=x[abs(x) <= 1]
  if(length(unique(res))<= 2) return(res)
  return((1-x^2)*(x<= 1)*(x>= (-1))*3/4)
}
# create function to return matrix of memory consumption
object.sizes <- function()
{
    return(rev(sort(sapply(ls(envir=.GlobalEnv), function(object.name)
        object.size(get(object.name))))))
}
################################################################################
Nest.prob=function(lamb,util,markt,nst,opt='exp')
{
  marketnest=paste(markt,nst,sep="-")
  #lamb.vec=rep(0,length(util))
  #lamb.vec=lamb.v[nst]
  atr=Attraction(util/lamb,option=opt)$val
  xixi=group.fn(as.factor(paste(markt,nst,sep="-")),atr,"sum")
  prob.vec=atr/(xixi$results)
  I.vec=log(xixi$results)
  Qtemp.vec=exp(lamb*I.vec)
  Qtemp2.vec=Qtemp.vec/group.fn(as.factor(paste(markt,nst,sep="-")),rep(1,length(atr)),"sum")$results
  Q.vec=Qtemp.vec/(group.fn(as.factor(markt),Qtemp2.vec,"sum")$results)
  return(Q.vec*prob.vec)
}
################################################################################
Boost.Nest=function(varPart,varReg,var.n,var.N,name.split,market=NA,nest=NA,param.init=rep(0,ncol(varReg)),M=4,nBoost=100,nu=0.1,toln=1e-6,n.iter=500,lambda=1,separate=T)
{
##############################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
 n=nrow(varReg)
 lambda.cur=lambda
 marketnest=paste(market,nest,sep="-")
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N/lambda.cur-2*Nest.prob(lambda.cur,apply(coef.init*varReg,1,sum),market,nest)+2*(1-1/lambda.cur)/var.N*group.fn(marketnest,var.n,"sum")$results
 #var.imp=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 tree.vec=c()
 slope.vec=c()
 intercept.mat=c()
 leaf.mat=c()
 lambda.vec=c()
 for(iBoost in 1:nBoost)
 {
  cat(iBoost,'\n')
  lambda.vec=c(lambda.vec,lambda.cur)
  crit.root=sum((regression(resid.cur,varReg,method="linear")$residuals)^2)
  res=Simple.tree(varPart,nodecur="1",varReg,resid.cur,crit.root,name.split,M,crit.min=0,maxsplit=8,minisize=20,regmethod="linear",ncate_low=0,ncate_high=1)
  ## Separately estimate the scale parameter.
  if(separate)
  {
   coef.increment=split.predict(varPart,varReg,res$split.krit,res$model.est,cols=3:4)$coefs
   coef.Boost=coef.Boost+nu*coef.increment
   lambda.est=function(lamb)
   {
    return((-2)*sum(log(Nest.prob(lamb,util=apply(coef.Boost*varReg,1,sum),markt=market,nst=nest))*var.n))
   }
   lambda.cur=optimize(lambda.est,lower=0,upper=100)$minimum
  }
  ## Estimate the scale parameter along with mean parameters.
  if(!separate)
  {
  varReg.cur=gen.design(as.factor(res$leaf.assoc),cont=varReg)$design
  #varReg.cur=varReg.cur[,-1]
  #cat(ncol(varReg.cur),'\n')
  xixi=regression(aus.sales$SALES.UNITS[index.train],varReg.cur,N.vec=aus.sales$size.periodreg[index.train],n.vec=aus.sales$SALES.UNITS[index.train],market=as.factor(aus.sales$SEGMENT[index.train]),method="nest",nests=aus.sales$NEST[index.train],param.init=rep(0,ncol(varReg.cur)+1),off.set=apply(coef.Boost*varReg,1,sum))
  coef.increment=(kronecker(rep(1,nrow(varReg)),t(xixi$coef[-length(xixi$coef)]))*varReg.cur)%*%gen.design(c(rep(1,M),rep(2,M)))$design
  coef.Boost=coef.Boost+nu*coef.increment
  lambda.cur=exp(xixi$coef[length(xixi$coef)])
  }
  coefs.mat[[iBoost]]=coef.Boost
  prob.cur=Nest.prob(lambda.cur,apply(coef.Boost*varReg,1,sum),market,nest)
  resid.cur=2*var.n/var.N/lambda.cur-2*prob.cur+2*(1-1/lambda.cur)/var.N*group.fn(marketnest,var.n,"sum")$results
  R2.in=rbind(R2.in,calc.R2(prob.cur*var.N,var.n))
  crit.temp=(-2)*sum(var.n*log(prob.cur))
  crit.vec=c(crit.vec,crit.temp)
  cat(lambda.cur,'\n')
  }
 return(list(res.tree=tree.vec,coefs=coef.Boost,leaves=leaf.mat,slopes=slope.vec,intercepts=intercept.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
## Boosted choice model where IRLS is iterated.
Boost.Choice=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),M=4,nBoost=100,nu=0.1,fit.method="linear",mini.size=20,ncatelow=5,ncatehigh=20,opt="exp",toln=1e-5,n.iter=500,Ftest=F,Cate.cont=10,mtry=ncol(varPart),additive=T,shrinkpen=0,shrinkval=(-0.002),initial='Param')
{
##############################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
###########################
 n=nrow(varReg)
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 if(initial=='fit') coef.init=coef.Boost=param.init
 resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
 resid.cur=resid.cur # Shrinkage penalty.
 var.imp=rep(0,length(name.split))
 var.freq=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 for(iBoost in 1:nBoost)
 {
  cat(iBoost,'\n')
  indice=sort(sample(1:ncol(varPart),mtry))
  varPart.cur=(varPart[,indice])
  if(mtry<2) varPart.cur=as.matrix(varPart[,indice])
  #cat(name.split[indice],dim(varPart.cur),'\n')
  #print(varPart.cur)
  name.split.cur=name.split[indice]
  #cat(name.split.cur,'\n')#if(iBoost %% 9==1) {par(mfrow=c(3,3))}
  crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  #cat(iBoost,crit.root,'\n')
  res=Simple.tree(varPart.cur,"1",varReg,resid.cur,crit.root,name.split.cur,M,crit.min=0,minisize=mini.size,regmethod=fit.method,ncate_low=ncatelow,ncate_high=ncatehigh,F.test=Ftest,L.cont=Cate.cont)
  #cat(iBoost,'\n')
  #if(<2))
  # write.table(cbind(resid.cur,varReg,varPart),"xixi.csv",sep=',',row.names=F)
  #print(table(res$leaf.assoc))
  #print(res$model.est)
  #cat(length(unique(res$leaf.assoc)),'\n')
  #cat(name.split.cur,'\n')
  #cat(dim(varReg),'\n')
  #cat("Start boosting",iBoost,'\n')
  tmp=gen.design(res$leaf.assoc,cont=varReg)
  #cat("Finish generating design","\n")
  design.cur=cbind(tmp$design)
  #cat(dim(design.cur),'\n')
  #cat(apply(design.cur,2,sum),'\n')
  temp=var.n/var.N
  temp[temp< 1e-5]=1e-5
  gamm.cur=lm(lm(log(temp)~market)$residuals-apply(coef.Boost*varReg,1,sum)~design.cur-1)$coef#rep(0,ncol(design.cur))
  for(iter in 1:n.iter)
  {
   #cat(iter,'\n')
   if(shrinkpen>0) gamm.new=glmnetlogit.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F,GAM=opt,slopes=coef.Boost[,2],shrink.pen=shrinkpen,shrink.val=shrinkval)$gamm
   if(shrinkpen==0) gamm.new=mlogit.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F,GAM=opt,slopes=coef.Boost[,2])$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  if(ncol(varReg)==2)
  {
   gamm.est=matrix(gamm.cur,nrow=ncol(design.cur)/2,byrow=F)
   coef.increment=nu*(cbind(cbind(gen.design(res$leaf.assoc)$design),cbind(gen.design(res$leaf.assoc)$design))%*%gen.design(as.factor(c(rep(1,ncol(design.cur)/2),rep(2,ncol(design.cur)/2))),cont=as.matrix(gamm.cur))$design)
  }
  #cat("Finish iteration 2",ncol(varReg),"\n")
  if(ncol(varReg)==1)
  {
   gamm.est=as.matrix(gamm.cur)#,nrow=ncol(design.cur)/2,byrow=F)
   coef.increment=nu*gen.design(res$leaf.assoc)$design%*%gamm.cur#gen.design(as.factor(rep(1,ncol(design.cur))),cont=)$design)
  }
  #coef.increment[,1]=coef.increment[,1]-group.fn(market,coef.increment[,1],'mean')$results
  #cat("Finish iteration 1","\n")
  #cat(name.split.cur,(var.imp),",",'\n')
  var.imp[indice]=var.imp[indice]+(nu^2)*res$split.impvec
  var.freq[indice]=var.freq[indice]+(res$split.impvec>0)
  if(!(additive)) coef.Boost=coef.increment
  if(additive) coef.Boost=coef.Boost+coef.increment
  coef.Boost[,1]=coef.Boost[,1]-group.fn(market,coef.Boost[,1],"mean")$results
  coefs.mat[[iBoost]]=coef.Boost

  if(additive) resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  for(irow in 1:length(tmp$nam))
  {
    inode=tmp$nam[irow]
    #cat(iBoost,res$model.est[,1],inode,ncol(varReg),gamm.est[irow,],'\n')
    res$model.est[res$model.est[,1]==inode,2+(1:ncol(varReg))]=gamm.est[irow,]
  }
  splt.modelest[[iBoost]]=res$model.est
  splt.crit=c(splt.crit,res$split.krit)
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
 }
 if(initial=='fit') return(list(variable.imp=var.imp,variable.freq=var.freq,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,coefs.pred=coefs.mat,crit=crit.vec,f0.hat=coef.init,R2.train=R2.in))
 return(list(variable.imp=var.imp,variable.freq=var.freq,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,coefs.pred=coefs.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
Boost.MixedChoice=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),M=4,nBoost=100,nu=0.1,fit.method="linear",mini.size=20,ncatelow=5,ncatehigh=20,opt="exp",toln=1e-5,n.iter=500)
{
##############################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
###########################
 n=nrow(varReg)
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
 var.imp=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 for(iBoost in 1:nBoost)
 {
  #if(iBoost %% 9==1) {par(mfrow=c(3,3))}
  crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  #cat(iBoost,crit.root,'\n')
  res=Simple.tree(varPart,"1",varReg,resid.cur,crit.root,name.split,M,crit.min=0,minisize=mini.size,regmethod=fit.method,ncate_low=ncatelow,ncate_high=ncatehigh)
  tmp=gen.design(res$leaf.assoc,cont=varReg)
  design.cur=cbind(tmp$design)
  #cat(dim(design.cur),'\n')
  #cat(apply(design.cur,2,sum),'\n')
  gamm.cur=rep(0,ncol(design.cur))
  for(iter in 1:n.iter)
  {
   gamm.new=mlogit.choice.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F)$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  gamm.est=matrix(gamm.cur,nrow=ncol(design.cur)/2,byrow=F)
  coef.increment=nu*(cbind(cbind(gen.design(res$leaf.assoc)$design),cbind(gen.design(res$leaf.assoc)$design))%*%gen.design(as.factor(c(rep(1,ncol(design.cur)/2),rep(2,ncol(design.cur)/2))),cont=gamm.cur)$design)
  var.imp=var.imp+(nu^2)*res$split.impvec
  coef.Boost=coef.Boost+coef.increment
  coef.Boost[,1]=coef.Boost[,1]-group.fn(market,apply(varReg*coef.Boost,1,sum),"mean")$results
  coefs.mat[[iBoost]]=coef.Boost
  resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  for(irow in 1:length(tmp$nam))
  {
    inode=tmp$nam[irow]
    res$model.est[res$model.est[,1]==inode,3:4]=gamm.est[irow,]
  }
  splt.modelest[[iBoost]]=res$model.est
  splt.crit=c(splt.crit,res$split.krit)
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
  cat(iBoost,":",crit.temp,sum((resid.cur)^2),'\n')
 }
 return(list(variable.imp=var.imp,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,coefs.pred=coefs.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
TreeBoost.Choice=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),depth=2,nBoost=100,nu=0.1,toln=1e-6,n.iter=500,opt='exp',Ftest=F)
{
 n=nrow(varReg)
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N-2*Attraction(coef.Boost,option=opt)$val/group.fn(market,Attraction(apply(coef.Boost,1,sum),option=opt)$val,"sum")$results
 var.imp=as.matrix(t(rep(0,length(name.split))))
 colnames(var.imp)=name.split#c(names.part,"PRICE_RESID")
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 tree.vec=c()
 slope.vec=c()
 intercept.mat=c()
 leaf.mat=c()
 for(iBoost in 1:nBoost)
 {
  cat(iBoost,'\n')
  dat.cur=cbind(resid.cur,varPart)
  colnames(dat.cur)=c("RESID",colnames(varPart))
  if(!Ftest)
  {
   tree.cur=rpart(as.formula(paste("RESID~",paste(colnames(varPart),collapse="+"),sep="")),data=dat.cur,cp=-1,maxdepth=depth)
   design.cur=cbind(gen.design(tree.cur$where)$design)
  }
  if(Ftest)
  {
   tree.cur=ctree(as.formula(paste("RESID~",paste(colnames(varPart),collapse="+"),sep="")),data=dat.cur, controls = ctree_control(maxdepth=depth))
   design.cur=cbind(gen.design(where(tree.cur))$design)
  }
  res.pred=predict(tree.cur,varPart)
  gamm.cur=rep(0,ncol(design.cur))
  for(iter in 1:n.iter)
  {
   gamm.new=mlogit.choice.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=coef.Boost,tolern=toln,oo=F,GAM=opt)$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  coef.increment=design.cur[,]%*%gamm.cur
  coef.Boost=coef.Boost+nu*coef.increment
  intercept.mat[[iBoost]]=gamm.cur#[-length(gamm.cur)]
  coefs.mat[[iBoost]]=coef.Boost
  resid.cur=2*var.n/var.N-2*Attraction(coef.Boost,option=opt)$val/group.fn(market,Attraction(apply(coef.Boost,1,sum),option=opt)$val,"sum")$results
  attraction=Attraction(coef.Boost,option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(coef.Boost,option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(coef.Boost,option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
  if(!Ftest)
  {
   tree.vec[[iBoost]]=tree.cur
   tree.cur$frame$yval=as.numeric(row.names(tree.cur$frame))
   xixi=tree.cur$frame
   leaf.mat[[iBoost]]=sort(xixi[xixi$var=="<leaf>","yval"])
   ### variable importance.
   # Code to  calculate variable effectiveness from output of rpart
   # convert rpart output to a data.frame
   df<-data.frame(Variable=rownames(tree.cur$splits), tree.cur$splits[,1:ncol(tree.cur$splits)], row.names=1:nrow(tree.cur$splits))
   #print(df)
   #aggregate the improvement for each variable
   attach(df)
   var.scores<-aggregate(as.data.frame(improve), by=list(Variable=Variable), sum)
   detach(df)
   print(var.scores)
   cat(match(var.scores$Variable,name.split),'\n')
   var.imp[1,match(var.scores$Variable,name.split)]=var.imp[1,match(var.scores$Variable,name.split)]+t(var.scores$improve)[]
  }
  #cat(iBoost,":",crit.temp,xixi[xixi$var=="<leaf>","yval"],'\n')
 }
 return(list(variable.imp=var.imp,res.tree=tree.vec,coefs=coef.Boost,leaves=leaf.mat,intercepts=intercept.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
## Out-sample prediction based on boosting.
pred.TreeBoostChoice=function(varPart,varReg,varY,obj.Boost,nBoost,nu=0.1,market=NA,var.n=0,var.N=0,thin=1,opt='exp')
{
  #pred=varReg-varReg
  pred=varReg %*% as.vector(obj.Boost$f0.hat)
  pred.mat=Attraction(pred,option=opt)$val/group.fn(market,Attraction(pred,option=opt)$val,"sum")$results*var.N
  R2.out=c()
  coef.mat=varReg-varReg
  coef.mat[,1]=obj.Boost$f0.hat[1]
  for(iBoost in 1:nBoost)
  {
   spt.pred=gen.design(predict(obj.Boost$res.tree[[iBoost]],varPart),unik=obj.Boost$leaves[[iBoost]])$design
   coef.mat[,1]=coef.mat[,1]+(spt.pred%*%obj.Boost$intercepts[[iBoost]])*nu
   pred=apply(varReg*coef.mat,1,sum)
   pred.units=Attraction(pred,option=opt)$val/group.fn(market,Attraction(pred,option=opt)$val,"sum")$results*var.N
   R2.out=c(R2.out,calc.R2(pred.units,var.n)$R2)
   if(iBoost %% thin ==0) pred.mat=cbind(pred.mat,pred.units)
  }
  return(list(predictions=pred.mat,R2=R2.out,coefs=coef.mat))
}
################################################################################
## Tabulate the data; separate data into row identifiers and remaining variables, and a single variable
## that we want to tabulate
tabulat=function(id,market,y)
{
  #cat(id,'\n')
  #cat(market,'\n')
  #cat(y,'\n')
  #y
  #id=c("B","B","A","A")
  #market=c(1,2,1,2)
  #y=rnorm(4)
  num.id=as.numeric(as.factor(id))
  unk.market=sort(unique(market))
  res=matrix(NA,nrow=length(unique(num.id)),ncol=length(unk.market),byrow=T)
  colnames(res)=unk.market
  rownames(res)=levels(as.factor(id))
  #res.temp=res[,1]
  for(imarket in 1:length(unk.market))
  {
    res.temp=rep(NA,length(unique(num.id)))
    #dat.temp=y[market==unk.market[imarket]]
    #num.id[market==unk.market[imarket]]
    temp=tapply(y[market==unk.market[imarket]], num.id[market==unk.market[imarket]],mean)
    res.temp[as.numeric(names(temp))]=temp
    res[,imarket]=res.temp
  }
  #print(res)
  return(res)
}
################################################################################
## In a Linear regression model, identify the predictor variable
## with the highest F-value, variable-wise.
F.stat=function(yvec,x.mat,covar,Lcont=5)
{
  ## number of categories for continuous variable.
  #Lcont=10
  #yvec=aus.sales$SALES.UNITS
  #x.mat=aus.sales[,names.part]
  #covar=as.matrix(aus.sales$PRICE)
  dat=data.frame(cbind(cbind(yvec,x.mat),covar[,ncol(covar)]))
  colnames(dat)=c("y",colnames(x.mat),"z")
  dat$y=as.numeric(as.character(dat$y))
  dat$z=as.numeric(as.character(dat$z))
  pvalue=c()
  #cat("F.stat",'\n')
  for(j in colnames(x.mat))
  {
   if(lengthunique(dat[,j])<2)
   {
    pvalue=c(pvalue,-Inf)
    next;
   }
   if((is.numeric(dat[,j])) & lengthunique(dat[,j])> Lcont)
   {
    dat[,j]=gen.cut(dat[,j],ncut=Lcont)
   }
   if((!is.factor(dat[,j]))) dat[,j]=as.factor(dat[,j])
   #cat(nlevels(dat[,j]),'\n')
   #if(lengthunique(dat[,j])==2) write.table(dat,"temp.csv",sep=',')
   #cat("inside F.stat",colnames(x.mat),unique(dat[,j]),'\n')
   #for(i in 1:ncol(dat))
   #cat(dat[,i],'\n')
   mat=as.matrix(anova(lm(formula(paste("y~",j,"*z",sep='')),data=dat)))
   #print(mat)
   # {print(mat)}
   nums=1:(nrow(mat)-1)
   denoms=nrow(mat)
   df.num=sum(mat[nums,1],na.rm=T)
   df.denom=mat[denoms,1]
   MS.num=sum(mat[nums,2],na.rm=T)/df.num
   MS.denom=mat[denoms,3]
   F.value=MS.num/MS.denom
   pvalue=c(pvalue,pf(F.value, df.num, df.denom,log.p=T))#as.numeric(gsub("\\D", "", names(which.max(-mat[,ncol(mat)]))))
   #cat(j,df.num,'\n')
  }
  #print(dat)
  #cat("Finish F.stat",'\n')
  #cat(exp(pvalue),'\n')
  var.number=which.max(pvalue)
  return(list(opt=colnames(x.mat)[var.number[1]],pvalues=pvalue))
  #return(pvalue)
}
## Test robustness again # of cuts.
# pvals=c()
# for(L.cont in 3:10)
# {
#  pvals=cbind(pvals,F.stat(aus.sales$SALES.UNITS,aus.sales[,names.part],as.matrix(aus.sales$PRICE),Lcont=L.cont))
# }
# parcoord(pvals)
################################################################################
## Bagging-based choice model where IRLS is iterated.
Bagging.Choice=function(VarPart,VarReg,Var.n,Var.N,name.split,Market=NA,param.init=rep(0,ncol(VarReg)),M=4,nBoost=100,nu=0.1,fit.method="linear",mini.size=20,ncatelow=5,ncatehigh=20,opt="exp",toln=1e-5,n.iter=500,Ftest=F,Cate.cont=10,mtry=ncol(VarPart))
{
 n=nrow(VarReg)
 var.imp=rep(0,length(name.split))
 var.freq=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=R2.out=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 #
 for(iBoost in 1:nBoost)
 {
  cat(iBoost,'\n')
  mkt.test=sample(unique(Market),floor(length(unique(Market))/2))
  index.train=!(Market %in% mkt.test)
  index.pred=!index.train
  split.nd="1"
  Leaf=rep("1",nrow(VarPart))
  indice=sort(sample(1:ncol(VarPart),mtry))
  cat(iBoost,name.split[indice],'\n')###
  varPart.cur=(VarPart[index.train,indice])
  if(mtry<2) varPart.cur=as.matrix(VarPart[index.train,indice])
  varReg=as.matrix(VarReg[index.train,])
  var.n=Var.n[index.train]
  var.N=Var.N[index.train]
  market=as.character(Market[index.train])
  coef.init=coef.Boost=matrix(rep(param.init,sum(index.train)),nrow=sum(index.train),byrow=T)
  coefs.mat[[1]]=coef.Boost
  resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  #print(cbind(cbind(2*var.n/var.N,group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results),market))
  #cat(resid.cur,'\n')
  name.split.cur=name.split[indice]
  crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  #cat(dim(varPart.cur),dim(varReg),length(resid.cur),'\n')
  #print(varReg)
  #print(cbind(cbind(exp(apply(varReg*coef.Boost,1,sum)),resid.cur),market))
  #cat(,'\n')
  res=Simple.tree(varPart.cur,nodecur="1",varReg,resid.cur,crit.root,name.split.cur,M,crit.min=0,minisize=mini.size,regmethod=fit.method,ncate_low=ncatelow,ncate_high=ncatehigh,F.test=Ftest,L.cont=Cate.cont)
  cat("Finish iteration 0","\n")
  #print(res$leaf.assoc)
  tmp=gen.design(res$leaf.assoc,cont=varReg)
  design.cur=cbind(tmp$design)
  gamm.cur=rep(0,ncol(design.cur))
  for(iter in 1:n.iter)
  {
   gamm.new=mlogit.choice.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F,GAM=opt)$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  if(ncol(varReg)==2)
  {
   gamm.est=matrix(gamm.cur,nrow=ncol(design.cur)/2,byrow=F)
   coef.increment=nu*(cbind(cbind(gen.design(res$leaf.assoc)$design),cbind(gen.design(res$leaf.assoc)$design))%*%gen.design(as.factor(c(rep(1,ncol(design.cur)/2),rep(2,ncol(design.cur)/2))),cont=as.matrix(gamm.cur))$design)
  }
  if(ncol(varReg)==1)
  {
   gamm.est=as.matrix(gamm.cur)#,nrow=ncol(design.cur)/2,byrow=F)
   coef.increment=nu*gen.design(res$leaf.assoc)$design%*%gamm.cur#gen.design(as.factor(rep(1,ncol(design.cur))),cont=)$design)
  }
  cat("Finish iteration 1","\n")
  #cat(name.split.cur,(var.imp),",",'\n')
  var.imp[indice]=var.imp[indice]+(nu^2)*res$split.impvec
  var.freq[indice]=var.freq[indice]+(res$split.impvec>0)
  coef.Boost=coef.increment
  coef.Boost[,1]=coef.Boost[,1]-group.fn(market,apply(varReg*coef.Boost,1,sum),"mean")$results
  coefs.mat[[iBoost]]=coef.Boost
  #if(additive) resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  for(irow in 1:length(tmp$nam))
  {
    inode=tmp$nam[irow]
    res$model.est[res$model.est[,1]==inode,2+(1:ncol(varReg))]=gamm.est[irow,]
  }
  splt.modelest[[iBoost]]=res$model.est
  splt.crit=c(splt.crit,res$split.krit)
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  #cat(length(attraction/group.fn(market,attraction,"sum")$results*var.N),length(var.n),'\n')
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
  ## Prediction
  #print(res$split.krit)
  #print(res$model.est)
  spt.pred=split.predict(VarPart[!index.train,],as.matrix(VarReg[!index.train,]),res$split.krit,res$model.est,cols=(2+1:ncol(VarReg)))
  cat("Start prediction",'\n')
  pred=spt.pred$pred
  pred.units=Attraction(pred,opt)$val/group.fn(Market[!index.train],Attraction(pred,opt)$val,"sum")$results*Var.N[!index.train]
  #cat(length(pred.units),length(var.n),'\n')
  R2.out=c(R2.out,calc.R2(pred.units,Var.n[!index.train])$R2)
}
 return(list(variable.imp=var.imp,variable.freq=var.freq,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,coefs.pred=coefs.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in,R2.test=R2.out))
}
################################################################################
summari=function(x)
{
  return(c(mean(x),median(x),sd(x),quantile(x,probs=.025),quantile(x,probs=.975),skewness(x)))
}
################################################################################
## Multivariate shrinked boosted choice model where IRLS is iterated;
#library(mvpart)
mvsBoost.Choice=function(varPart,varReg,var.n,var.N,name.split,market=NA,param.init=rep(0,ncol(varReg)),M=4,nBoost=100,nu=0.1,fit.method="linear",mini.size=20,ncatelow=5,ncatehigh=20,opt="exp",toln=1e-5,n.iter=500,Ftest=F,Cate.cont=10,mtry=ncol(varPart),additive=T,shrink.pen=0,shrink.val=rep(0,ncol(varReg)))
{
##############################
#varPart=aus.sales[Leaf==split.nd &index.train,names.part]
#varReg=as.matrix(cbind(1,aus.sales$PRICE)[Leaf==split.nd & index.train,])
#var.n=aus.sales[Leaf==split.nd &index.train,"SALES.UNITS"]
#var.N=aus.sales[Leaf==split.nd &index.train,"size.periodreg"]
#name.split=names.part
#param.init=rep(0,ncol(varReg))
#M=4
#nBoost=9
#nu=0.1
#fit.method="linear"
#mini.size=20
#ncatelow=5
#ncatehigh=20
#market=as.factor(aus.sales[Leaf==split.nd &index.train,"SEGMENT"])
###########################
 n=nrow(varReg)
 coef.init=coef.Boost=matrix(rep(param.init,n),nrow=n,byrow=T)
 resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
 resid.cur=varReg*resid.cur#+2*nrow(varReg)*shrink.pen*() # Shrinkage penalty.
 var.imp=rep(0,length(name.split))
 var.freq=rep(0,length(name.split))
 splt.crit=c()
 splt.modelest=c()
 R2.in=c()
 error.in=c()
 crit.vec=c()
 coefs.mat=c()
 coefs.mat[[1]]=coef.Boost
 for(iBoost in 1:nBoost)
 {
  cat(iBoost,'\n')
  #indice=sort(sample(1:ncol(varPart),mtry))
  #varPart.cur=(varPart[,indice])
  #if(mtry<2) varPart.cur=as.matrix(varPart[,indice])
  #cat(name.split[indice],dim(varPart.cur),'\n')
  #name.split.cur=name.split[indice]
  #crit.root=sum((regression(resid.cur,varReg,method=fit.method)$residuals)^2)
  dat.cur=cbind((resid.cur),varPart)
  res=mvpart(as.formula(paste("resid.cur~",paste(colnames(varPart),collapse="+"),sep="")),data=dat.cur,size=M,xv="min",minbucket=mini.size)
  #if(<2))
  # write.table(cbind(resid.cur,varReg,varPart),"xixi.csv",sep=',',row.names=F)
  #print(table(res$leaf.assoc))
  #print(res$model.est)
  #cat(length(unique(res$leaf.assoc)),'\n')
  #cat(name.split.cur,'\n')
  #cat(dim(varReg),'\n')
  #cat("Start boosting",iBoost,'\n')
  tmp=gen.design(res$where,cont=varReg)
  #cat("Finish generating design","\n")
  design.cur=cbind(tmp$design)
  #cat(dim(design.cur),'\n')
  #cat(apply(design.cur,2,sum),'\n')
  gamm.cur=lm(lm(log(var.n/var.N)~market)$residuals-apply(coef.Boost*varReg,1,sum)~design.cur-1)$coef#rep(0,ncol(design.cur))
  cat(gamm.cur,'\n')
  #apply(design.cur*as.vector(var.n-var.N*prob.vec)*as.vector(Attraction(util,GAM)$gradlog),2,sum)
  for(iter in 1:n.iter)
  {
   cat(iter,'\n')
   gamm.new=mlogit.IRLS(market,var.N,var.n,design.cur,gamm.cur,fixed=apply(coef.Boost*varReg,1,sum),tolern=toln,oo=F,GAM=opt)$gamm
   if(sum(abs(gamm.new-gamm.cur))< toln) break;
   gamm.cur=gamm.new
  }
  cat(gamm.new,'\n')
  #gamm.cur=optim(gamm.cur,mlogit.choice.llhd,mlogit.choice.gr)$par
  #cat(ncol(varReg),'\n')
  if(ncol(varReg)==2)
  {
   gamm.est=matrix(gamm.cur,nrow=ncol(design.cur)/2,byrow=F)
   coef.increment=nu*(cbind(cbind(gen.design(res$where)$design),cbind(gen.design(res$where)$design))%*%gen.design(as.factor(c(rep(1,ncol(design.cur)/2),rep(2,ncol(design.cur)/2))),cont=as.matrix(gamm.cur))$design)
  }
  #cat("Finish iteration 2",ncol(varReg),"\n")
  if(ncol(varReg)==1)
  {
   gamm.est=as.matrix(gamm.cur)#,nrow=ncol(design.cur)/2,byrow=F)
   coef.increment=nu*gen.design(res$where)$design%*%gamm.cur#gen.design(as.factor(rep(1,ncol(design.cur))),cont=)$design)
  }
  #coef.increment[,1]=coef.increment[,1]-group.fn(market,coef.increment[,1],'mean')$results
  #cat("Finish iteration 1","\n")
  #cat(name.split.cur,(var.imp),",",'\n')
  #var.imp[indice]=var.imp[indice]+(nu^2)*res$split.impvec
  #var.freq[indice]=var.freq[indice]+(res$split.impvec>0)
  if(!(additive)) coef.Boost=coef.increment
  if(additive) coef.Boost=coef.Boost+coef.increment
  coef.Boost[,1]=coef.Boost[,1]-group.fn(market,coef.Boost[,1],"mean")$results
  coefs.mat[[iBoost]]=coef.Boost
  if(additive) resid.cur=2*var.n/var.N-2*Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val/group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results
  for(irow in 1:length(tmp$nam))
  {
    inode=tmp$nam[irow]
    #res$model.est[res$model.est[,1]==inode,2+(1:ncol(varReg))]=gamm.est[irow,]
  }
  #splt.modelest[[iBoost]]=res$model.est
  #splt.crit=c(splt.crit,res$split.krit)
  attraction=Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val
  R2.in=rbind(R2.in,calc.R2(attraction/group.fn(market,attraction,"sum")$results*var.N,var.n))
  crit.temp=(-2)*sum(log(Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val)*var.n)+2*sum(var.n*log(group.fn(market,Attraction(apply(varReg*coef.Boost,1,sum),option=opt)$val,"sum")$results))
  crit.vec=c(crit.vec,crit.temp)
 }
 return(list(variable.imp=var.imp,variable.freq=var.freq,model.est=splt.modelest,split.krit=splt.crit,coefs=coef.Boost,coefs.pred=coefs.mat,crit=crit.vec,f0.hat=coef.init[1,],R2.train=R2.in))
}
################################################################################
## MCEM algorithm for mixed logit graphical LASSO
Mlogit.MCEM=function(xmat,zmat,markt,var.n,var.N,bet.init,Sig.init,lamb=0,covmethod="glasso",niter.IRLS=50,niter.quad=1,oniter=100,toln=1e-4,validation=F,fixed.var=F)
{
    bet.cur=bet.init
    Sig.cur=Sig.init
    market.ind=gen.design(as.factor(markt))$design
    xixi=unlist(strsplit(colnames(zmat),","))
    market.randeff=xixi[(1:length(xixi)) %% 2 ==1]
    fact.randeff=xixi[(1:length(xixi)) %% 2 ==0]
    eta.cur=rep(0,ncol(zmat))
    cmat=cbind(xmat,zmat)
    etamean=0
    etamean.mat=c()
################################################################################
## MCE in MCEM algorithm.
  for(oiter in 1:oniter)
   {
   #oiter=1
## Quadratic approximation to the multinomial distribution
   #if(oiter>1) niter.quad=1
   #for(iter in 1:niter.quad)
   #{
    theta.cur=c(bet.cur,eta.cur)
    attr.cur=Attraction(cmat%*%theta.cur)$val
    prob.cur=as.vector(attr.cur/group.fn(markt,attr.cur,"sum")$results)
    ztilde=((lm(zmat*prob.cur~as.factor(markt)-1)$fitted)*group.fn(markt,rep(1,length(markt)),"sum")$results)
    bvec=apply((zmat-ztilde)*var.n,2,sum)
    #print(Sig.cur)
    #cat("Sig.cur",sort(unique(eigen(Sig.cur)$values))[1:10],'\n')
    #cat("covar",sort(eigen(Sig.cur)$values)[1:10],'\n')
    Omega1=inverse(Sig.cur)#kronecker(diag(rep(1,nmarket)),solve(Sig.cur))#solve(init$Sigma)
    Omega1.half=chol(Omega1)
    Atemp=zmat*sqrt(prob.cur*var.N)
    Att=as.matrix(aggregate(ztilde*sqrt(var.N),list(fact=as.factor(markt)),mean)[,-1])
    Amat=outerprod(Atemp) + t(Att)%*%Att
    #cat("Amat+Omega1",sort(unique(eigen(Amat+Omega1)$values))[1:10],'\n')
    cov.new=inverse(Amat+Omega1)
    eta.new=eta.cur+cov.new%*%(bvec-Omega1%*%eta.cur)
    eta.cur=eta.new
################################################################################
    B=10000
    memory.limit(size=3600)
    eita=t(mvrnorm(B,mu=rep(0,dim(cov.new)[1]), cov.new))
    theta.sim=eita+as.vector(eta.cur)
    utilm=(zmat%*%theta.sim)+ as.vector(xmat%*%bet.cur)
    probm=exp.rob(utilm)/((lm((exp.rob(utilm))~as.factor(markt)-1)$fitted)*group.fn(as.factor(markt),rep(1,length(markt)),"sum")$results)
    logllhd=log(probm)*var.n
    logwt.num=t(market.ind)%*% (logllhd)- (t(market.ind)%*%zmat !=0) %*%(-(Omega1.half%*%theta.sim)^2/2)
    logwt.denom=-(t(market.ind)%*%zmat !=0)%*%(-(chol(Amat+Omega1)%*%eita)^2/2)
    logwt.imp=logwt.num-logwt.denom-apply(logwt.num-logwt.denom,1,max)
    wt=t(t(market.ind)%*%zmat !=0) %*% (exp(logwt.imp)/apply(exp(logwt.imp),1,sum))
    etamean=apply(theta.sim*wt,1,sum)
    etamean.mat=cbind(etamean.mat,etamean)
    rm(eita)
    rm(theta.sim)
    rm(utilm)
    rm(probm)
    rm(logllhd)
    rm(logwt.denom)
    rm(logwt.num)
    rm(wt)
################################################################################
##  Maximization in MCEM algorithm.
    mat=(tabulat(fact.randeff,market.randeff,etamean))
    #print(mat)
    loglike=log(prod(diag(Omega1.half)))-(t(etamean)%*%Omega1%*%etamean)/2
    if(validation) return(list(logllhd=loglike))
    if(covmethod=="glasso")
    {
     if(lamb==0) covar=mat%*%t(mat)/ncol(mat)
     if(lamb>0) covar=glasso(mat%*%t(mat)/ncol(mat),lamb/ncol(mat))
    }
    if(covmethod=="diagonal")
    {
     #cat("diagonal",'\n')
     covar=diag(diag(cov(t(tabulat(fact.randeff,market.randeff,etamean)))))
    }
    if(covmethod=="global")
      {
       covar=diag(rep(mean(diag(cov(t(tabulat(fact.randeff,market.randeff,etamean))))),nrow(mat)))
       }
    #cat(covar,'\n')
    Sig.update=matrix(0,nrow=ncol(zmat),ncol=ncol(zmat),byrow=T)
    for(imark in 1:lengthunique(market.randeff))
    {
     indices.mark=(market.randeff==unique(market.randeff)[imark])
     perturb.mark=match(fact.randeff[indices.mark], rownames(mat))
     #cat(imark,fact.randeff[indices.mark],'\n')
     #cat(imark, indices.mark,'\n')
     Sig.update[indices.mark,indices.mark]=covar[perturb.mark,perturb.mark]
    }
    cat("xixi:",eigen(Sig.update)$values,'\n')
    if(fixed.var) diag(Sig.update)=.1
    bet.update=bet.cur
    for(iter in 1:niter.IRLS)
    {
     bet.new=mlogit.IRLS(markt,var.N,var.n,xmat,bet.update,fixed=as.vector(zmat%*%etamean),oo=F)$gamm
     if(max(abs(bet.new-bet.update))< toln) break;
     bet.update=bet.new
    }
     if(max(abs(bet.update-bet.cur))< toln & max(abs(Sig.update-Sig.cur))< toln) break;
     Sig.cur=Sig.update
     bet.cur=bet.update

    }
    return(list(bet.est=bet.cur,Sig.est=Sig.cur,niter=oiter,logllhd=loglike,randeff=etamean.mat))
  }
################################################################################
###  For every set of random effects, estimate fixed effects parameters.
##   Return updated Sigma matrix and fixed coefficient beta (one weight per sample).
mlogit.MCE=function(X,Z,t.vec,Nvec,nvec,B,etam,toln=1e-6,niter=150)
{
   ### Estimate the vector of beta and calculate log-likelihood.
   sfInit(parallel=TRUE, cpus=ncpus, type="SOCK")
   begintime=strptime(date(), "%a %b %d %H:%M:%S %Y")
   mlogitIRLS.wrap=function(icol,fixm=NA,var.n=NA,var.N=NA,X.mat=NA,market=NA,n.iter=NA)
   {
    source("F:\\Research\\MPO\\Application\\mlogitIRLS.R")
    fixt=etam[,icol]
    gamm.cur=lm(lm(log(var.n/var.N)~market-1)$residuals-fixt~X.mat-1)$coef
    for(iter in 1:n.iter)
    {
     res.new=mlogit.IRLS(market,var.N,var.n,X.mat,gamm.cur,fixed=fixt)
     gamm.new=res.new$gamm
     if(sum(abs(gamm.new-gamm.cur))< toln) break;
     gamm.cur=gamm.new
     logL.cur=res.new$crit
    }
    bet.est=gamm.cur
    log.prob=logL.cur
    return(list(bet=bet.est, logL=log.prob))
   }
   resu <- sfLapply(1:B, mlogitIRLS.wrap,fixm=etam,var.n=nvec,var.N=Nvec,X.mat=X,market=t.vec,n.iter=niter)
   sfStop()
   xixi=matrix(unlist(resu),nrow=B,byrow=T)
   result=list(estim=xixi[,-ncol(xixi)],logL=xixi[,ncol(xixi)])
   return(result)
 }
################################################################################
## Quadratic approximation and estimate the vector of theta.
mlogit.QuadE=function(X,Z,t.vec,Nvec,nvec,B,etam,toln=1e-6,niter=150)
{
    xmat=Xmat
    zmat=Zmat
    referen.index=reference.index
    markt=market
    var.n=nvec
    var.N=Nvec
    init=Init

    cmat=cbind(xmat,zmat)
    dimx=1:ncol(xmat)
    dimz=(ncol(xmat)+1):ncol(cmat)
    theta.cur=c(init$bet,rep(0,ncol(zmat)))
    #theta.cur=0
    attr.cur=Attraction(cmat%*%theta.cur)$val
    prob.cur=as.vector(attr.cur/group.fn(markt,attr.cur,"sum")$results)
    ctilde=((lm(cmat*prob.cur~as.factor(markt)-1)$fitted)*group.fn(markt,rep(1,length(markt)),"sum")$results)
    bvec=apply((cmat-ctilde)*var.n,2,sum)
    #hist(prob.cur)
    #Atemp=cmat*sqrt(prob.cur*var.n)
    Omega1.half=Omega1=matrix(0,nrow=ncol(cmat),ncol=ncol(cmat),byrow=F)
    #print(init$Sigma)
    Omega1[dimz,dimz]=solve(init$Sigma)
    Omega1.half[dimz,dimz]=chol(solve(init$Sigma))
    #cat("heihei",'\n')
    #Amat11=Amat12=0
    #for(im in 1:lengthunique(markt))
    #{
    #im=1
    #  imark=unique(markt)[im]
    #  cpmat.cur=(cmat*prob.cur)[markt==imark,]
    #  cp2mat.cur=(cmat*sqrt(prob.cur))[markt==imark,]
    #  N.cur=var.N[markt==imark]
    #  Amat11=Amat11+ (outerprod(cp2mat.cur))*N.cur[1]
    #  Amat12=Amat12+apply(cpmat.cur,2,sum) %*%  t(apply(cpmat.cur,2,sum))*N.cur[1]
    #}
    Atemp=cmat*sqrt(prob.cur*var.N)
    Att=as.matrix(aggregate(ctilde*sqrt(var.N),list(fact=as.factor(markt)),mean)[,-1])
    Amat21=outerprod(Atemp)
    Amat22=t(Att)%*%Att
    Amat=Amat21+Amat22
    Achol1=Atemp
    Achol2=Att
    #sum(abs(Amat22-Amat12))
    #print(round(Amat[1:20,1:20],3))
    #tapply(ctilde*sqrt(var.N),markt,sum)
    cov.new=blockinverse(Amat+Omega1,max(dimx))
    theta.new=theta.cur+cov.new%*%(bvec-Omega1%*%theta.cur)
}
################################################################################
## Calculate matrix inverse of a symmetric block matrix;
## where p1 << p, and M22 is diagonal.
blockinverse=function(mat, p1)
{
  A=mat-mat
  p=dim(mat)[1]
  M11=mat[1:p1,1:p1]
  M12=mat[1:p1,(1+p1):p]
  M22=mat[(1+p1):p,(1+p1):p]
  M22inv=solve(M22)
  A11=solve(M11-M12%*%solve(M22)%*%t(M12))
  A12=-A11%*%M12%*%M22inv
  A22=M22inv+M22inv%*%t(M12)%*%A11%*%M12%*%M22inv
  A21=t(A12)
  A[1:p1,1:p1]=A11
  A[1:p1,(1+p1):p]=A12
  A[(1+p1):p,1:p1]=A21
  A[(1+p1):p,(1+p1):p]=A22
  return(A)
}
#p=7
#m=(rWishart(1,10,diag(rep(1,p))))[,,1]
#blockinverse(m,4)
#solve(m)
################################################################################
# estimate parameter beta in MNL model, given the eita vector.
## each row of X and bet needs to have the same length;
updBeta.mlogit.mixed=function(t_vec,Nvec,nvec,X,Z,eita.vec,bet)
{
 fixed=eita.vec
 mlogit.res=mlogit.IRLS(seg=tvec,Nvec,nvec,zmat=X,gamm.param=bet,fixed=fixed,tolern=1e-6,oo=FALSE,GAM="exp")
 ## mlogit.choice.IRLS: One iteration of the IRLS algorithm for multi-logistic choice model, with multiple market segments.
 bet.new=mlogit.res$gamm
 log.prob=mlogit.res$crit
 ################################

   q=q/mean(Nvec)
	 q=exp(q-median(q))/sum(exp(q-median(q)))
   bet.wtd=t(q)%*%bet.mat
   ## After obtaining weighted estimation beta, update Omega by gLasso
   eita.frame=data.frame(t(eita))  ## dim: (K-1)T*B
   # if( method=='spatial'){
   #  eita.list=lapply(eita.frame,function(x){matrix(x,ncol=1)%*%matrix(x,nrow=1)})  ##  each list element is a KT*KT matrix
   Smat=matrix(0,nrow=dim(Sigma.cur)[2],ncol=dim(Sigma.cur)[2])  ## dim: KT*KT
   # }
   # if(method=='glasso'){
   eita.list=lapply(eita.frame,function(x){
	 xx=matrix(x,ncol=dim(Sigma.cur)[1],byrow=TRUE)
   cov(xx)})
	 Smat=matrix(0,nrow=dim(Sigma.cur)[1],ncol=dim(Sigma.cur)[1])
   #}
   #for (bb in 1:dim(eita)[1])
   for (b in 1:B)
   {
    ## Smat=Smat+q[round((bb-1)/no.market)+1]*eita.list[[bb]]
	  Smat=Smat+q[b]*eita.list[[b]] # (K-1)*(K-1)
   }
    if (method=='glasso')
  	{
	   temp=glasso(Smat,rho)
	   Sigma.new=temp$w
	   loglik=temp$loglik
	   Sigma.new=Smat
	   #loglik=0.5*log(det(ginv(Smat))-sum(diag(Smat%*%ginv())
	   }
	if (method=='spatial')
    {
  	temp=nlminb(gam.start,spatial,Smat=Smat,thrd=threshold,Dist=Dist,M.mat=M.mat,lower=0.2)  #optimal gamma
		print(temp$par)
		Sigma.new=exp(-temp$par*as.matrix(Dist))  ## estimated component covariance matrix
		Sigma.new[abs(Sigma.new)<threshold]=0     ## is this 'gating' thresholding step reasonable?
		diag(Sigma.new)=1+0.001
		}
	if(method=='glasso') return(list(Sigma=Sigma.new,bet=bet.wtd,loglik=loglik))
	return(list(gam=temp$par,Sigma=Sigma.new,bet=bet.wtd))
 ################################
 return(list(bet=bet.new, logL=log.prob))
}
################################################################################
## Spatial Correlation loglikehood for Optimization, where the input Dist is a distance matrix
spatial=function(gam,Smat,thrd,Dist,M.mat,sig.diag=rep(1,nrow(Smat))){
Dist=as.matrix(Dist)
Sigma=exp(-gam*Dist)
Sigma[abs(Sigma)<thrd]=0
SD.mat=sig.diag%*%t(sig.diag)
Sigma=Sigma*SD.mat
chol.Sigma=chol(Sigma)
Omega=ginv(chol.Sigma)%*%ginv(t(chol.Sigma))
#M.inverse=ginv(M.mat)
#Sigma.kronecker=kronecker(M.inverse,Omega)
loglkhd=0.5*log(det(Omega))-0.5*sum(t(Smat)*Omega)
#loglkhd=0.5*log(det(Sigma.kronecker))-0.5*sum(diag(Smat%*%Sigma.kronecker))
return(log.min=-loglkhd)
}
################################################################################
#####   investigate how does the likelihood change w.r.t beta parameter and  s.d  in mixed-logit choice model (assume diagnal are the same)
log.likelihood.par<-function(bet,sig, Sigma.cur,gam,thrd){
  eita=mvrnorm(B*dim(M.mat)[1], mu=rep(0,dim(Sigma.cur)[1]), Sigma.cur, tol = 1e-6, empirical = FALSE)
   eita=matrix(eita,nrow=B,byrow=TRUE);
  ### Associate eita samples with weights:
   q=rep(NaN,B)
   bet.mat=matrix(NaN,nrow=B,ncol=dim(X)[2])

    for (b in 1:B)
     {
	    eita.vec=eita[b,non.index]
      upd.MNL=updBeta.mlogit.mixed(tvec,Nvec,nvec,X,Z,eita.vec,bet)  ## one step update of fixed coefficient beta and obtaining the correponding weight
       bet.mat[b,]=upd.MNL$bet
      q[b]=upd.MNL$logL ## weight for Monte Carlo sample b.
	 # print(q[b])
     }

      #q=exp(q-median(q))/sum(exp(q-median(q)))
       q=(q-min(q))/sum(q-min(q))
     #print(q)
	 #hist(q)
	 #hist(mean(X)*bet.mat)
     bet.wtd=t(q)%*%bet.mat
     rm(bet.mat)
   ## After obtaining weighted estimation beta, update Omega by gLasso
   eita.frame=data.frame(t(eita))  ## dim: KT*B
     # if( method=='spatial'){
 #  eita.list=lapply(eita.frame,function(x){matrix(x,ncol=1)%*%matrix(x,nrow=1)})  ##  each list element is a KT*KT matrix
   Smat=matrix(0,nrow=dim(Sigma.cur)[2],ncol=dim(Sigma.cur)[2])  ## dim: KT*KT
  # }
  # if(method=='glasso'){
       eita.list=lapply(eita.frame,function(x){
	   xx=matrix(x,ncol=dim(Sigma.cur)[1],byrow=TRUE)
	  cov(xx)})
	   Smat=matrix(0,nrow=dim(Sigma.cur)[1],ncol=dim(Sigma.cur)[1])
   #}
   #for (bb in 1:dim(eita)[1])
   for (b in 1:B)
   {
     ## Smat=Smat+q[round((bb-1)/no.market)+1]*eita.list[[bb]]
	 Smat=Smat+q[b]*eita.list[[b]]
   }
   #print(Smat)
   # Smat is the weighted estimate for Sigma
 if (method=='glasso')
	{
	temp=glasso(Smat,rho)
	   Sigma.new=temp$w

	   }
like=spatial(gam,Smat,thrd,Dist,M.mat,rep(sig,nrow(Sigma.cur)))
return(like)
}
################################################################################
f_K_fold <- function(Nobs,K=5){
    rs <- runif(Nobs)
    id <- seq(Nobs)[order(rs)]
    k <- as.integer(Nobs*seq(1,K-1)/K)
    k <- matrix(c(0,rep(k,each=2),Nobs),ncol=2,byrow=TRUE)
    k[,1] <- k[,1]+1
    l <- lapply(seq.int(K),function(x,k,d)
                list(train=d[!(seq(d) %in% seq(k[x,1],k[x,2]))],
                     test=d[seq(k[x,1],k[x,2])]),k=k,d=id)
   return(l)
}
################################################################################
heatmap.cor=function(corm,cexC=.6,cexR=.6,ncolr=100,ncolr.lgd=16)
{
   pseudo=corm/max(abs(corm))*(ncolr-1)/2+(ncolr-1)/2+1
   #cat(min(pseudo):max(pseudo),'\n')
   pseudo.ldg=corm/max(abs(corm))*(ncolr.lgd-1)/2+(ncolr.lgd-1)/2+1
   #cat(min(pseudo.ldg):max(pseudo.ldg),'\n')
   heatmap(corm,symm=TRUE,margin=c(10,7),cexCol=cexC,cexRow=cexR,col=cm.colors(ncolr)[(min(pseudo):max(pseudo))])
   legend('left',legend=round(seq(min(corm),max(corm),length.out=length(min(pseudo.ldg):max(pseudo.ldg))),2),fill=cm.colors(ncolr.lgd)[min(pseudo.ldg):max(pseudo.ldg)],cex=0.7)
   return()
}
################################################################################
## each row of mat is x_i, return \sum x_i x_i^{prime}.
outerprod=function(mat)
{
   meanv=apply(mat,2,mean)
   return((nrow(mat)-1)*cov(mat) + nrow(mat)*(meanv%*%t(meanv)))
}
################################################################################
## Simulate from a mixed logit model to return the predicted choice probabilities.
sim.mixedlogit=function(xmat.sim,zmat.sim,bet.sim,Sig.sim,market.sim,fact1.sim,fact2.sim,Nvec.sim=rep(1,nrow(xmat.sim)))
  {
   B=10000
   eita.sim=t(mvrnorm(B,mu=rep(0,dim(Sig.sim)[1]), diag(rep(1,nrow(Sig.sim))))%*% t(chol(Sig.sim)))
   utilm.sim=zmat.sim%*%eita.sim+as.vector(xmat.sim%*%bet.sim)
   if(lengthunique(market.sim)>1) probm.sim=exp.rob(utilm.sim)/((lm((exp.rob(utilm.sim))~as.factor(market.sim)-1)$fitted)*group.fn(as.factor(market.sim),rep(1,length(market.sim)),"sum")$results)
   if(lengthunique(market.sim)==1) probm.sim=exp.rob(utilm.sim)/apply(exp.rob(utilm.sim),2,sum)
   probm.pred=apply(probm.sim,1,mean)
   tab.sim=xtabs((probm.pred*Nvec.sim)~fact1.sim+fact2.sim)
   return(tab.sim)
  }
################################################################################
## Simulate from a mixed logit model to return the predicted choice probabilities.
 mosaic.plus=function(tab,titles="")
 {
  #rownames(tab.whole)
  #If Row Proportions are wanted later
  proportions <- round(prop.table(tab , 1 )*100)
  #Hand role the structure for the mosaic
  values <- c(tab)
  rowvarcat <- rownames(tab)#c("A","B")
  columnvarcat <- colnames(tab)#c("1","2")
  names=c(names(dimnames(tab)[1]),names(dimnames(tab)[2]))
  dims <- c(length(rowvarcat),length(columnvarcat)) #columns then rows
  TABS <- structure( c(values), .Dim = as.integer(dims), .Dimnames = structure( list(rowvarcat,columnvarcat ),
  .Names = c(names) ) , class = "table")
  PROPORTIONS <- structure( c(proportions), .Dim = as.integer(dims), .Dimnames = structure( list(rowvarcat,columnvarcat ),
  .Names = c(names) ) , class = "table")
  TABSPROPORTIONS <- structure( c(paste(proportions,"%","\n", "(",round(values),")",sep="")), .Dim = as.integer(dims), .Dimnames = structure( list(rowvarcat,columnvarcat ),
  .Names = c(names) ) , class = "table")
  mosaic(TABS,pop=F,shade=T,legend=F, main=titles)
  labeling_cells(text=TABSPROPORTIONS , clip_cells=FALSE)(TABS)
  return(NULL)
 }
################################################################################
# ----- Define a function for plotting a matrix ----- #
myImagePlot <- function(x, x.lab,y.lab, ...){
     min <- min(x)
     max <- max(x)
     yLabels <- rownames(x)
     xLabels <- colnames(x)
     title <-c()
  # check for additional function arguments
  if( length(list(...)) ){
    Lst <- list(...)
    if( !is.null(Lst$zlim) ){
       min <- Lst$zlim[1]
       max <- Lst$zlim[2]
    }
    if( !is.null(Lst$yLabels) ){
       yLabels <- c(Lst$yLabels)
    }
    if( !is.null(Lst$xLabels) ){
       xLabels <- c(Lst$xLabels)
    }
    if( !is.null(Lst$title) ){
       title <- Lst$title
    }
  }
# check for null values
if( is.null(xLabels) ){
   xLabels <- c(1:ncol(x))
}
if( is.null(yLabels) ){
   yLabels <- c(1:nrow(x))
}

layout(matrix(data=c(1,2), nrow=1, ncol=2), widths=c(4,1), heights=c(1,1))

 # Red and green range from 0 to 1 while Blue ranges from 1 to 0
 ColorRamp <- heat.colors(256)#rgb( seq(0,1,length=256),  # Red
              #     seq(0,1,length=256),  # Green
              #     seq(1,0,length=256))  # Blue
 ColorLevels <- seq(min, max, length=length(ColorRamp))

 # Reverse Y axis
 reverse <- nrow(x) : 1
 yLabels <- yLabels[reverse]
 x <- x[reverse,]

 # Data Map
 par(mar = c(4.2,5,2.5,2))
 image(1:length(xLabels), 1:length(yLabels), t(x), col=ColorRamp, xlab=x.lab,
 ylab=y.lab, axes=FALSE, zlim=c(min,max))
 if( !is.null(title) ){
    title(main=title)
 }
axis(BELOW<-1, at=1:length(xLabels), labels=xLabels, cex.axis=1)
axis(LEFT <-2, at=1:length(yLabels), labels=yLabels, las= HORIZONTAL<-1,
 cex.axis=1)

 # Color Scale
 par(mar = c(3,2.5,2.5,2))
 image(1, ColorLevels,
      matrix(data=ColorLevels, ncol=length(ColorLevels),nrow=1),
      col=ColorRamp,
      xlab="",ylab="",
      xaxt="n")
 layout(1)
}
# ----- END plot function ----- #
